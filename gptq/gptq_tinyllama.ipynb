{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23681762",
   "metadata": {},
   "source": [
    "## TinyLLama Multi-bit GPTQ Quantization - Jupyter Notebook\n",
    "### This notebook will save the original model locally and create 8-bit, 4-bit, and 2-bit quantized versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9554e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import warnings\n",
    "\n",
    "# Set these BEFORE importing GPTQModel\n",
    "os.environ['DISABLE_TRITON'] = '1'\n",
    "os.environ['TORCHDYNAMO_DISABLE'] = '1'\n",
    "os.environ['TORCH_COMPILE_DISABLE'] = '1'\n",
    "os.environ['PYTORCH_DISABLE_DYNAMO'] = '1'\n",
    "\n",
    "# Suppress warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "from gptqmodel import GPTQModel, QuantizeConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "64883ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA available: True\n",
      "GPU: Quadro RTX 3000\n"
     ]
    }
   ],
   "source": [
    "# Quantisation environment set up\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ee451",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Model configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0cc76bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "Source model: TinyLlama/TinyLlama_v1.1\n",
      "Base output directory: TinyLlama-models\n",
      "Q8 output directory: TinyLlama-models/TinyLlama-gptqmodel-8bit\n",
      "Q4 output directory: TinyLlama-models/TinyLlama-gptqmodel-4bit\n",
      "Q2 output directory: TinyLlama-models/TinyLlama-gptqmodel-2bit\n"
     ]
    }
   ],
   "source": [
    "model_id = \"TinyLlama/TinyLlama_v1.1\"\n",
    "base_output_dir = \"TinyLlama-models\"\n",
    "original_model_dir = f\"{base_output_dir}/original\"\n",
    "output_dir_8= f\"{base_output_dir}/TinyLlama-gptqmodel-8bit\"\n",
    "output_dir_4= f\"{base_output_dir}/TinyLlama-gptqmodel-4bit\"\n",
    "output_dir_2= f\"{base_output_dir}/TinyLlama-gptqmodel-2bit\"\n",
    "\n",
    "# Quantization configurations\n",
    "quantize_config_8 = QuantizeConfig(\n",
    "        bits=8,  # quantize model to 8-bit\n",
    "        group_size=128,  # it is recommended to set the value to 128\n",
    "    )\n",
    "\n",
    "quantize_config_4 = QuantizeConfig(\n",
    "        bits=4,  # quantize model to 4-bit\n",
    "        group_size=128,  # it is recommended to set the value to 128\n",
    "    )\n",
    "\n",
    "quantize_config_2 = QuantizeConfig(\n",
    "        bits=2,  # quantize model to 2-bit\n",
    "        group_size=64,  # it is recommended to set the value to 128\n",
    "    )\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"Source model: {model_id}\")\n",
    "print(f\"Base output directory: {base_output_dir}\")\n",
    "print(f\"Q8 output directory: {output_dir_8}\")\n",
    "print(f\"Q4 output directory: {output_dir_4}\")\n",
    "print(f\"Q2 output directory: {output_dir_2}\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "os.makedirs(output_dir_8, exist_ok=True)\n",
    "os.makedirs(output_dir_4, exist_ok=True)\n",
    "os.makedirs(output_dir_2, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3eac5ad8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving original model to: TinyLlama-models/original\n",
      "Original model saved!\n",
      "Original model size: 2.05 GB\n"
     ]
    }
   ],
   "source": [
    "# Load tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load original model\n",
    "original_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# Save original model locally\n",
    "print(f\"Saving original model to: {original_model_dir}\")\n",
    "original_model.save_pretrained(original_model_dir, safe_serialization=True, max_shard_size=\"3.5GB\")\n",
    "tokenizer.save_pretrained(original_model_dir)\n",
    "\n",
    "# Get original model size\n",
    "original_size = sum(os.path.getsize(os.path.join(original_model_dir, f)) \n",
    "                   for f in os.listdir(original_model_dir) \n",
    "                   if os.path.isfile(os.path.join(original_model_dir, f)))\n",
    "\n",
    "print(f\"Original model saved!\")\n",
    "print(f\"Original model size: {original_size / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "82196f36",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def QuantizeModelandSaveP(quantization_config, output_dir):\n",
    "    model = GPTQModel.load(original_model_dir, quantization_config)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(original_model_dir, use_fast=True)\n",
    "    calibration_dataset = [\n",
    "        tokenizer(\n",
    "            \"gptqmodel is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model.quantize(calibration_dataset)\n",
    "    model.save(output_dir)\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5602d7b5",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native float16): `torch.float16`                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=0 (token='<unk>').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 2048,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`      \n",
      "\u001b[33mWARN\u001b[0m  Calibration dataset size should be more than 256. Current: 1.            \n",
      "\u001b[33mWARN\u001b[0m  The average length of input_ids of calibration_dataset should be greater than 256: actual avg: 31.0.\n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_ungrieved_time_07_28_2025_11h_57m_26s.log`\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.k_proj     | \u001b[92m0.00051882\u001b[0m | 1           | 0.01000     | 2.063     | 0.374        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.v_proj     | \u001b[92m0.00001211\u001b[0m | 1           | 0.01000     | 1.932     | 0.374        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.q_proj     | \u001b[92m0.00092513\u001b[0m | 1           | 0.01000     | 1.890     | 0.374        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.o_proj     | \u001b[92m0.00000089\u001b[0m | 1           | 0.01000     | 1.797     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.up_proj          | \u001b[92m0.00051592\u001b[0m | 1           | 0.01000     | 2.190     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.gate_proj        | \u001b[92m0.00051009\u001b[0m | 1           | 0.01000     | 2.051     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.down_proj        | \u001b[92m0.00278950\u001b[0m | 1           | 0.01000     | 6.447     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.k_proj     | \u001b[92m0.00080696\u001b[0m | 1           | 0.01000     | 1.880     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.v_proj     | \u001b[92m0.00004102\u001b[0m | 1           | 0.01000     | 1.815     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.q_proj     | \u001b[92m0.00287710\u001b[0m | 1           | 0.01000     | 1.921     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.o_proj     | \u001b[92m0.00000168\u001b[0m | 1           | 0.01000     | 1.869     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.up_proj          | \u001b[92m0.00089655\u001b[0m | 1           | 0.01000     | 1.953     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.gate_proj        | \u001b[92m0.00099165\u001b[0m | 1           | 0.01000     | 1.867     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.down_proj        | \u001b[92m0.00001827\u001b[0m | 1           | 0.01000     | 6.010     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.k_proj     | \u001b[92m0.00222330\u001b[0m | 1           | 0.01000     | 1.675     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.v_proj     | \u001b[92m0.00009904\u001b[0m | 1           | 0.01000     | 1.593     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.q_proj     | \u001b[92m0.00606226\u001b[0m | 1           | 0.01000     | 1.682     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.o_proj     | \u001b[92m0.00000809\u001b[0m | 1           | 0.01000     | 1.651     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.up_proj          | \u001b[92m0.00122875\u001b[0m | 1           | 0.01000     | 1.973     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.gate_proj        | \u001b[92m0.00133369\u001b[0m | 1           | 0.01000     | 1.883     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.down_proj        | \u001b[92m0.00000678\u001b[0m | 1           | 0.01000     | 6.009     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.k_proj     | \u001b[92m0.00169049\u001b[0m | 1           | 0.01000     | 1.723     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.v_proj     | \u001b[92m0.00013950\u001b[0m | 1           | 0.01000     | 1.666     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.q_proj     | \u001b[92m0.00548247\u001b[0m | 1           | 0.01000     | 1.710     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.o_proj     | \u001b[92m0.00000595\u001b[0m | 1           | 0.01000     | 1.660     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.up_proj          | \u001b[92m0.00155039\u001b[0m | 1           | 0.01000     | 2.054     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.gate_proj        | \u001b[92m0.00185651\u001b[0m | 1           | 0.01000     | 2.044     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.down_proj        | \u001b[92m0.00000669\u001b[0m | 1           | 0.01000     | 6.610     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.k_proj     | \u001b[92m0.00254361\u001b[0m | 1           | 0.01000     | 2.327     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.v_proj     | \u001b[92m0.00015291\u001b[0m | 1           | 0.01000     | 2.327     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.q_proj     | \u001b[92m0.00657682\u001b[0m | 1           | 0.01000     | 1.943     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.o_proj     | \u001b[92m0.00000406\u001b[0m | 1           | 0.01000     | 1.762     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.up_proj          | \u001b[92m0.00182080\u001b[0m | 1           | 0.01000     | 1.871     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.gate_proj        | \u001b[92m0.00216327\u001b[0m | 1           | 0.01000     | 1.969     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.down_proj        | \u001b[92m0.00000972\u001b[0m | 1           | 0.01000     | 6.661     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.k_proj     | \u001b[92m0.00248353\u001b[0m | 1           | 0.01000     | 1.778     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.v_proj     | \u001b[92m0.00014576\u001b[0m | 1           | 0.01000     | 1.773     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.q_proj     | \u001b[92m0.00640778\u001b[0m | 1           | 0.01000     | 2.001     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.o_proj     | \u001b[92m0.00000331\u001b[0m | 1           | 0.01000     | 1.854     | 0.005        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.up_proj          | \u001b[92m0.00217219\u001b[0m | 1           | 0.01000     | 2.105     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.gate_proj        | \u001b[92m0.00255769\u001b[0m | 1           | 0.01000     | 1.919     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.down_proj        | \u001b[92m0.00001137\u001b[0m | 1           | 0.01000     | 6.763     | 0.013        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.k_proj     | \u001b[92m0.00340194\u001b[0m | 1           | 0.01000     | 1.907     | 0.019        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.v_proj     | \u001b[92m0.00016664\u001b[0m | 1           | 0.01000     | 1.745     | 0.019        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.q_proj     | \u001b[92m0.00781302\u001b[0m | 1           | 0.01000     | 1.808     | 0.019        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.o_proj     | \u001b[92m0.00000411\u001b[0m | 1           | 0.01000     | 1.950     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.up_proj          | \u001b[92m0.00260401\u001b[0m | 1           | 0.01000     | 2.352     | 0.032        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.gate_proj        | \u001b[92m0.00322510\u001b[0m | 1           | 0.01000     | 2.015     | 0.032        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.down_proj        | \u001b[92m0.00001958\u001b[0m | 1           | 0.01000     | 6.225     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.k_proj     | \u001b[92m0.00279211\u001b[0m | 1           | 0.01000     | 1.729     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.v_proj     | \u001b[92m0.00016039\u001b[0m | 1           | 0.01000     | 1.679     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.q_proj     | \u001b[92m0.00736329\u001b[0m | 1           | 0.01000     | 1.920     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.o_proj     | \u001b[92m0.00000452\u001b[0m | 1           | 0.01000     | 1.912     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.up_proj          | \u001b[92m0.00271578\u001b[0m | 1           | 0.01000     | 2.286     | 0.032        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.gate_proj        | \u001b[92m0.00341759\u001b[0m | 1           | 0.01000     | 1.976     | 0.032        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.down_proj        | \u001b[92m0.00002443\u001b[0m | 1           | 0.01000     | 6.346     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.k_proj     | \u001b[92m0.00369120\u001b[0m | 1           | 0.01000     | 1.803     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.v_proj     | \u001b[92m0.00015656\u001b[0m | 1           | 0.01000     | 1.681     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.q_proj     | \u001b[92m0.00896131\u001b[0m | 1           | 0.01000     | 1.777     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.o_proj     | \u001b[92m0.00003319\u001b[0m | 1           | 0.01000     | 1.761     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.up_proj          | \u001b[92m0.00317500\u001b[0m | 1           | 0.01000     | 1.930     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.gate_proj        | \u001b[92m0.00363738\u001b[0m | 1           | 0.01000     | 1.926     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.down_proj        | \u001b[92m0.00002674\u001b[0m | 1           | 0.01000     | 6.273     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.k_proj     | \u001b[92m0.00265515\u001b[0m | 1           | 0.01000     | 1.774     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.v_proj     | \u001b[92m0.00019853\u001b[0m | 1           | 0.01000     | 1.777     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.q_proj     | \u001b[92m0.00764772\u001b[0m | 1           | 0.01000     | 1.764     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.o_proj     | \u001b[92m0.00002721\u001b[0m | 1           | 0.01000     | 1.773     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.up_proj          | \u001b[92m0.00339501\u001b[0m | 1           | 0.01000     | 2.006     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.gate_proj        | \u001b[92m0.00391460\u001b[0m | 1           | 0.01000     | 2.013     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.down_proj        | \u001b[92m0.00002955\u001b[0m | 1           | 0.01000     | 6.382     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.k_proj     | \u001b[92m0.00342138\u001b[0m | 1           | 0.01000     | 1.788     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.v_proj     | \u001b[92m0.00022896\u001b[0m | 1           | 0.01000     | 1.672     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.q_proj     | \u001b[92m0.00944302\u001b[0m | 1           | 0.01000     | 1.791     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.o_proj     | \u001b[92m0.00004620\u001b[0m | 1           | 0.01000     | 1.734     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.up_proj          | \u001b[92m0.00371678\u001b[0m | 1           | 0.01000     | 1.940     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.gate_proj        | \u001b[92m0.00494442\u001b[0m | 1           | 0.01000     | 1.932     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.down_proj        | \u001b[92m0.00004693\u001b[0m | 1           | 0.01000     | 6.318     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.k_proj     | \u001b[92m0.00348900\u001b[0m | 1           | 0.01000     | 1.807     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.v_proj     | \u001b[92m0.00030692\u001b[0m | 1           | 0.01000     | 1.689     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.q_proj     | \u001b[92m0.00846503\u001b[0m | 1           | 0.01000     | 1.751     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.o_proj     | \u001b[92m0.00005786\u001b[0m | 1           | 0.01000     | 1.646     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.up_proj          | \u001b[92m0.00416755\u001b[0m | 1           | 0.01000     | 1.899     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.gate_proj        | \u001b[92m0.00604209\u001b[0m | 1           | 0.01000     | 2.008     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.down_proj        | \u001b[92m0.00004941\u001b[0m | 1           | 0.01000     | 6.732     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.k_proj     | \u001b[92m0.00364284\u001b[0m | 1           | 0.01000     | 1.854     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.v_proj     | \u001b[92m0.00027547\u001b[0m | 1           | 0.01000     | 1.689     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.q_proj     | \u001b[92m0.00906908\u001b[0m | 1           | 0.01000     | 1.747     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.o_proj     | \u001b[92m0.00003425\u001b[0m | 1           | 0.01000     | 1.733     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.up_proj          | \u001b[92m0.00446561\u001b[0m | 1           | 0.01000     | 1.969     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.gate_proj        | \u001b[92m0.00655250\u001b[0m | 1           | 0.01000     | 1.921     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.down_proj        | \u001b[92m0.00006024\u001b[0m | 1           | 0.01000     | 6.232     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.k_proj     | \u001b[92m0.00375506\u001b[0m | 1           | 0.01000     | 1.785     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.v_proj     | \u001b[92m0.00022585\u001b[0m | 1           | 0.01000     | 1.668     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.q_proj     | \u001b[92m0.00889064\u001b[0m | 1           | 0.01000     | 1.733     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.o_proj     | \u001b[92m0.00005738\u001b[0m | 1           | 0.01000     | 2.090     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.up_proj          | \u001b[92m0.00478764\u001b[0m | 1           | 0.01000     | 2.148     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.gate_proj        | \u001b[92m0.00635924\u001b[0m | 1           | 0.01000     | 2.005     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.down_proj        | \u001b[92m0.00007076\u001b[0m | 1           | 0.01000     | 6.382     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.k_proj     | \u001b[92m0.00388607\u001b[0m | 1           | 0.01000     | 1.772     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.v_proj     | \u001b[92m0.00035866\u001b[0m | 1           | 0.01000     | 1.720     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.q_proj     | \u001b[92m0.01182740\u001b[0m | 1           | 0.01000     | 1.729     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.o_proj     | \u001b[92m0.00012804\u001b[0m | 1           | 0.01000     | 1.786     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.up_proj          | \u001b[92m0.00581007\u001b[0m | 1           | 0.01000     | 1.943     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.gate_proj        | \u001b[92m0.00780590\u001b[0m | 1           | 0.01000     | 1.980     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.down_proj        | \u001b[92m0.00013223\u001b[0m | 1           | 0.01000     | 6.430     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.k_proj     | \u001b[92m0.00380934\u001b[0m | 1           | 0.01000     | 1.691     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.v_proj     | \u001b[92m0.00042987\u001b[0m | 1           | 0.01000     | 1.636     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.q_proj     | \u001b[92m0.01007403\u001b[0m | 1           | 0.01000     | 1.675     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.o_proj     | \u001b[92m0.00014636\u001b[0m | 1           | 0.01000     | 1.678     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.up_proj          | \u001b[92m0.00656036\u001b[0m | 1           | 0.01000     | 1.977     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.gate_proj        | \u001b[92m0.00833433\u001b[0m | 1           | 0.01000     | 1.957     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.down_proj        | \u001b[92m0.00018452\u001b[0m | 1           | 0.01000     | 6.347     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.k_proj     | \u001b[92m0.00381659\u001b[0m | 1           | 0.01000     | 1.820     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.v_proj     | \u001b[92m0.00047409\u001b[0m | 1           | 0.01000     | 1.842     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.q_proj     | \u001b[92m0.00972593\u001b[0m | 1           | 0.01000     | 1.794     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.o_proj     | \u001b[92m0.00013745\u001b[0m | 1           | 0.01000     | 1.779     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.up_proj          | \u001b[92m0.00765137\u001b[0m | 1           | 0.01000     | 2.086     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.gate_proj        | \u001b[92m0.00964348\u001b[0m | 1           | 0.01000     | 2.027     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.down_proj        | \u001b[92m0.00023123\u001b[0m | 1           | 0.01000     | 6.397     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.k_proj     | \u001b[92m0.00383661\u001b[0m | 1           | 0.01000     | 1.840     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.v_proj     | \u001b[92m0.00056874\u001b[0m | 1           | 0.01000     | 1.727     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.q_proj     | \u001b[92m0.00994518\u001b[0m | 1           | 0.01000     | 1.795     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.o_proj     | \u001b[92m0.00011730\u001b[0m | 1           | 0.01000     | 1.789     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.up_proj          | \u001b[92m0.00936889\u001b[0m | 1           | 0.01000     | 2.064     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.gate_proj        | \u001b[92m0.01210778\u001b[0m | 1           | 0.01000     | 1.966     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.down_proj        | \u001b[92m0.00036333\u001b[0m | 1           | 0.01000     | 6.441     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.k_proj     | \u001b[92m0.00344781\u001b[0m | 1           | 0.01000     | 1.765     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.v_proj     | \u001b[92m0.00080013\u001b[0m | 1           | 0.01000     | 1.731     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.q_proj     | \u001b[92m0.01183529\u001b[0m | 1           | 0.01000     | 1.803     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.o_proj     | \u001b[92m0.00012906\u001b[0m | 1           | 0.01000     | 1.807     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.up_proj          | \u001b[92m0.01062711\u001b[0m | 1           | 0.01000     | 1.966     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.gate_proj        | \u001b[92m0.01332856\u001b[0m | 1           | 0.01000     | 1.950     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.down_proj        | \u001b[92m0.00037857\u001b[0m | 1           | 0.01000     | 7.139     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.k_proj     | \u001b[92m0.00363782\u001b[0m | 1           | 0.01000     | 1.830     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.v_proj     | \u001b[92m0.00060554\u001b[0m | 1           | 0.01000     | 1.671     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.q_proj     | \u001b[92m0.00974268\u001b[0m | 1           | 0.01000     | 1.822     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.o_proj     | \u001b[92m0.00020802\u001b[0m | 1           | 0.01000     | 1.746     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.up_proj          | \u001b[92m0.01241853\u001b[0m | 1           | 0.01000     | 1.902     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.gate_proj        | \u001b[92m0.01477227\u001b[0m | 1           | 0.01000     | 1.927     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.down_proj        | \u001b[92m0.00742236\u001b[0m | 1           | 0.01000     | 6.928     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.k_proj     | \u001b[92m0.00283143\u001b[0m | 1           | 0.01000     | 2.011     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.v_proj     | \u001b[92m0.00241041\u001b[0m | 1           | 0.01000     | 1.774     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.q_proj     | \u001b[92m0.01049083\u001b[0m | 1           | 0.01000     | 1.846     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.o_proj     | \u001b[92m0.00081422\u001b[0m | 1           | 0.01000     | 1.745     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.up_proj          | \u001b[92m0.01450003\u001b[0m | 1           | 0.01000     | 2.060     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.gate_proj        | \u001b[92m0.01652286\u001b[0m | 1           | 0.01000     | 1.990     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.down_proj        | \u001b[92m0.00134774\u001b[0m | 1           | 0.01000     | 6.648     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.k_proj     | \u001b[92m0.00317643\u001b[0m | 1           | 0.01000     | 1.935     | 0.017        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.v_proj     | \u001b[92m0.00116977\u001b[0m | 1           | 0.01000     | 1.883     | 0.017        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.q_proj     | \u001b[92m0.01037796\u001b[0m | 1           | 0.01000     | 1.950     | 0.017        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.o_proj     | \u001b[92m0.00107454\u001b[0m | 1           | 0.01000     | 2.026     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.up_proj          | \u001b[92m0.01516681\u001b[0m | 1           | 0.01000     | 2.208     | 0.032        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.gate_proj        | \u001b[92m0.02286477\u001b[0m | 1           | 0.01000     | 2.151     | 0.032        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.down_proj        | \u001b[92m0.00490850\u001b[0m | 1           | 0.01000     | 6.683     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.k_proj', 'loss': '0.00051882', 'samples': '1', 'damp': '0.01000', 'time': '2.063', 'fwd_time': '0.374'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.v_proj', 'loss': '0.00001211', 'samples': '1', 'damp': '0.01000', 'time': '1.932', 'fwd_time': '0.374'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.q_proj', 'loss': '0.00092513', 'samples': '1', 'damp': '0.01000', 'time': '1.890', 'fwd_time': '0.374'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.o_proj', 'loss': '0.00000089', 'samples': '1', 'damp': '0.01000', 'time': '1.797', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.up_proj', 'loss': '0.00051592', 'samples': '1', 'damp': '0.01000', 'time': '2.190', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.gate_proj', 'loss': '0.00051009', 'samples': '1', 'damp': '0.01000', 'time': '2.051', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.down_proj', 'loss': '0.00278950', 'samples': '1', 'damp': '0.01000', 'time': '6.447', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.k_proj', 'loss': '0.00080696', 'samples': '1', 'damp': '0.01000', 'time': '1.880', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.v_proj', 'loss': '0.00004102', 'samples': '1', 'damp': '0.01000', 'time': '1.815', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.q_proj', 'loss': '0.00287710', 'samples': '1', 'damp': '0.01000', 'time': '1.921', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.o_proj', 'loss': '0.00000168', 'samples': '1', 'damp': '0.01000', 'time': '1.869', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.up_proj', 'loss': '0.00089655', 'samples': '1', 'damp': '0.01000', 'time': '1.953', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.gate_proj', 'loss': '0.00099165', 'samples': '1', 'damp': '0.01000', 'time': '1.867', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.down_proj', 'loss': '0.00001827', 'samples': '1', 'damp': '0.01000', 'time': '6.010', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.k_proj', 'loss': '0.00222330', 'samples': '1', 'damp': '0.01000', 'time': '1.675', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.v_proj', 'loss': '0.00009904', 'samples': '1', 'damp': '0.01000', 'time': '1.593', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.q_proj', 'loss': '0.00606226', 'samples': '1', 'damp': '0.01000', 'time': '1.682', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.o_proj', 'loss': '0.00000809', 'samples': '1', 'damp': '0.01000', 'time': '1.651', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.up_proj', 'loss': '0.00122875', 'samples': '1', 'damp': '0.01000', 'time': '1.973', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.gate_proj', 'loss': '0.00133369', 'samples': '1', 'damp': '0.01000', 'time': '1.883', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.down_proj', 'loss': '0.00000678', 'samples': '1', 'damp': '0.01000', 'time': '6.009', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.k_proj', 'loss': '0.00169049', 'samples': '1', 'damp': '0.01000', 'time': '1.723', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.v_proj', 'loss': '0.00013950', 'samples': '1', 'damp': '0.01000', 'time': '1.666', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.q_proj', 'loss': '0.00548247', 'samples': '1', 'damp': '0.01000', 'time': '1.710', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.o_proj', 'loss': '0.00000595', 'samples': '1', 'damp': '0.01000', 'time': '1.660', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.up_proj', 'loss': '0.00155039', 'samples': '1', 'damp': '0.01000', 'time': '2.054', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.gate_proj', 'loss': '0.00185651', 'samples': '1', 'damp': '0.01000', 'time': '2.044', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.down_proj', 'loss': '0.00000669', 'samples': '1', 'damp': '0.01000', 'time': '6.610', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.k_proj', 'loss': '0.00254361', 'samples': '1', 'damp': '0.01000', 'time': '2.327', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.v_proj', 'loss': '0.00015291', 'samples': '1', 'damp': '0.01000', 'time': '2.327', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.q_proj', 'loss': '0.00657682', 'samples': '1', 'damp': '0.01000', 'time': '1.943', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.o_proj', 'loss': '0.00000406', 'samples': '1', 'damp': '0.01000', 'time': '1.762', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.up_proj', 'loss': '0.00182080', 'samples': '1', 'damp': '0.01000', 'time': '1.871', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.gate_proj', 'loss': '0.00216327', 'samples': '1', 'damp': '0.01000', 'time': '1.969', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.down_proj', 'loss': '0.00000972', 'samples': '1', 'damp': '0.01000', 'time': '6.661', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.k_proj', 'loss': '0.00248353', 'samples': '1', 'damp': '0.01000', 'time': '1.778', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.v_proj', 'loss': '0.00014576', 'samples': '1', 'damp': '0.01000', 'time': '1.773', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.q_proj', 'loss': '0.00640778', 'samples': '1', 'damp': '0.01000', 'time': '2.001', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.o_proj', 'loss': '0.00000331', 'samples': '1', 'damp': '0.01000', 'time': '1.854', 'fwd_time': '0.005'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.up_proj', 'loss': '0.00217219', 'samples': '1', 'damp': '0.01000', 'time': '2.105', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.gate_proj', 'loss': '0.00255769', 'samples': '1', 'damp': '0.01000', 'time': '1.919', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.down_proj', 'loss': '0.00001137', 'samples': '1', 'damp': '0.01000', 'time': '6.763', 'fwd_time': '0.013'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.k_proj', 'loss': '0.00340194', 'samples': '1', 'damp': '0.01000', 'time': '1.907', 'fwd_time': '0.019'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.v_proj', 'loss': '0.00016664', 'samples': '1', 'damp': '0.01000', 'time': '1.745', 'fwd_time': '0.019'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.q_proj', 'loss': '0.00781302', 'samples': '1', 'damp': '0.01000', 'time': '1.808', 'fwd_time': '0.019'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.o_proj', 'loss': '0.00000411', 'samples': '1', 'damp': '0.01000', 'time': '1.950', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.up_proj', 'loss': '0.00260401', 'samples': '1', 'damp': '0.01000', 'time': '2.352', 'fwd_time': '0.032'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.gate_proj', 'loss': '0.00322510', 'samples': '1', 'damp': '0.01000', 'time': '2.015', 'fwd_time': '0.032'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.down_proj', 'loss': '0.00001958', 'samples': '1', 'damp': '0.01000', 'time': '6.225', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.k_proj', 'loss': '0.00279211', 'samples': '1', 'damp': '0.01000', 'time': '1.729', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.v_proj', 'loss': '0.00016039', 'samples': '1', 'damp': '0.01000', 'time': '1.679', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.q_proj', 'loss': '0.00736329', 'samples': '1', 'damp': '0.01000', 'time': '1.920', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.o_proj', 'loss': '0.00000452', 'samples': '1', 'damp': '0.01000', 'time': '1.912', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.up_proj', 'loss': '0.00271578', 'samples': '1', 'damp': '0.01000', 'time': '2.286', 'fwd_time': '0.032'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.gate_proj', 'loss': '0.00341759', 'samples': '1', 'damp': '0.01000', 'time': '1.976', 'fwd_time': '0.032'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.down_proj', 'loss': '0.00002443', 'samples': '1', 'damp': '0.01000', 'time': '6.346', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.k_proj', 'loss': '0.00369120', 'samples': '1', 'damp': '0.01000', 'time': '1.803', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.v_proj', 'loss': '0.00015656', 'samples': '1', 'damp': '0.01000', 'time': '1.681', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.q_proj', 'loss': '0.00896131', 'samples': '1', 'damp': '0.01000', 'time': '1.777', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.o_proj', 'loss': '0.00003319', 'samples': '1', 'damp': '0.01000', 'time': '1.761', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.up_proj', 'loss': '0.00317500', 'samples': '1', 'damp': '0.01000', 'time': '1.930', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.gate_proj', 'loss': '0.00363738', 'samples': '1', 'damp': '0.01000', 'time': '1.926', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.down_proj', 'loss': '0.00002674', 'samples': '1', 'damp': '0.01000', 'time': '6.273', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.k_proj', 'loss': '0.00265515', 'samples': '1', 'damp': '0.01000', 'time': '1.774', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.v_proj', 'loss': '0.00019853', 'samples': '1', 'damp': '0.01000', 'time': '1.777', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.q_proj', 'loss': '0.00764772', 'samples': '1', 'damp': '0.01000', 'time': '1.764', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.o_proj', 'loss': '0.00002721', 'samples': '1', 'damp': '0.01000', 'time': '1.773', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.up_proj', 'loss': '0.00339501', 'samples': '1', 'damp': '0.01000', 'time': '2.006', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.gate_proj', 'loss': '0.00391460', 'samples': '1', 'damp': '0.01000', 'time': '2.013', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.down_proj', 'loss': '0.00002955', 'samples': '1', 'damp': '0.01000', 'time': '6.382', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.k_proj', 'loss': '0.00342138', 'samples': '1', 'damp': '0.01000', 'time': '1.788', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.v_proj', 'loss': '0.00022896', 'samples': '1', 'damp': '0.01000', 'time': '1.672', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.q_proj', 'loss': '0.00944302', 'samples': '1', 'damp': '0.01000', 'time': '1.791', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.o_proj', 'loss': '0.00004620', 'samples': '1', 'damp': '0.01000', 'time': '1.734', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.up_proj', 'loss': '0.00371678', 'samples': '1', 'damp': '0.01000', 'time': '1.940', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.gate_proj', 'loss': '0.00494442', 'samples': '1', 'damp': '0.01000', 'time': '1.932', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.down_proj', 'loss': '0.00004693', 'samples': '1', 'damp': '0.01000', 'time': '6.318', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.k_proj', 'loss': '0.00348900', 'samples': '1', 'damp': '0.01000', 'time': '1.807', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.v_proj', 'loss': '0.00030692', 'samples': '1', 'damp': '0.01000', 'time': '1.689', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.q_proj', 'loss': '0.00846503', 'samples': '1', 'damp': '0.01000', 'time': '1.751', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.o_proj', 'loss': '0.00005786', 'samples': '1', 'damp': '0.01000', 'time': '1.646', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.up_proj', 'loss': '0.00416755', 'samples': '1', 'damp': '0.01000', 'time': '1.899', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.gate_proj', 'loss': '0.00604209', 'samples': '1', 'damp': '0.01000', 'time': '2.008', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.down_proj', 'loss': '0.00004941', 'samples': '1', 'damp': '0.01000', 'time': '6.732', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.k_proj', 'loss': '0.00364284', 'samples': '1', 'damp': '0.01000', 'time': '1.854', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.v_proj', 'loss': '0.00027547', 'samples': '1', 'damp': '0.01000', 'time': '1.689', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.q_proj', 'loss': '0.00906908', 'samples': '1', 'damp': '0.01000', 'time': '1.747', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.o_proj', 'loss': '0.00003425', 'samples': '1', 'damp': '0.01000', 'time': '1.733', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.up_proj', 'loss': '0.00446561', 'samples': '1', 'damp': '0.01000', 'time': '1.969', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.gate_proj', 'loss': '0.00655250', 'samples': '1', 'damp': '0.01000', 'time': '1.921', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.down_proj', 'loss': '0.00006024', 'samples': '1', 'damp': '0.01000', 'time': '6.232', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.k_proj', 'loss': '0.00375506', 'samples': '1', 'damp': '0.01000', 'time': '1.785', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.v_proj', 'loss': '0.00022585', 'samples': '1', 'damp': '0.01000', 'time': '1.668', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.q_proj', 'loss': '0.00889064', 'samples': '1', 'damp': '0.01000', 'time': '1.733', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.o_proj', 'loss': '0.00005738', 'samples': '1', 'damp': '0.01000', 'time': '2.090', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.up_proj', 'loss': '0.00478764', 'samples': '1', 'damp': '0.01000', 'time': '2.148', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.gate_proj', 'loss': '0.00635924', 'samples': '1', 'damp': '0.01000', 'time': '2.005', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.down_proj', 'loss': '0.00007076', 'samples': '1', 'damp': '0.01000', 'time': '6.382', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.k_proj', 'loss': '0.00388607', 'samples': '1', 'damp': '0.01000', 'time': '1.772', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.v_proj', 'loss': '0.00035866', 'samples': '1', 'damp': '0.01000', 'time': '1.720', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.q_proj', 'loss': '0.01182740', 'samples': '1', 'damp': '0.01000', 'time': '1.729', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.o_proj', 'loss': '0.00012804', 'samples': '1', 'damp': '0.01000', 'time': '1.786', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.up_proj', 'loss': '0.00581007', 'samples': '1', 'damp': '0.01000', 'time': '1.943', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.gate_proj', 'loss': '0.00780590', 'samples': '1', 'damp': '0.01000', 'time': '1.980', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.down_proj', 'loss': '0.00013223', 'samples': '1', 'damp': '0.01000', 'time': '6.430', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.k_proj', 'loss': '0.00380934', 'samples': '1', 'damp': '0.01000', 'time': '1.691', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.v_proj', 'loss': '0.00042987', 'samples': '1', 'damp': '0.01000', 'time': '1.636', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.q_proj', 'loss': '0.01007403', 'samples': '1', 'damp': '0.01000', 'time': '1.675', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.o_proj', 'loss': '0.00014636', 'samples': '1', 'damp': '0.01000', 'time': '1.678', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.up_proj', 'loss': '0.00656036', 'samples': '1', 'damp': '0.01000', 'time': '1.977', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.gate_proj', 'loss': '0.00833433', 'samples': '1', 'damp': '0.01000', 'time': '1.957', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.down_proj', 'loss': '0.00018452', 'samples': '1', 'damp': '0.01000', 'time': '6.347', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.k_proj', 'loss': '0.00381659', 'samples': '1', 'damp': '0.01000', 'time': '1.820', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.v_proj', 'loss': '0.00047409', 'samples': '1', 'damp': '0.01000', 'time': '1.842', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.q_proj', 'loss': '0.00972593', 'samples': '1', 'damp': '0.01000', 'time': '1.794', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.o_proj', 'loss': '0.00013745', 'samples': '1', 'damp': '0.01000', 'time': '1.779', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.up_proj', 'loss': '0.00765137', 'samples': '1', 'damp': '0.01000', 'time': '2.086', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.gate_proj', 'loss': '0.00964348', 'samples': '1', 'damp': '0.01000', 'time': '2.027', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.down_proj', 'loss': '0.00023123', 'samples': '1', 'damp': '0.01000', 'time': '6.397', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.k_proj', 'loss': '0.00383661', 'samples': '1', 'damp': '0.01000', 'time': '1.840', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.v_proj', 'loss': '0.00056874', 'samples': '1', 'damp': '0.01000', 'time': '1.727', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.q_proj', 'loss': '0.00994518', 'samples': '1', 'damp': '0.01000', 'time': '1.795', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.o_proj', 'loss': '0.00011730', 'samples': '1', 'damp': '0.01000', 'time': '1.789', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.up_proj', 'loss': '0.00936889', 'samples': '1', 'damp': '0.01000', 'time': '2.064', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.gate_proj', 'loss': '0.01210778', 'samples': '1', 'damp': '0.01000', 'time': '1.966', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.down_proj', 'loss': '0.00036333', 'samples': '1', 'damp': '0.01000', 'time': '6.441', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.k_proj', 'loss': '0.00344781', 'samples': '1', 'damp': '0.01000', 'time': '1.765', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.v_proj', 'loss': '0.00080013', 'samples': '1', 'damp': '0.01000', 'time': '1.731', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.q_proj', 'loss': '0.01183529', 'samples': '1', 'damp': '0.01000', 'time': '1.803', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.o_proj', 'loss': '0.00012906', 'samples': '1', 'damp': '0.01000', 'time': '1.807', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.up_proj', 'loss': '0.01062711', 'samples': '1', 'damp': '0.01000', 'time': '1.966', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.gate_proj', 'loss': '0.01332856', 'samples': '1', 'damp': '0.01000', 'time': '1.950', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.down_proj', 'loss': '0.00037857', 'samples': '1', 'damp': '0.01000', 'time': '7.139', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.k_proj', 'loss': '0.00363782', 'samples': '1', 'damp': '0.01000', 'time': '1.830', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.v_proj', 'loss': '0.00060554', 'samples': '1', 'damp': '0.01000', 'time': '1.671', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.q_proj', 'loss': '0.00974268', 'samples': '1', 'damp': '0.01000', 'time': '1.822', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.o_proj', 'loss': '0.00020802', 'samples': '1', 'damp': '0.01000', 'time': '1.746', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.up_proj', 'loss': '0.01241853', 'samples': '1', 'damp': '0.01000', 'time': '1.902', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.gate_proj', 'loss': '0.01477227', 'samples': '1', 'damp': '0.01000', 'time': '1.927', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.down_proj', 'loss': '0.00742236', 'samples': '1', 'damp': '0.01000', 'time': '6.928', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.k_proj', 'loss': '0.00283143', 'samples': '1', 'damp': '0.01000', 'time': '2.011', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.v_proj', 'loss': '0.00241041', 'samples': '1', 'damp': '0.01000', 'time': '1.774', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.q_proj', 'loss': '0.01049083', 'samples': '1', 'damp': '0.01000', 'time': '1.846', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.o_proj', 'loss': '0.00081422', 'samples': '1', 'damp': '0.01000', 'time': '1.745', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.up_proj', 'loss': '0.01450003', 'samples': '1', 'damp': '0.01000', 'time': '2.060', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.gate_proj', 'loss': '0.01652286', 'samples': '1', 'damp': '0.01000', 'time': '1.990', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.down_proj', 'loss': '0.00134774', 'samples': '1', 'damp': '0.01000', 'time': '6.648', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.k_proj', 'loss': '0.00317643', 'samples': '1', 'damp': '0.01000', 'time': '1.935', 'fwd_time': '0.017'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.v_proj', 'loss': '0.00116977', 'samples': '1', 'damp': '0.01000', 'time': '1.883', 'fwd_time': '0.017'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.q_proj', 'loss': '0.01037796', 'samples': '1', 'damp': '0.01000', 'time': '1.950', 'fwd_time': '0.017'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.o_proj', 'loss': '0.00107454', 'samples': '1', 'damp': '0.01000', 'time': '2.026', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.up_proj', 'loss': '0.01516681', 'samples': '1', 'damp': '0.01000', 'time': '2.208', 'fwd_time': '0.032'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.gate_proj', 'loss': '0.02286477', 'samples': '1', 'damp': '0.01000', 'time': '2.151', 'fwd_time': '0.032'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.down_proj', 'loss': '0.00490850', 'samples': '1', 'damp': '0.01000', 'time': '6.683', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                         \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`      \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                               \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                  \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                            0%\n",
      "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v2 to v1                                         \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:2.2.0\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0\n",
      "  }\n",
      "}\n",
      "Files in directory:\n",
      "config.json\n",
      "generation_config.json\n",
      "quantize_config.json\n",
      "quant_log.csv\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"bos_token_id\": 1,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"max_length\": 2048,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"transformers_version\": \"4.53.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"architectures\": [\n",
      "        \"LlamaForCausalLM\"\n",
      "    ],\n",
      "    \"attention_bias\": false,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bos_token_id\": 1,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"head_dim\": 64,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 2048,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 5632,\n",
      "    \"max_position_embeddings\": 2048,\n",
      "    \"mlp_bias\": false,\n",
      "    \"model_type\": \"llama\",\n",
      "    \"num_attention_heads\": 32,\n",
      "    \"num_hidden_layers\": 22,\n",
      "    \"num_key_value_heads\": 4,\n",
      "    \"pretraining_tp\": 1,\n",
      "    \"quantization_config\": {\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:2.2.0\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\"\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true\n",
      "    },\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"rope_scaling\": null,\n",
      "    \"rope_theta\": 10000.0,\n",
      "    \"tie_word_embeddings\": false,\n",
      "    \"torch_dtype\": \"float16\",\n",
      "    \"transformers_version\": \"4.53.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 32000\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 2098.20MB, 2.05GB                              \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 1197.41MB, 1.17GB                                  \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 900.79MB, 0.88GB - 42.93%                               \n"
     ]
    }
   ],
   "source": [
    "QuantizeModelandSaveP(quantize_config_8, output_dir_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5911dd4e",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 4.2875 bpw, based on [bits: 4, group_size: 128]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native float16): `torch.float16`                     6%\n",
      "Quantizing mlp.gate_proj in layer     [2 of 21] | 12:20:41 / 3 days, 18:31:40 [3/22] 13.6%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=0 (token='<unk>').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 2048,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                   6%\n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`      6%\n",
      "\u001b[33mWARN\u001b[0m  Calibration dataset size should be more than 256. Current: 1.            6%\n",
      "\u001b[33mWARN\u001b[0m  The average length of input_ids of calibration_dataset should be greater than 256: actual avg: 31.0.\n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_cystocolostomy_time_07_29_2025_09h_47m_02s.log`\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.k_proj     | \u001b[96m0.14800051\u001b[0m | 1           | 0.01000     | 2.137     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.v_proj     | \u001b[92m0.00347352\u001b[0m | 1           | 0.01000     | 2.284     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.q_proj     | \u001b[96m0.26973695\u001b[0m | 1           | 0.01000     | 1.980     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.o_proj     | \u001b[92m0.00025855\u001b[0m | 1           | 0.01000     | 1.963     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.up_proj          | \u001b[96m0.14944430\u001b[0m | 1           | 0.01000     | 2.248     | 0.032        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.gate_proj        | \u001b[96m0.14762200\u001b[0m | 1           | 0.01000     | 2.097     | 0.032        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.down_proj        | \u001b[96m0.82560241\u001b[0m | 1           | 0.01000     | 6.596     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.k_proj     | \u001b[96m0.23195116\u001b[0m | 1           | 0.01000     | 1.901     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.v_proj     | \u001b[92m0.01184271\u001b[0m | 1           | 0.01000     | 1.800     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.q_proj     | \u001b[96m0.83455884\u001b[0m | 1           | 0.01000     | 1.888     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.o_proj     | \u001b[92m0.00048424\u001b[0m | 1           | 0.01000     | 1.950     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.up_proj          | \u001b[96m0.26084259\u001b[0m | 1           | 0.01000     | 2.033     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.gate_proj        | \u001b[96m0.28741685\u001b[0m | 1           | 0.01000     | 1.996     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.down_proj        | \u001b[92m0.00529797\u001b[0m | 1           | 0.01000     | 6.845     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.k_proj     | \u001b[96m0.64601654\u001b[0m | 1           | 0.01000     | 1.936     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.v_proj     | \u001b[92m0.02856080\u001b[0m | 1           | 0.01000     | 1.749     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.q_proj     | \u001b[93m1.76246715\u001b[0m | 1           | 0.01000     | 1.866     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.o_proj     | \u001b[92m0.00235459\u001b[0m | 1           | 0.01000     | 2.347     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.up_proj          | \u001b[96m0.35690558\u001b[0m | 1           | 0.01000     | 2.275     | 0.032        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.gate_proj        | \u001b[96m0.38693184\u001b[0m | 1           | 0.01000     | 2.096     | 0.032        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.down_proj        | \u001b[92m0.00197021\u001b[0m | 1           | 0.01000     | 6.694     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.k_proj     | \u001b[96m0.49117780\u001b[0m | 1           | 0.01000     | 1.808     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.v_proj     | \u001b[92m0.03981943\u001b[0m | 1           | 0.01000     | 1.912     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.q_proj     | \u001b[93m1.59195030\u001b[0m | 1           | 0.01000     | 2.059     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.o_proj     | \u001b[92m0.00174010\u001b[0m | 1           | 0.01000     | 1.933     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.up_proj          | \u001b[96m0.44945088\u001b[0m | 1           | 0.01000     | 2.114     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.gate_proj        | \u001b[96m0.53812623\u001b[0m | 1           | 0.01000     | 2.038     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.down_proj        | \u001b[92m0.00193861\u001b[0m | 1           | 0.01000     | 6.453     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.k_proj     | \u001b[96m0.73155057\u001b[0m | 1           | 0.01000     | 1.805     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.v_proj     | \u001b[92m0.04405088\u001b[0m | 1           | 0.01000     | 1.811     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.q_proj     | \u001b[93m1.91151834\u001b[0m | 1           | 0.01000     | 1.915     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.o_proj     | \u001b[92m0.00118212\u001b[0m | 1           | 0.01000     | 1.873     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.up_proj          | \u001b[96m0.52675408\u001b[0m | 1           | 0.01000     | 2.063     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.gate_proj        | \u001b[96m0.62512434\u001b[0m | 1           | 0.01000     | 2.064     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.down_proj        | \u001b[92m0.00281798\u001b[0m | 1           | 0.01000     | 6.644     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.k_proj     | \u001b[96m0.71909732\u001b[0m | 1           | 0.01000     | 1.820     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.v_proj     | \u001b[92m0.04187158\u001b[0m | 1           | 0.01000     | 1.715     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.q_proj     | \u001b[93m1.86984372\u001b[0m | 1           | 0.01000     | 1.872     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.o_proj     | \u001b[92m0.00095670\u001b[0m | 1           | 0.01000     | 1.869     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.up_proj          | \u001b[96m0.62906682\u001b[0m | 1           | 0.01000     | 2.030     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.gate_proj        | \u001b[96m0.74063522\u001b[0m | 1           | 0.01000     | 1.963     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.down_proj        | \u001b[92m0.00328434\u001b[0m | 1           | 0.01000     | 6.526     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.k_proj     | \u001b[96m0.98030949\u001b[0m | 1           | 0.01000     | 1.881     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.v_proj     | \u001b[92m0.04793773\u001b[0m | 1           | 0.01000     | 1.814     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.q_proj     | \u001b[93m2.27248144\u001b[0m | 1           | 0.01000     | 1.982     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.o_proj     | \u001b[92m0.00118265\u001b[0m | 1           | 0.01000     | 1.875     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.up_proj          | \u001b[96m0.75285542\u001b[0m | 1           | 0.01000     | 1.977     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.gate_proj        | \u001b[96m0.93193400\u001b[0m | 1           | 0.01000     | 2.007     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.down_proj        | \u001b[92m0.00564729\u001b[0m | 1           | 0.01000     | 6.395     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.k_proj     | \u001b[96m0.81816399\u001b[0m | 1           | 0.01000     | 1.829     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.v_proj     | \u001b[92m0.04599106\u001b[0m | 1           | 0.01000     | 1.750     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.q_proj     | \u001b[93m2.14848042\u001b[0m | 1           | 0.01000     | 1.848     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.o_proj     | \u001b[92m0.00129719\u001b[0m | 1           | 0.01000     | 1.783     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.up_proj          | \u001b[96m0.78806174\u001b[0m | 1           | 0.01000     | 2.049     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.gate_proj        | \u001b[96m0.99281961\u001b[0m | 1           | 0.01000     | 2.030     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.down_proj        | \u001b[92m0.00704466\u001b[0m | 1           | 0.01000     | 6.523     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.k_proj     | \u001b[93m1.06420100\u001b[0m | 1           | 0.01000     | 1.801     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.v_proj     | \u001b[92m0.04508825\u001b[0m | 1           | 0.01000     | 1.751     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.q_proj     | \u001b[93m2.59440231\u001b[0m | 1           | 0.01000     | 1.856     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.o_proj     | \u001b[92m0.00967510\u001b[0m | 1           | 0.01000     | 1.797     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.up_proj          | \u001b[96m0.92008734\u001b[0m | 1           | 0.01000     | 2.024     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.gate_proj        | \u001b[93m1.05250621\u001b[0m | 1           | 0.01000     | 1.981     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.down_proj        | \u001b[92m0.00770114\u001b[0m | 1           | 0.01000     | 6.837     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.k_proj     | \u001b[96m0.76577234\u001b[0m | 1           | 0.01000     | 1.835     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.v_proj     | \u001b[92m0.05711044\u001b[0m | 1           | 0.01000     | 1.788     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.q_proj     | \u001b[93m2.20550013\u001b[0m | 1           | 0.01000     | 1.772     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.o_proj     | \u001b[92m0.00782862\u001b[0m | 1           | 0.01000     | 1.809     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.up_proj          | \u001b[96m0.98110312\u001b[0m | 1           | 0.01000     | 2.339     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.gate_proj        | \u001b[93m1.13231170\u001b[0m | 1           | 0.01000     | 2.253     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.down_proj        | \u001b[92m0.00831588\u001b[0m | 1           | 0.01000     | 6.490     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.k_proj     | \u001b[96m0.99576122\u001b[0m | 1           | 0.01000     | 1.826     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.v_proj     | \u001b[92m0.06612855\u001b[0m | 1           | 0.01000     | 1.777     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.q_proj     | \u001b[93m2.73123789\u001b[0m | 1           | 0.01000     | 1.780     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.o_proj     | \u001b[92m0.01345933\u001b[0m | 1           | 0.01000     | 1.780     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.up_proj          | \u001b[93m1.07801199\u001b[0m | 1           | 0.01000     | 2.039     | 0.017        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.gate_proj        | \u001b[93m1.43164611\u001b[0m | 1           | 0.01000     | 2.004     | 0.017        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.down_proj        | \u001b[92m0.01344320\u001b[0m | 1           | 0.01000     | 6.518     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.k_proj     | \u001b[93m1.01412916\u001b[0m | 1           | 0.01000     | 1.887     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.v_proj     | \u001b[92m0.08740665\u001b[0m | 1           | 0.01000     | 1.787     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.q_proj     | \u001b[93m2.44200945\u001b[0m | 1           | 0.01000     | 1.901     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.o_proj     | \u001b[92m0.01656007\u001b[0m | 1           | 0.01000     | 1.774     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.up_proj          | \u001b[93m1.20504045\u001b[0m | 1           | 0.01000     | 2.045     | 0.017        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.gate_proj        | \u001b[93m1.74824977\u001b[0m | 1           | 0.01000     | 2.047     | 0.017        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.down_proj        | \u001b[92m0.01409362\u001b[0m | 1           | 0.01000     | 6.542     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.k_proj     | \u001b[93m1.04128623\u001b[0m | 1           | 0.01000     | 1.792     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.v_proj     | \u001b[92m0.07925136\u001b[0m | 1           | 0.01000     | 1.725     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.q_proj     | \u001b[93m2.63220978\u001b[0m | 1           | 0.01000     | 1.799     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.o_proj     | \u001b[92m0.01018958\u001b[0m | 1           | 0.01000     | 1.788     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.up_proj          | \u001b[93m1.29685450\u001b[0m | 1           | 0.01000     | 2.109     | 0.032        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.gate_proj        | \u001b[93m1.90123272\u001b[0m | 1           | 0.01000     | 1.935     | 0.032        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.down_proj        | \u001b[92m0.01765139\u001b[0m | 1           | 0.01000     | 6.987     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.k_proj     | \u001b[93m1.08468103\u001b[0m | 1           | 0.01000     | 1.792     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.v_proj     | \u001b[92m0.06495653\u001b[0m | 1           | 0.01000     | 1.706     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.q_proj     | \u001b[93m2.57177019\u001b[0m | 1           | 0.01000     | 1.853     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.o_proj     | \u001b[92m0.01693052\u001b[0m | 1           | 0.01000     | 1.773     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.up_proj          | \u001b[93m1.38495171\u001b[0m | 1           | 0.01000     | 1.976     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.gate_proj        | \u001b[93m1.83714008\u001b[0m | 1           | 0.01000     | 2.087     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.down_proj        | \u001b[92m0.02041383\u001b[0m | 1           | 0.01000     | 6.767     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.k_proj     | \u001b[93m1.12995791\u001b[0m | 1           | 0.01000     | 1.915     | 0.013        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.v_proj     | \u001b[96m0.10309613\u001b[0m | 1           | 0.01000     | 1.828     | 0.013        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.q_proj     | \u001b[93m3.42403150\u001b[0m | 1           | 0.01000     | 1.820     | 0.013        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.o_proj     | \u001b[92m0.03971560\u001b[0m | 1           | 0.01000     | 1.837     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.up_proj          | \u001b[93m1.67907703\u001b[0m | 1           | 0.01000     | 2.046     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.gate_proj        | \u001b[93m2.25721693\u001b[0m | 1           | 0.01000     | 1.939     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.down_proj        | \u001b[92m0.03909773\u001b[0m | 1           | 0.01000     | 6.829     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.k_proj     | \u001b[93m1.10756350\u001b[0m | 1           | 0.01000     | 1.922     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.v_proj     | \u001b[96m0.12275578\u001b[0m | 1           | 0.01000     | 1.772     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.q_proj     | \u001b[93m2.90932488\u001b[0m | 1           | 0.01000     | 1.977     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.o_proj     | \u001b[92m0.04257073\u001b[0m | 1           | 0.01000     | 1.749     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.up_proj          | \u001b[93m1.90927470\u001b[0m | 1           | 0.01000     | 2.041     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.gate_proj        | \u001b[93m2.42662287\u001b[0m | 1           | 0.01000     | 2.014     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.down_proj        | \u001b[92m0.05367062\u001b[0m | 1           | 0.01000     | 6.769     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.k_proj     | \u001b[93m1.11556125\u001b[0m | 1           | 0.01000     | 1.812     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.v_proj     | \u001b[96m0.13731849\u001b[0m | 1           | 0.01000     | 1.779     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.q_proj     | \u001b[93m2.82507801\u001b[0m | 1           | 0.01000     | 1.835     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.o_proj     | \u001b[92m0.03849703\u001b[0m | 1           | 0.01000     | 1.827     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.up_proj          | \u001b[93m2.23409653\u001b[0m | 1           | 0.01000     | 2.191     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.gate_proj        | \u001b[93m2.81495595\u001b[0m | 1           | 0.01000     | 2.184     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.down_proj        | \u001b[92m0.06671679\u001b[0m | 1           | 0.01000     | 6.698     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.k_proj     | \u001b[93m1.10021424\u001b[0m | 1           | 0.01000     | 1.783     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.v_proj     | \u001b[96m0.16238821\u001b[0m | 1           | 0.01000     | 1.737     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.q_proj     | \u001b[93m2.86855793\u001b[0m | 1           | 0.01000     | 1.844     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.o_proj     | \u001b[92m0.03369877\u001b[0m | 1           | 0.01000     | 1.866     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.up_proj          | \u001b[93m2.70153141\u001b[0m | 1           | 0.01000     | 2.019     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.gate_proj        | \u001b[93m3.49715519\u001b[0m | 1           | 0.01000     | 2.027     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.down_proj        | \u001b[96m0.10559007\u001b[0m | 1           | 0.01000     | 6.695     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.k_proj     | \u001b[96m0.99162185\u001b[0m | 1           | 0.01000     | 1.923     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.v_proj     | \u001b[96m0.22864181\u001b[0m | 1           | 0.01000     | 1.762     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.q_proj     | \u001b[93m3.42728829\u001b[0m | 1           | 0.01000     | 1.812     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.o_proj     | \u001b[92m0.03737143\u001b[0m | 1           | 0.01000     | 1.847     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.up_proj          | \u001b[93m3.03627491\u001b[0m | 1           | 0.01000     | 2.009     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.gate_proj        | \u001b[93m3.81179214\u001b[0m | 1           | 0.01000     | 2.007     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.down_proj        | \u001b[96m0.10912511\u001b[0m | 1           | 0.01000     | 6.853     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.k_proj     | \u001b[93m1.05409360\u001b[0m | 1           | 0.01000     | 1.895     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.v_proj     | \u001b[96m0.17521822\u001b[0m | 1           | 0.01000     | 2.550     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.q_proj     | \u001b[93m2.80833149\u001b[0m | 1           | 0.01000     | 1.977     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.o_proj     | \u001b[92m0.05945527\u001b[0m | 1           | 0.01000     | 1.822     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.up_proj          | \u001b[93m3.59295082\u001b[0m | 1           | 0.01000     | 2.098     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.gate_proj        | \u001b[93m4.27605867\u001b[0m | 1           | 0.01000     | 1.987     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.down_proj        | \u001b[93m2.03395867\u001b[0m | 1           | 0.01000     | 7.063     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.k_proj     | \u001b[96m0.81634945\u001b[0m | 1           | 0.01000     | 1.790     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.v_proj     | \u001b[96m0.70448846\u001b[0m | 1           | 0.01000     | 1.829     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.q_proj     | \u001b[93m3.03732157\u001b[0m | 1           | 0.01000     | 1.824     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.o_proj     | \u001b[96m0.22671530\u001b[0m | 1           | 0.01000     | 1.756     | 0.005        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.up_proj          | \u001b[93m4.16390419\u001b[0m | 1           | 0.01000     | 1.971     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.gate_proj        | \u001b[93m4.75274754\u001b[0m | 1           | 0.01000     | 1.962     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.down_proj        | \u001b[96m0.38828292\u001b[0m | 1           | 0.01000     | 6.657     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.k_proj     | \u001b[96m0.91655505\u001b[0m | 1           | 0.01000     | 1.925     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.v_proj     | \u001b[96m0.33506715\u001b[0m | 1           | 0.01000     | 2.019     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.q_proj     | \u001b[93m2.98317146\u001b[0m | 1           | 0.01000     | 2.107     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.o_proj     | \u001b[96m0.30907726\u001b[0m | 1           | 0.01000     | 1.981     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.up_proj          | \u001b[93m4.34757805\u001b[0m | 1           | 0.01000     | 2.269     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.gate_proj        | \u001b[33m6.55129242\u001b[0m | 1           | 0.01000     | 2.044     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.down_proj        | \u001b[93m1.41580963\u001b[0m | 1           | 0.01000     | 6.622     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.k_proj', 'loss': '0.14800051', 'samples': '1', 'damp': '0.01000', 'time': '2.137', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.v_proj', 'loss': '0.00347352', 'samples': '1', 'damp': '0.01000', 'time': '2.284', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.q_proj', 'loss': '0.26973695', 'samples': '1', 'damp': '0.01000', 'time': '1.980', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.o_proj', 'loss': '0.00025855', 'samples': '1', 'damp': '0.01000', 'time': '1.963', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.up_proj', 'loss': '0.14944430', 'samples': '1', 'damp': '0.01000', 'time': '2.248', 'fwd_time': '0.032'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.gate_proj', 'loss': '0.14762200', 'samples': '1', 'damp': '0.01000', 'time': '2.097', 'fwd_time': '0.032'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.down_proj', 'loss': '0.82560241', 'samples': '1', 'damp': '0.01000', 'time': '6.596', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.k_proj', 'loss': '0.23195116', 'samples': '1', 'damp': '0.01000', 'time': '1.901', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.v_proj', 'loss': '0.01184271', 'samples': '1', 'damp': '0.01000', 'time': '1.800', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.q_proj', 'loss': '0.83455884', 'samples': '1', 'damp': '0.01000', 'time': '1.888', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.o_proj', 'loss': '0.00048424', 'samples': '1', 'damp': '0.01000', 'time': '1.950', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.up_proj', 'loss': '0.26084259', 'samples': '1', 'damp': '0.01000', 'time': '2.033', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.gate_proj', 'loss': '0.28741685', 'samples': '1', 'damp': '0.01000', 'time': '1.996', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.down_proj', 'loss': '0.00529797', 'samples': '1', 'damp': '0.01000', 'time': '6.845', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.k_proj', 'loss': '0.64601654', 'samples': '1', 'damp': '0.01000', 'time': '1.936', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.v_proj', 'loss': '0.02856080', 'samples': '1', 'damp': '0.01000', 'time': '1.749', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.q_proj', 'loss': '1.76246715', 'samples': '1', 'damp': '0.01000', 'time': '1.866', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.o_proj', 'loss': '0.00235459', 'samples': '1', 'damp': '0.01000', 'time': '2.347', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.up_proj', 'loss': '0.35690558', 'samples': '1', 'damp': '0.01000', 'time': '2.275', 'fwd_time': '0.032'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.gate_proj', 'loss': '0.38693184', 'samples': '1', 'damp': '0.01000', 'time': '2.096', 'fwd_time': '0.032'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.down_proj', 'loss': '0.00197021', 'samples': '1', 'damp': '0.01000', 'time': '6.694', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.k_proj', 'loss': '0.49117780', 'samples': '1', 'damp': '0.01000', 'time': '1.808', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.v_proj', 'loss': '0.03981943', 'samples': '1', 'damp': '0.01000', 'time': '1.912', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.q_proj', 'loss': '1.59195030', 'samples': '1', 'damp': '0.01000', 'time': '2.059', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.o_proj', 'loss': '0.00174010', 'samples': '1', 'damp': '0.01000', 'time': '1.933', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.up_proj', 'loss': '0.44945088', 'samples': '1', 'damp': '0.01000', 'time': '2.114', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.gate_proj', 'loss': '0.53812623', 'samples': '1', 'damp': '0.01000', 'time': '2.038', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.down_proj', 'loss': '0.00193861', 'samples': '1', 'damp': '0.01000', 'time': '6.453', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.k_proj', 'loss': '0.73155057', 'samples': '1', 'damp': '0.01000', 'time': '1.805', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.v_proj', 'loss': '0.04405088', 'samples': '1', 'damp': '0.01000', 'time': '1.811', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.q_proj', 'loss': '1.91151834', 'samples': '1', 'damp': '0.01000', 'time': '1.915', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.o_proj', 'loss': '0.00118212', 'samples': '1', 'damp': '0.01000', 'time': '1.873', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.up_proj', 'loss': '0.52675408', 'samples': '1', 'damp': '0.01000', 'time': '2.063', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.gate_proj', 'loss': '0.62512434', 'samples': '1', 'damp': '0.01000', 'time': '2.064', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.down_proj', 'loss': '0.00281798', 'samples': '1', 'damp': '0.01000', 'time': '6.644', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.k_proj', 'loss': '0.71909732', 'samples': '1', 'damp': '0.01000', 'time': '1.820', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.v_proj', 'loss': '0.04187158', 'samples': '1', 'damp': '0.01000', 'time': '1.715', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.q_proj', 'loss': '1.86984372', 'samples': '1', 'damp': '0.01000', 'time': '1.872', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.o_proj', 'loss': '0.00095670', 'samples': '1', 'damp': '0.01000', 'time': '1.869', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.up_proj', 'loss': '0.62906682', 'samples': '1', 'damp': '0.01000', 'time': '2.030', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.gate_proj', 'loss': '0.74063522', 'samples': '1', 'damp': '0.01000', 'time': '1.963', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.down_proj', 'loss': '0.00328434', 'samples': '1', 'damp': '0.01000', 'time': '6.526', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.k_proj', 'loss': '0.98030949', 'samples': '1', 'damp': '0.01000', 'time': '1.881', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.v_proj', 'loss': '0.04793773', 'samples': '1', 'damp': '0.01000', 'time': '1.814', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.q_proj', 'loss': '2.27248144', 'samples': '1', 'damp': '0.01000', 'time': '1.982', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.o_proj', 'loss': '0.00118265', 'samples': '1', 'damp': '0.01000', 'time': '1.875', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.up_proj', 'loss': '0.75285542', 'samples': '1', 'damp': '0.01000', 'time': '1.977', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.gate_proj', 'loss': '0.93193400', 'samples': '1', 'damp': '0.01000', 'time': '2.007', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.down_proj', 'loss': '0.00564729', 'samples': '1', 'damp': '0.01000', 'time': '6.395', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.k_proj', 'loss': '0.81816399', 'samples': '1', 'damp': '0.01000', 'time': '1.829', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.v_proj', 'loss': '0.04599106', 'samples': '1', 'damp': '0.01000', 'time': '1.750', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.q_proj', 'loss': '2.14848042', 'samples': '1', 'damp': '0.01000', 'time': '1.848', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.o_proj', 'loss': '0.00129719', 'samples': '1', 'damp': '0.01000', 'time': '1.783', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.up_proj', 'loss': '0.78806174', 'samples': '1', 'damp': '0.01000', 'time': '2.049', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.gate_proj', 'loss': '0.99281961', 'samples': '1', 'damp': '0.01000', 'time': '2.030', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.down_proj', 'loss': '0.00704466', 'samples': '1', 'damp': '0.01000', 'time': '6.523', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.k_proj', 'loss': '1.06420100', 'samples': '1', 'damp': '0.01000', 'time': '1.801', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.v_proj', 'loss': '0.04508825', 'samples': '1', 'damp': '0.01000', 'time': '1.751', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.q_proj', 'loss': '2.59440231', 'samples': '1', 'damp': '0.01000', 'time': '1.856', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.o_proj', 'loss': '0.00967510', 'samples': '1', 'damp': '0.01000', 'time': '1.797', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.up_proj', 'loss': '0.92008734', 'samples': '1', 'damp': '0.01000', 'time': '2.024', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.gate_proj', 'loss': '1.05250621', 'samples': '1', 'damp': '0.01000', 'time': '1.981', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.down_proj', 'loss': '0.00770114', 'samples': '1', 'damp': '0.01000', 'time': '6.837', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.k_proj', 'loss': '0.76577234', 'samples': '1', 'damp': '0.01000', 'time': '1.835', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.v_proj', 'loss': '0.05711044', 'samples': '1', 'damp': '0.01000', 'time': '1.788', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.q_proj', 'loss': '2.20550013', 'samples': '1', 'damp': '0.01000', 'time': '1.772', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.o_proj', 'loss': '0.00782862', 'samples': '1', 'damp': '0.01000', 'time': '1.809', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.up_proj', 'loss': '0.98110312', 'samples': '1', 'damp': '0.01000', 'time': '2.339', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.gate_proj', 'loss': '1.13231170', 'samples': '1', 'damp': '0.01000', 'time': '2.253', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.down_proj', 'loss': '0.00831588', 'samples': '1', 'damp': '0.01000', 'time': '6.490', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.k_proj', 'loss': '0.99576122', 'samples': '1', 'damp': '0.01000', 'time': '1.826', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.v_proj', 'loss': '0.06612855', 'samples': '1', 'damp': '0.01000', 'time': '1.777', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.q_proj', 'loss': '2.73123789', 'samples': '1', 'damp': '0.01000', 'time': '1.780', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.o_proj', 'loss': '0.01345933', 'samples': '1', 'damp': '0.01000', 'time': '1.780', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.up_proj', 'loss': '1.07801199', 'samples': '1', 'damp': '0.01000', 'time': '2.039', 'fwd_time': '0.017'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.gate_proj', 'loss': '1.43164611', 'samples': '1', 'damp': '0.01000', 'time': '2.004', 'fwd_time': '0.017'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.down_proj', 'loss': '0.01344320', 'samples': '1', 'damp': '0.01000', 'time': '6.518', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.k_proj', 'loss': '1.01412916', 'samples': '1', 'damp': '0.01000', 'time': '1.887', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.v_proj', 'loss': '0.08740665', 'samples': '1', 'damp': '0.01000', 'time': '1.787', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.q_proj', 'loss': '2.44200945', 'samples': '1', 'damp': '0.01000', 'time': '1.901', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.o_proj', 'loss': '0.01656007', 'samples': '1', 'damp': '0.01000', 'time': '1.774', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.up_proj', 'loss': '1.20504045', 'samples': '1', 'damp': '0.01000', 'time': '2.045', 'fwd_time': '0.017'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.gate_proj', 'loss': '1.74824977', 'samples': '1', 'damp': '0.01000', 'time': '2.047', 'fwd_time': '0.017'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.down_proj', 'loss': '0.01409362', 'samples': '1', 'damp': '0.01000', 'time': '6.542', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.k_proj', 'loss': '1.04128623', 'samples': '1', 'damp': '0.01000', 'time': '1.792', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.v_proj', 'loss': '0.07925136', 'samples': '1', 'damp': '0.01000', 'time': '1.725', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.q_proj', 'loss': '2.63220978', 'samples': '1', 'damp': '0.01000', 'time': '1.799', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.o_proj', 'loss': '0.01018958', 'samples': '1', 'damp': '0.01000', 'time': '1.788', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.up_proj', 'loss': '1.29685450', 'samples': '1', 'damp': '0.01000', 'time': '2.109', 'fwd_time': '0.032'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.gate_proj', 'loss': '1.90123272', 'samples': '1', 'damp': '0.01000', 'time': '1.935', 'fwd_time': '0.032'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.down_proj', 'loss': '0.01765139', 'samples': '1', 'damp': '0.01000', 'time': '6.987', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.k_proj', 'loss': '1.08468103', 'samples': '1', 'damp': '0.01000', 'time': '1.792', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.v_proj', 'loss': '0.06495653', 'samples': '1', 'damp': '0.01000', 'time': '1.706', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.q_proj', 'loss': '2.57177019', 'samples': '1', 'damp': '0.01000', 'time': '1.853', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.o_proj', 'loss': '0.01693052', 'samples': '1', 'damp': '0.01000', 'time': '1.773', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.up_proj', 'loss': '1.38495171', 'samples': '1', 'damp': '0.01000', 'time': '1.976', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.gate_proj', 'loss': '1.83714008', 'samples': '1', 'damp': '0.01000', 'time': '2.087', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.down_proj', 'loss': '0.02041383', 'samples': '1', 'damp': '0.01000', 'time': '6.767', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.k_proj', 'loss': '1.12995791', 'samples': '1', 'damp': '0.01000', 'time': '1.915', 'fwd_time': '0.013'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.v_proj', 'loss': '0.10309613', 'samples': '1', 'damp': '0.01000', 'time': '1.828', 'fwd_time': '0.013'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.q_proj', 'loss': '3.42403150', 'samples': '1', 'damp': '0.01000', 'time': '1.820', 'fwd_time': '0.013'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.o_proj', 'loss': '0.03971560', 'samples': '1', 'damp': '0.01000', 'time': '1.837', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.up_proj', 'loss': '1.67907703', 'samples': '1', 'damp': '0.01000', 'time': '2.046', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.gate_proj', 'loss': '2.25721693', 'samples': '1', 'damp': '0.01000', 'time': '1.939', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.down_proj', 'loss': '0.03909773', 'samples': '1', 'damp': '0.01000', 'time': '6.829', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.k_proj', 'loss': '1.10756350', 'samples': '1', 'damp': '0.01000', 'time': '1.922', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.v_proj', 'loss': '0.12275578', 'samples': '1', 'damp': '0.01000', 'time': '1.772', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.q_proj', 'loss': '2.90932488', 'samples': '1', 'damp': '0.01000', 'time': '1.977', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.o_proj', 'loss': '0.04257073', 'samples': '1', 'damp': '0.01000', 'time': '1.749', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.up_proj', 'loss': '1.90927470', 'samples': '1', 'damp': '0.01000', 'time': '2.041', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.gate_proj', 'loss': '2.42662287', 'samples': '1', 'damp': '0.01000', 'time': '2.014', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.down_proj', 'loss': '0.05367062', 'samples': '1', 'damp': '0.01000', 'time': '6.769', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.k_proj', 'loss': '1.11556125', 'samples': '1', 'damp': '0.01000', 'time': '1.812', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.v_proj', 'loss': '0.13731849', 'samples': '1', 'damp': '0.01000', 'time': '1.779', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.q_proj', 'loss': '2.82507801', 'samples': '1', 'damp': '0.01000', 'time': '1.835', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.o_proj', 'loss': '0.03849703', 'samples': '1', 'damp': '0.01000', 'time': '1.827', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.up_proj', 'loss': '2.23409653', 'samples': '1', 'damp': '0.01000', 'time': '2.191', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.gate_proj', 'loss': '2.81495595', 'samples': '1', 'damp': '0.01000', 'time': '2.184', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.down_proj', 'loss': '0.06671679', 'samples': '1', 'damp': '0.01000', 'time': '6.698', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.k_proj', 'loss': '1.10021424', 'samples': '1', 'damp': '0.01000', 'time': '1.783', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.v_proj', 'loss': '0.16238821', 'samples': '1', 'damp': '0.01000', 'time': '1.737', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.q_proj', 'loss': '2.86855793', 'samples': '1', 'damp': '0.01000', 'time': '1.844', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.o_proj', 'loss': '0.03369877', 'samples': '1', 'damp': '0.01000', 'time': '1.866', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.up_proj', 'loss': '2.70153141', 'samples': '1', 'damp': '0.01000', 'time': '2.019', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.gate_proj', 'loss': '3.49715519', 'samples': '1', 'damp': '0.01000', 'time': '2.027', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.down_proj', 'loss': '0.10559007', 'samples': '1', 'damp': '0.01000', 'time': '6.695', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.k_proj', 'loss': '0.99162185', 'samples': '1', 'damp': '0.01000', 'time': '1.923', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.v_proj', 'loss': '0.22864181', 'samples': '1', 'damp': '0.01000', 'time': '1.762', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.q_proj', 'loss': '3.42728829', 'samples': '1', 'damp': '0.01000', 'time': '1.812', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.o_proj', 'loss': '0.03737143', 'samples': '1', 'damp': '0.01000', 'time': '1.847', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.up_proj', 'loss': '3.03627491', 'samples': '1', 'damp': '0.01000', 'time': '2.009', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.gate_proj', 'loss': '3.81179214', 'samples': '1', 'damp': '0.01000', 'time': '2.007', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.down_proj', 'loss': '0.10912511', 'samples': '1', 'damp': '0.01000', 'time': '6.853', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.k_proj', 'loss': '1.05409360', 'samples': '1', 'damp': '0.01000', 'time': '1.895', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.v_proj', 'loss': '0.17521822', 'samples': '1', 'damp': '0.01000', 'time': '2.550', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.q_proj', 'loss': '2.80833149', 'samples': '1', 'damp': '0.01000', 'time': '1.977', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.o_proj', 'loss': '0.05945527', 'samples': '1', 'damp': '0.01000', 'time': '1.822', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.up_proj', 'loss': '3.59295082', 'samples': '1', 'damp': '0.01000', 'time': '2.098', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.gate_proj', 'loss': '4.27605867', 'samples': '1', 'damp': '0.01000', 'time': '1.987', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.down_proj', 'loss': '2.03395867', 'samples': '1', 'damp': '0.01000', 'time': '7.063', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.k_proj', 'loss': '0.81634945', 'samples': '1', 'damp': '0.01000', 'time': '1.790', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.v_proj', 'loss': '0.70448846', 'samples': '1', 'damp': '0.01000', 'time': '1.829', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.q_proj', 'loss': '3.03732157', 'samples': '1', 'damp': '0.01000', 'time': '1.824', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.o_proj', 'loss': '0.22671530', 'samples': '1', 'damp': '0.01000', 'time': '1.756', 'fwd_time': '0.005'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.up_proj', 'loss': '4.16390419', 'samples': '1', 'damp': '0.01000', 'time': '1.971', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.gate_proj', 'loss': '4.75274754', 'samples': '1', 'damp': '0.01000', 'time': '1.962', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.down_proj', 'loss': '0.38828292', 'samples': '1', 'damp': '0.01000', 'time': '6.657', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.k_proj', 'loss': '0.91655505', 'samples': '1', 'damp': '0.01000', 'time': '1.925', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.v_proj', 'loss': '0.33506715', 'samples': '1', 'damp': '0.01000', 'time': '2.019', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.q_proj', 'loss': '2.98317146', 'samples': '1', 'damp': '0.01000', 'time': '2.107', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.o_proj', 'loss': '0.30907726', 'samples': '1', 'damp': '0.01000', 'time': '1.981', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.up_proj', 'loss': '4.34757805', 'samples': '1', 'damp': '0.01000', 'time': '2.269', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.gate_proj', 'loss': '6.55129242', 'samples': '1', 'damp': '0.01000', 'time': '2.044', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.down_proj', 'loss': '1.41580963', 'samples': '1', 'damp': '0.01000', 'time': '6.622', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                         \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`      \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                               \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                  \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                            0%\n",
      "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v2 to v1                                         \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 4,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:2.2.0\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0\n",
      "  }\n",
      "}\n",
      "Files in directory:\n",
      "config.json\n",
      "generation_config.json\n",
      "model.safetensors\n",
      "quantize_config.json\n",
      "quant_log.csv\n",
      "special_tokens_map.json\n",
      "tokenizer.json\n",
      "tokenizer_config.json\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"bos_token_id\": 1,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"max_length\": 2048,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"transformers_version\": \"4.53.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"architectures\": [\n",
      "        \"LlamaForCausalLM\"\n",
      "    ],\n",
      "    \"attention_bias\": false,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bos_token_id\": 1,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"head_dim\": 64,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 2048,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 5632,\n",
      "    \"max_position_embeddings\": 2048,\n",
      "    \"mlp_bias\": false,\n",
      "    \"model_type\": \"llama\",\n",
      "    \"num_attention_heads\": 32,\n",
      "    \"num_hidden_layers\": 22,\n",
      "    \"num_key_value_heads\": 4,\n",
      "    \"pretraining_tp\": 1,\n",
      "    \"quantization_config\": {\n",
      "        \"bits\": 4,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:2.2.0\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\"\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true\n",
      "    },\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"rope_scaling\": null,\n",
      "    \"rope_theta\": 10000.0,\n",
      "    \"tie_word_embeddings\": false,\n",
      "    \"torch_dtype\": \"float16\",\n",
      "    \"transformers_version\": \"4.53.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 32000\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 2098.20MB, 2.05GB                              \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 731.80MB, 0.71GB                                   \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1366.40MB, 1.33GB - 65.12%                              \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaTokenizerFast(name_or_path='TinyLlama-models/original', vocab_size=32000, model_max_length=1000000000000000019884624838656, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<s>', 'eos_token': '</s>', 'unk_token': '<unk>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=False, added_tokens_decoder={\n",
       "\t0: AddedToken(\"<unk>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t1: AddedToken(\"<s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "\t2: AddedToken(\"</s>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
       "}\n",
       ")"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "QuantizeModelandSaveP(quantize_config_4, output_dir_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e34ffd0b",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 2.44375 bpw, based on [bits: 2, group_size: 64]\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native float16): `torch.float16`                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=0 (token='<unk>').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 2048,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`      \n",
      "\u001b[33mWARN\u001b[0m  Calibration dataset size should be more than 256. Current: 1.            \n",
      "\u001b[33mWARN\u001b[0m  The average length of input_ids of calibration_dataset should be greater than 256: actual avg: 31.0.\n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_ultrainvolved_time_07_28_2025_12h_12m_38s.log`\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.k_proj     | \u001b[93m3.35429287\u001b[0m | 1           | 0.01000     | 1.985     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.v_proj     | \u001b[92m0.07129981\u001b[0m | 1           | 0.01000     | 1.833     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.q_proj     | \u001b[33m5.99983311\u001b[0m | 1           | 0.01000     | 1.926     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.o_proj     | \u001b[92m0.00484878\u001b[0m | 1           | 0.01000     | 1.952     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.up_proj          | \u001b[93m3.29495549\u001b[0m | 1           | 0.01000     | 2.072     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.gate_proj        | \u001b[93m3.20866919\u001b[0m | 1           | 0.01000     | 2.127     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.down_proj        | \u001b[91m47.42726135\u001b[0m | 1           | 0.01000     | 6.581     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.k_proj     | \u001b[93m4.88939571\u001b[0m | 1           | 0.01000     | 1.891     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.v_proj     | \u001b[96m0.24299817\u001b[0m | 1           | 0.01000     | 1.765     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.q_proj     | \u001b[33m15.09903908\u001b[0m | 1           | 0.01000     | 1.995     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.o_proj     | \u001b[92m0.00968238\u001b[0m | 1           | 0.01000     | 2.207     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.up_proj          | \u001b[33m5.69273949\u001b[0m | 1           | 0.01000     | 2.249     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.gate_proj        | \u001b[33m6.28679562\u001b[0m | 1           | 0.01000     | 2.018     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.down_proj        | \u001b[96m0.11143301\u001b[0m | 1           | 0.01000     | 6.649     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.k_proj     | \u001b[33m14.15008068\u001b[0m | 1           | 0.01000     | 1.826     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.v_proj     | \u001b[96m0.62948251\u001b[0m | 1           | 0.01000     | 1.745     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.q_proj     | \u001b[91m39.49887848\u001b[0m | 1           | 0.01000     | 1.744     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.o_proj     | \u001b[92m0.05123036\u001b[0m | 1           | 0.01000     | 1.751     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.up_proj          | \u001b[33m8.35054970\u001b[0m | 1           | 0.01000     | 1.999     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.gate_proj        | \u001b[33m9.05655575\u001b[0m | 1           | 0.01000     | 2.034     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.down_proj        | \u001b[92m0.04341260\u001b[0m | 1           | 0.01000     | 6.926     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.k_proj     | \u001b[33m10.80197334\u001b[0m | 1           | 0.01000     | 1.839     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.v_proj     | \u001b[96m0.86512268\u001b[0m | 1           | 0.01000     | 1.861     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.q_proj     | \u001b[91m39.31965256\u001b[0m | 1           | 0.01000     | 1.886     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.o_proj     | \u001b[92m0.03871281\u001b[0m | 1           | 0.01000     | 1.770     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.up_proj          | \u001b[33m10.23445892\u001b[0m | 1           | 0.01000     | 1.975     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.gate_proj        | \u001b[33m12.26315880\u001b[0m | 1           | 0.01000     | 3.241     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.down_proj        | \u001b[92m0.04404548\u001b[0m | 1           | 0.01000     | 7.066     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.k_proj     | \u001b[33m17.06078911\u001b[0m | 1           | 0.01000     | 1.800     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.v_proj     | \u001b[96m0.98603642\u001b[0m | 1           | 0.01000     | 1.746     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.q_proj     | \u001b[91m48.72138214\u001b[0m | 1           | 0.01000     | 1.716     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.o_proj     | \u001b[92m0.02364118\u001b[0m | 1           | 0.01000     | 1.726     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.up_proj          | \u001b[33m12.13622952\u001b[0m | 1           | 0.01000     | 1.799     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.gate_proj        | \u001b[33m14.50165939\u001b[0m | 1           | 0.01000     | 1.891     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.down_proj        | \u001b[92m0.05846796\u001b[0m | 1           | 0.01000     | 6.480     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.k_proj     | \u001b[33m16.32915115\u001b[0m | 1           | 0.01000     | 1.683     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.v_proj     | \u001b[96m0.93168175\u001b[0m | 1           | 0.01000     | 1.623     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.q_proj     | \u001b[91m45.91732407\u001b[0m | 1           | 0.01000     | 1.697     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.o_proj     | \u001b[92m0.01849056\u001b[0m | 1           | 0.01000     | 1.657     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.up_proj          | \u001b[33m14.29436874\u001b[0m | 1           | 0.01000     | 1.820     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.gate_proj        | \u001b[33m16.85290146\u001b[0m | 1           | 0.01000     | 1.944     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.down_proj        | \u001b[92m0.06888095\u001b[0m | 1           | 0.01000     | 6.219     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.k_proj     | \u001b[91m22.06508064\u001b[0m | 1           | 0.01000     | 1.650     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.v_proj     | \u001b[93m1.05358851\u001b[0m | 1           | 0.01000     | 1.517     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.q_proj     | \u001b[91m54.90336990\u001b[0m | 1           | 0.01000     | 1.535     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.o_proj     | \u001b[92m0.02258958\u001b[0m | 1           | 0.01000     | 1.514     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.up_proj          | \u001b[33m16.94911385\u001b[0m | 1           | 0.01000     | 1.706     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.gate_proj        | \u001b[91m21.02405930\u001b[0m | 1           | 0.01000     | 1.680     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.down_proj        | \u001b[96m0.12118343\u001b[0m | 1           | 0.01000     | 5.857     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.k_proj     | \u001b[33m17.75653076\u001b[0m | 1           | 0.01000     | 1.551     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.v_proj     | \u001b[96m0.97044516\u001b[0m | 1           | 0.01000     | 1.498     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.q_proj     | \u001b[91m50.29811859\u001b[0m | 1           | 0.01000     | 1.572     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.o_proj     | \u001b[92m0.02562436\u001b[0m | 1           | 0.01000     | 1.478     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.up_proj          | \u001b[33m17.82218361\u001b[0m | 1           | 0.01000     | 1.724     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.gate_proj        | \u001b[91m22.47087479\u001b[0m | 1           | 0.01000     | 1.669     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.down_proj        | \u001b[96m0.14242497\u001b[0m | 1           | 0.01000     | 5.814     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.k_proj     | \u001b[91m23.44088745\u001b[0m | 1           | 0.01000     | 1.543     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.v_proj     | \u001b[96m0.98053861\u001b[0m | 1           | 0.01000     | 1.733     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.q_proj     | \u001b[91m58.91450500\u001b[0m | 1           | 0.01000     | 1.593     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.o_proj     | \u001b[96m0.23913741\u001b[0m | 1           | 0.01000     | 1.467     | 0.005        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.up_proj          | \u001b[91m20.86757088\u001b[0m | 1           | 0.01000     | 1.744     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.gate_proj        | \u001b[91m23.95265198\u001b[0m | 1           | 0.01000     | 1.705     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.down_proj        | \u001b[96m0.16380136\u001b[0m | 1           | 0.01000     | 5.737     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.k_proj     | \u001b[33m16.72767639\u001b[0m | 1           | 0.01000     | 1.510     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.v_proj     | \u001b[93m1.20539939\u001b[0m | 1           | 0.01000     | 1.518     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.q_proj     | \u001b[91m50.45584488\u001b[0m | 1           | 0.01000     | 1.537     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.o_proj     | \u001b[96m0.17128864\u001b[0m | 1           | 0.01000     | 1.493     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.up_proj          | \u001b[91m21.99033546\u001b[0m | 1           | 0.01000     | 1.696     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.gate_proj        | \u001b[91m25.37271881\u001b[0m | 1           | 0.01000     | 1.675     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.down_proj        | \u001b[96m0.17482069\u001b[0m | 1           | 0.01000     | 5.689     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.k_proj     | \u001b[91m21.52802277\u001b[0m | 1           | 0.01000     | 1.529     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.v_proj     | \u001b[93m1.41495538\u001b[0m | 1           | 0.01000     | 1.475     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.q_proj     | \u001b[91m61.54612350\u001b[0m | 1           | 0.01000     | 1.503     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.o_proj     | \u001b[96m0.37017414\u001b[0m | 1           | 0.01000     | 1.457     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.up_proj          | \u001b[91m24.13595200\u001b[0m | 1           | 0.01000     | 1.799     | 0.032        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.gate_proj        | \u001b[91m32.16529083\u001b[0m | 1           | 0.01000     | 1.725     | 0.032        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.down_proj        | \u001b[96m0.31623328\u001b[0m | 1           | 0.01000     | 5.776     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.k_proj     | \u001b[91m22.25916290\u001b[0m | 1           | 0.01000     | 1.557     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.v_proj     | \u001b[93m1.84390223\u001b[0m | 1           | 0.01000     | 1.526     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.q_proj     | \u001b[91m55.25947571\u001b[0m | 1           | 0.01000     | 1.635     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.o_proj     | \u001b[96m0.53263199\u001b[0m | 1           | 0.01000     | 1.511     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.up_proj          | \u001b[91m26.92916870\u001b[0m | 1           | 0.01000     | 1.779     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.gate_proj        | \u001b[91m39.09677124\u001b[0m | 1           | 0.01000     | 1.799     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.down_proj        | \u001b[96m0.35757628\u001b[0m | 1           | 0.01000     | 5.865     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.k_proj     | \u001b[91m23.42418671\u001b[0m | 1           | 0.01000     | 1.679     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.v_proj     | \u001b[93m1.67506742\u001b[0m | 1           | 0.01000     | 1.557     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.q_proj     | \u001b[91m59.45076752\u001b[0m | 1           | 0.01000     | 1.585     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.o_proj     | \u001b[96m0.27497438\u001b[0m | 1           | 0.01000     | 1.607     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.up_proj          | \u001b[91m28.88018036\u001b[0m | 1           | 0.01000     | 1.803     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.gate_proj        | \u001b[91m42.37448883\u001b[0m | 1           | 0.01000     | 1.796     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.down_proj        | \u001b[96m0.37459540\u001b[0m | 1           | 0.01000     | 5.796     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.k_proj     | \u001b[91m23.27747536\u001b[0m | 1           | 0.01000     | 1.622     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.v_proj     | \u001b[93m1.38574684\u001b[0m | 1           | 0.01000     | 1.538     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.q_proj     | \u001b[91m56.00373077\u001b[0m | 1           | 0.01000     | 1.607     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.o_proj     | \u001b[96m0.43550521\u001b[0m | 1           | 0.01000     | 1.621     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.up_proj          | \u001b[91m30.43117905\u001b[0m | 1           | 0.01000     | 1.886     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.gate_proj        | \u001b[91m40.49756622\u001b[0m | 1           | 0.01000     | 1.724     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.down_proj        | \u001b[96m0.44016266\u001b[0m | 1           | 0.01000     | 6.333     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.k_proj     | \u001b[91m24.58087349\u001b[0m | 1           | 0.01000     | 1.685     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.v_proj     | \u001b[93m2.11760044\u001b[0m | 1           | 0.01000     | 1.586     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.q_proj     | \u001b[91m74.76759338\u001b[0m | 1           | 0.01000     | 1.595     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.o_proj     | \u001b[96m0.84828281\u001b[0m | 1           | 0.01000     | 1.558     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.up_proj          | \u001b[91m36.87287903\u001b[0m | 1           | 0.01000     | 1.805     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.gate_proj        | \u001b[91m49.76138306\u001b[0m | 1           | 0.01000     | 1.728     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.down_proj        | \u001b[96m0.83630747\u001b[0m | 1           | 0.01000     | 5.830     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.k_proj     | \u001b[91m24.13965607\u001b[0m | 1           | 0.01000     | 1.589     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.v_proj     | \u001b[93m2.56754541\u001b[0m | 1           | 0.01000     | 1.547     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.q_proj     | \u001b[91m63.40667343\u001b[0m | 1           | 0.01000     | 1.634     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.o_proj     | \u001b[93m1.02306604\u001b[0m | 1           | 0.01000     | 1.541     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.up_proj          | \u001b[91m43.11441803\u001b[0m | 1           | 0.01000     | 1.719     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.gate_proj        | \u001b[91m54.70350266\u001b[0m | 1           | 0.01000     | 1.701     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.down_proj        | \u001b[93m1.07618904\u001b[0m | 1           | 0.01000     | 5.576     | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.k_proj     | \u001b[91m24.55357361\u001b[0m | 1           | 0.01000     | 1.529     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.v_proj     | \u001b[93m2.96154332\u001b[0m | 1           | 0.01000     | 1.529     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.q_proj     | \u001b[91m66.16550446\u001b[0m | 1           | 0.01000     | 1.544     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.o_proj     | \u001b[96m0.78954929\u001b[0m | 1           | 0.01000     | 1.449     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.up_proj          | \u001b[91m49.57173920\u001b[0m | 1           | 0.01000     | 1.722     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.gate_proj        | \u001b[91m62.71314621\u001b[0m | 1           | 0.01000     | 1.724     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.down_proj        | \u001b[93m1.28991330\u001b[0m | 1           | 0.01000     | 5.712     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.k_proj     | \u001b[91m24.07358170\u001b[0m | 1           | 0.01000     | 1.572     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.v_proj     | \u001b[93m3.54707050\u001b[0m | 1           | 0.01000     | 1.459     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.q_proj     | \u001b[91m64.79780579\u001b[0m | 1           | 0.01000     | 1.535     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.o_proj     | \u001b[96m0.67919797\u001b[0m | 1           | 0.01000     | 1.485     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.up_proj          | \u001b[91m60.18290710\u001b[0m | 1           | 0.01000     | 1.727     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.gate_proj        | \u001b[91m78.03961182\u001b[0m | 1           | 0.01000     | 1.715     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.down_proj        | \u001b[93m1.98336816\u001b[0m | 1           | 0.01000     | 5.657     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.k_proj     | \u001b[91m21.63017845\u001b[0m | 1           | 0.01000     | 1.577     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.v_proj     | \u001b[93m4.89943123\u001b[0m | 1           | 0.01000     | 1.476     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.q_proj     | \u001b[91m78.49014282\u001b[0m | 1           | 0.01000     | 1.616     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.o_proj     | \u001b[96m0.79116225\u001b[0m | 1           | 0.01000     | 1.526     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.up_proj          | \u001b[91m68.39591217\u001b[0m | 1           | 0.01000     | 1.699     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.gate_proj        | \u001b[91m85.85959625\u001b[0m | 1           | 0.01000     | 1.699     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.down_proj        | \u001b[93m2.13820839\u001b[0m | 1           | 0.01000     | 5.654     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.k_proj     | \u001b[91m23.02910423\u001b[0m | 1           | 0.01000     | 1.506     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.v_proj     | \u001b[93m3.83452129\u001b[0m | 1           | 0.01000     | 1.530     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.q_proj     | \u001b[91m61.46907806\u001b[0m | 1           | 0.01000     | 1.532     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.o_proj     | \u001b[93m1.14557445\u001b[0m | 1           | 0.01000     | 1.491     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.up_proj          | \u001b[91m79.79539490\u001b[0m | 1           | 0.01000     | 1.698     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.gate_proj        | \u001b[91m94.16122437\u001b[0m | 1           | 0.01000     | 1.700     | 0.015        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.down_proj        | \u001b[91m40.31287384\u001b[0m | 1           | 0.01000     | 5.596     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.k_proj     | \u001b[33m16.66200447\u001b[0m | 1           | 0.01000     | 1.568     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.v_proj     | \u001b[33m14.59493351\u001b[0m | 1           | 0.01000     | 1.494     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.q_proj     | \u001b[91m62.41314697\u001b[0m | 1           | 0.01000     | 1.532     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.o_proj     | \u001b[33m5.57978725\u001b[0m | 1           | 0.01000     | 1.502     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.up_proj          | \u001b[91m94.74140930\u001b[0m | 1           | 0.01000     | 1.769     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.gate_proj        | \u001b[91m107.46596527\u001b[0m | 1           | 0.01000     | 1.718     | 0.031        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.down_proj        | \u001b[33m8.00170326\u001b[0m | 1           | 0.01000     | 5.603     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.k_proj     | \u001b[33m19.59189034\u001b[0m | 1           | 0.01000     | 1.518     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.v_proj     | \u001b[33m7.30687094\u001b[0m | 1           | 0.01000     | 1.495     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.q_proj     | \u001b[91m61.63952637\u001b[0m | 1           | 0.01000     | 1.579     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.o_proj     | \u001b[33m6.92501545\u001b[0m | 1           | 0.01000     | 1.536     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.up_proj          | \u001b[91m97.22499847\u001b[0m | 1           | 0.01000     | 1.757     | 0.030        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.gate_proj        | \u001b[91m146.49478149\u001b[0m | 1           | 0.01000     | 1.712     | 0.030        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.down_proj        | \u001b[91m27.40155983\u001b[0m | 1           | 0.01000     | 5.616     | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.k_proj', 'loss': '3.35429287', 'samples': '1', 'damp': '0.01000', 'time': '1.985', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.v_proj', 'loss': '0.07129981', 'samples': '1', 'damp': '0.01000', 'time': '1.833', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.q_proj', 'loss': '5.99983311', 'samples': '1', 'damp': '0.01000', 'time': '1.926', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.o_proj', 'loss': '0.00484878', 'samples': '1', 'damp': '0.01000', 'time': '1.952', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.up_proj', 'loss': '3.29495549', 'samples': '1', 'damp': '0.01000', 'time': '2.072', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.gate_proj', 'loss': '3.20866919', 'samples': '1', 'damp': '0.01000', 'time': '2.127', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.down_proj', 'loss': '47.42726135', 'samples': '1', 'damp': '0.01000', 'time': '6.581', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.k_proj', 'loss': '4.88939571', 'samples': '1', 'damp': '0.01000', 'time': '1.891', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.v_proj', 'loss': '0.24299817', 'samples': '1', 'damp': '0.01000', 'time': '1.765', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.q_proj', 'loss': '15.09903908', 'samples': '1', 'damp': '0.01000', 'time': '1.995', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.o_proj', 'loss': '0.00968238', 'samples': '1', 'damp': '0.01000', 'time': '2.207', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.up_proj', 'loss': '5.69273949', 'samples': '1', 'damp': '0.01000', 'time': '2.249', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.gate_proj', 'loss': '6.28679562', 'samples': '1', 'damp': '0.01000', 'time': '2.018', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.down_proj', 'loss': '0.11143301', 'samples': '1', 'damp': '0.01000', 'time': '6.649', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.k_proj', 'loss': '14.15008068', 'samples': '1', 'damp': '0.01000', 'time': '1.826', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.v_proj', 'loss': '0.62948251', 'samples': '1', 'damp': '0.01000', 'time': '1.745', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.q_proj', 'loss': '39.49887848', 'samples': '1', 'damp': '0.01000', 'time': '1.744', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.o_proj', 'loss': '0.05123036', 'samples': '1', 'damp': '0.01000', 'time': '1.751', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.up_proj', 'loss': '8.35054970', 'samples': '1', 'damp': '0.01000', 'time': '1.999', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.gate_proj', 'loss': '9.05655575', 'samples': '1', 'damp': '0.01000', 'time': '2.034', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.down_proj', 'loss': '0.04341260', 'samples': '1', 'damp': '0.01000', 'time': '6.926', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.k_proj', 'loss': '10.80197334', 'samples': '1', 'damp': '0.01000', 'time': '1.839', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.v_proj', 'loss': '0.86512268', 'samples': '1', 'damp': '0.01000', 'time': '1.861', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.q_proj', 'loss': '39.31965256', 'samples': '1', 'damp': '0.01000', 'time': '1.886', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.o_proj', 'loss': '0.03871281', 'samples': '1', 'damp': '0.01000', 'time': '1.770', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.up_proj', 'loss': '10.23445892', 'samples': '1', 'damp': '0.01000', 'time': '1.975', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.gate_proj', 'loss': '12.26315880', 'samples': '1', 'damp': '0.01000', 'time': '3.241', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.down_proj', 'loss': '0.04404548', 'samples': '1', 'damp': '0.01000', 'time': '7.066', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.k_proj', 'loss': '17.06078911', 'samples': '1', 'damp': '0.01000', 'time': '1.800', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.v_proj', 'loss': '0.98603642', 'samples': '1', 'damp': '0.01000', 'time': '1.746', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.q_proj', 'loss': '48.72138214', 'samples': '1', 'damp': '0.01000', 'time': '1.716', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.o_proj', 'loss': '0.02364118', 'samples': '1', 'damp': '0.01000', 'time': '1.726', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.up_proj', 'loss': '12.13622952', 'samples': '1', 'damp': '0.01000', 'time': '1.799', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.gate_proj', 'loss': '14.50165939', 'samples': '1', 'damp': '0.01000', 'time': '1.891', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.down_proj', 'loss': '0.05846796', 'samples': '1', 'damp': '0.01000', 'time': '6.480', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.k_proj', 'loss': '16.32915115', 'samples': '1', 'damp': '0.01000', 'time': '1.683', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.v_proj', 'loss': '0.93168175', 'samples': '1', 'damp': '0.01000', 'time': '1.623', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.q_proj', 'loss': '45.91732407', 'samples': '1', 'damp': '0.01000', 'time': '1.697', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.o_proj', 'loss': '0.01849056', 'samples': '1', 'damp': '0.01000', 'time': '1.657', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.up_proj', 'loss': '14.29436874', 'samples': '1', 'damp': '0.01000', 'time': '1.820', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.gate_proj', 'loss': '16.85290146', 'samples': '1', 'damp': '0.01000', 'time': '1.944', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.down_proj', 'loss': '0.06888095', 'samples': '1', 'damp': '0.01000', 'time': '6.219', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.k_proj', 'loss': '22.06508064', 'samples': '1', 'damp': '0.01000', 'time': '1.650', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.v_proj', 'loss': '1.05358851', 'samples': '1', 'damp': '0.01000', 'time': '1.517', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.q_proj', 'loss': '54.90336990', 'samples': '1', 'damp': '0.01000', 'time': '1.535', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.o_proj', 'loss': '0.02258958', 'samples': '1', 'damp': '0.01000', 'time': '1.514', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.up_proj', 'loss': '16.94911385', 'samples': '1', 'damp': '0.01000', 'time': '1.706', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.gate_proj', 'loss': '21.02405930', 'samples': '1', 'damp': '0.01000', 'time': '1.680', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.down_proj', 'loss': '0.12118343', 'samples': '1', 'damp': '0.01000', 'time': '5.857', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.k_proj', 'loss': '17.75653076', 'samples': '1', 'damp': '0.01000', 'time': '1.551', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.v_proj', 'loss': '0.97044516', 'samples': '1', 'damp': '0.01000', 'time': '1.498', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.q_proj', 'loss': '50.29811859', 'samples': '1', 'damp': '0.01000', 'time': '1.572', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.o_proj', 'loss': '0.02562436', 'samples': '1', 'damp': '0.01000', 'time': '1.478', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.up_proj', 'loss': '17.82218361', 'samples': '1', 'damp': '0.01000', 'time': '1.724', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.gate_proj', 'loss': '22.47087479', 'samples': '1', 'damp': '0.01000', 'time': '1.669', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.down_proj', 'loss': '0.14242497', 'samples': '1', 'damp': '0.01000', 'time': '5.814', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.k_proj', 'loss': '23.44088745', 'samples': '1', 'damp': '0.01000', 'time': '1.543', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.v_proj', 'loss': '0.98053861', 'samples': '1', 'damp': '0.01000', 'time': '1.733', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.q_proj', 'loss': '58.91450500', 'samples': '1', 'damp': '0.01000', 'time': '1.593', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.o_proj', 'loss': '0.23913741', 'samples': '1', 'damp': '0.01000', 'time': '1.467', 'fwd_time': '0.005'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.up_proj', 'loss': '20.86757088', 'samples': '1', 'damp': '0.01000', 'time': '1.744', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.gate_proj', 'loss': '23.95265198', 'samples': '1', 'damp': '0.01000', 'time': '1.705', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.down_proj', 'loss': '0.16380136', 'samples': '1', 'damp': '0.01000', 'time': '5.737', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.k_proj', 'loss': '16.72767639', 'samples': '1', 'damp': '0.01000', 'time': '1.510', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.v_proj', 'loss': '1.20539939', 'samples': '1', 'damp': '0.01000', 'time': '1.518', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.q_proj', 'loss': '50.45584488', 'samples': '1', 'damp': '0.01000', 'time': '1.537', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.o_proj', 'loss': '0.17128864', 'samples': '1', 'damp': '0.01000', 'time': '1.493', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.up_proj', 'loss': '21.99033546', 'samples': '1', 'damp': '0.01000', 'time': '1.696', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.gate_proj', 'loss': '25.37271881', 'samples': '1', 'damp': '0.01000', 'time': '1.675', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.down_proj', 'loss': '0.17482069', 'samples': '1', 'damp': '0.01000', 'time': '5.689', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.k_proj', 'loss': '21.52802277', 'samples': '1', 'damp': '0.01000', 'time': '1.529', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.v_proj', 'loss': '1.41495538', 'samples': '1', 'damp': '0.01000', 'time': '1.475', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.q_proj', 'loss': '61.54612350', 'samples': '1', 'damp': '0.01000', 'time': '1.503', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.o_proj', 'loss': '0.37017414', 'samples': '1', 'damp': '0.01000', 'time': '1.457', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.up_proj', 'loss': '24.13595200', 'samples': '1', 'damp': '0.01000', 'time': '1.799', 'fwd_time': '0.032'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.gate_proj', 'loss': '32.16529083', 'samples': '1', 'damp': '0.01000', 'time': '1.725', 'fwd_time': '0.032'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.down_proj', 'loss': '0.31623328', 'samples': '1', 'damp': '0.01000', 'time': '5.776', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.k_proj', 'loss': '22.25916290', 'samples': '1', 'damp': '0.01000', 'time': '1.557', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.v_proj', 'loss': '1.84390223', 'samples': '1', 'damp': '0.01000', 'time': '1.526', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.q_proj', 'loss': '55.25947571', 'samples': '1', 'damp': '0.01000', 'time': '1.635', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.o_proj', 'loss': '0.53263199', 'samples': '1', 'damp': '0.01000', 'time': '1.511', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.up_proj', 'loss': '26.92916870', 'samples': '1', 'damp': '0.01000', 'time': '1.779', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.gate_proj', 'loss': '39.09677124', 'samples': '1', 'damp': '0.01000', 'time': '1.799', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.down_proj', 'loss': '0.35757628', 'samples': '1', 'damp': '0.01000', 'time': '5.865', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.k_proj', 'loss': '23.42418671', 'samples': '1', 'damp': '0.01000', 'time': '1.679', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.v_proj', 'loss': '1.67506742', 'samples': '1', 'damp': '0.01000', 'time': '1.557', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.q_proj', 'loss': '59.45076752', 'samples': '1', 'damp': '0.01000', 'time': '1.585', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.o_proj', 'loss': '0.27497438', 'samples': '1', 'damp': '0.01000', 'time': '1.607', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.up_proj', 'loss': '28.88018036', 'samples': '1', 'damp': '0.01000', 'time': '1.803', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.gate_proj', 'loss': '42.37448883', 'samples': '1', 'damp': '0.01000', 'time': '1.796', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.down_proj', 'loss': '0.37459540', 'samples': '1', 'damp': '0.01000', 'time': '5.796', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.k_proj', 'loss': '23.27747536', 'samples': '1', 'damp': '0.01000', 'time': '1.622', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.v_proj', 'loss': '1.38574684', 'samples': '1', 'damp': '0.01000', 'time': '1.538', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.q_proj', 'loss': '56.00373077', 'samples': '1', 'damp': '0.01000', 'time': '1.607', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.o_proj', 'loss': '0.43550521', 'samples': '1', 'damp': '0.01000', 'time': '1.621', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.up_proj', 'loss': '30.43117905', 'samples': '1', 'damp': '0.01000', 'time': '1.886', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.gate_proj', 'loss': '40.49756622', 'samples': '1', 'damp': '0.01000', 'time': '1.724', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.down_proj', 'loss': '0.44016266', 'samples': '1', 'damp': '0.01000', 'time': '6.333', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.k_proj', 'loss': '24.58087349', 'samples': '1', 'damp': '0.01000', 'time': '1.685', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.v_proj', 'loss': '2.11760044', 'samples': '1', 'damp': '0.01000', 'time': '1.586', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.q_proj', 'loss': '74.76759338', 'samples': '1', 'damp': '0.01000', 'time': '1.595', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.o_proj', 'loss': '0.84828281', 'samples': '1', 'damp': '0.01000', 'time': '1.558', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.up_proj', 'loss': '36.87287903', 'samples': '1', 'damp': '0.01000', 'time': '1.805', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.gate_proj', 'loss': '49.76138306', 'samples': '1', 'damp': '0.01000', 'time': '1.728', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.down_proj', 'loss': '0.83630747', 'samples': '1', 'damp': '0.01000', 'time': '5.830', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.k_proj', 'loss': '24.13965607', 'samples': '1', 'damp': '0.01000', 'time': '1.589', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.v_proj', 'loss': '2.56754541', 'samples': '1', 'damp': '0.01000', 'time': '1.547', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.q_proj', 'loss': '63.40667343', 'samples': '1', 'damp': '0.01000', 'time': '1.634', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.o_proj', 'loss': '1.02306604', 'samples': '1', 'damp': '0.01000', 'time': '1.541', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.up_proj', 'loss': '43.11441803', 'samples': '1', 'damp': '0.01000', 'time': '1.719', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.gate_proj', 'loss': '54.70350266', 'samples': '1', 'damp': '0.01000', 'time': '1.701', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.down_proj', 'loss': '1.07618904', 'samples': '1', 'damp': '0.01000', 'time': '5.576', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.k_proj', 'loss': '24.55357361', 'samples': '1', 'damp': '0.01000', 'time': '1.529', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.v_proj', 'loss': '2.96154332', 'samples': '1', 'damp': '0.01000', 'time': '1.529', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.q_proj', 'loss': '66.16550446', 'samples': '1', 'damp': '0.01000', 'time': '1.544', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.o_proj', 'loss': '0.78954929', 'samples': '1', 'damp': '0.01000', 'time': '1.449', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.up_proj', 'loss': '49.57173920', 'samples': '1', 'damp': '0.01000', 'time': '1.722', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.gate_proj', 'loss': '62.71314621', 'samples': '1', 'damp': '0.01000', 'time': '1.724', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.down_proj', 'loss': '1.28991330', 'samples': '1', 'damp': '0.01000', 'time': '5.712', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.k_proj', 'loss': '24.07358170', 'samples': '1', 'damp': '0.01000', 'time': '1.572', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.v_proj', 'loss': '3.54707050', 'samples': '1', 'damp': '0.01000', 'time': '1.459', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.q_proj', 'loss': '64.79780579', 'samples': '1', 'damp': '0.01000', 'time': '1.535', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.o_proj', 'loss': '0.67919797', 'samples': '1', 'damp': '0.01000', 'time': '1.485', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.up_proj', 'loss': '60.18290710', 'samples': '1', 'damp': '0.01000', 'time': '1.727', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.gate_proj', 'loss': '78.03961182', 'samples': '1', 'damp': '0.01000', 'time': '1.715', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.down_proj', 'loss': '1.98336816', 'samples': '1', 'damp': '0.01000', 'time': '5.657', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.k_proj', 'loss': '21.63017845', 'samples': '1', 'damp': '0.01000', 'time': '1.577', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.v_proj', 'loss': '4.89943123', 'samples': '1', 'damp': '0.01000', 'time': '1.476', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.q_proj', 'loss': '78.49014282', 'samples': '1', 'damp': '0.01000', 'time': '1.616', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.o_proj', 'loss': '0.79116225', 'samples': '1', 'damp': '0.01000', 'time': '1.526', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.up_proj', 'loss': '68.39591217', 'samples': '1', 'damp': '0.01000', 'time': '1.699', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.gate_proj', 'loss': '85.85959625', 'samples': '1', 'damp': '0.01000', 'time': '1.699', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.down_proj', 'loss': '2.13820839', 'samples': '1', 'damp': '0.01000', 'time': '5.654', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.k_proj', 'loss': '23.02910423', 'samples': '1', 'damp': '0.01000', 'time': '1.506', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.v_proj', 'loss': '3.83452129', 'samples': '1', 'damp': '0.01000', 'time': '1.530', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.q_proj', 'loss': '61.46907806', 'samples': '1', 'damp': '0.01000', 'time': '1.532', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.o_proj', 'loss': '1.14557445', 'samples': '1', 'damp': '0.01000', 'time': '1.491', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.up_proj', 'loss': '79.79539490', 'samples': '1', 'damp': '0.01000', 'time': '1.698', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.gate_proj', 'loss': '94.16122437', 'samples': '1', 'damp': '0.01000', 'time': '1.700', 'fwd_time': '0.015'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.down_proj', 'loss': '40.31287384', 'samples': '1', 'damp': '0.01000', 'time': '5.596', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.k_proj', 'loss': '16.66200447', 'samples': '1', 'damp': '0.01000', 'time': '1.568', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.v_proj', 'loss': '14.59493351', 'samples': '1', 'damp': '0.01000', 'time': '1.494', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.q_proj', 'loss': '62.41314697', 'samples': '1', 'damp': '0.01000', 'time': '1.532', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.o_proj', 'loss': '5.57978725', 'samples': '1', 'damp': '0.01000', 'time': '1.502', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.up_proj', 'loss': '94.74140930', 'samples': '1', 'damp': '0.01000', 'time': '1.769', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.gate_proj', 'loss': '107.46596527', 'samples': '1', 'damp': '0.01000', 'time': '1.718', 'fwd_time': '0.031'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.down_proj', 'loss': '8.00170326', 'samples': '1', 'damp': '0.01000', 'time': '5.603', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.k_proj', 'loss': '19.59189034', 'samples': '1', 'damp': '0.01000', 'time': '1.518', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.v_proj', 'loss': '7.30687094', 'samples': '1', 'damp': '0.01000', 'time': '1.495', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.q_proj', 'loss': '61.63952637', 'samples': '1', 'damp': '0.01000', 'time': '1.579', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.o_proj', 'loss': '6.92501545', 'samples': '1', 'damp': '0.01000', 'time': '1.536', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.up_proj', 'loss': '97.22499847', 'samples': '1', 'damp': '0.01000', 'time': '1.757', 'fwd_time': '0.030'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.gate_proj', 'loss': '146.49478149', 'samples': '1', 'damp': '0.01000', 'time': '1.712', 'fwd_time': '0.030'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.down_proj', 'loss': '27.40155983', 'samples': '1', 'damp': '0.01000', 'time': '5.616', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                         \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`      \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                               \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                  \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                            0%\n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 2,\n",
      "  \"group_size\": 64,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:2.2.0\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0\n",
      "  }\n",
      "}\n",
      "Files in directory:\n",
      "config.json\n",
      "generation_config.json\n",
      "quantize_config.json\n",
      "quant_log.csv\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"bos_token_id\": 1,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"max_length\": 2048,\n",
      "    \"pad_token_id\": 0,\n",
      "    \"transformers_version\": \"4.53.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"architectures\": [\n",
      "        \"LlamaForCausalLM\"\n",
      "    ],\n",
      "    \"attention_bias\": false,\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bos_token_id\": 1,\n",
      "    \"eos_token_id\": 2,\n",
      "    \"head_dim\": 64,\n",
      "    \"hidden_act\": \"silu\",\n",
      "    \"hidden_size\": 2048,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 5632,\n",
      "    \"max_position_embeddings\": 2048,\n",
      "    \"mlp_bias\": false,\n",
      "    \"model_type\": \"llama\",\n",
      "    \"num_attention_heads\": 32,\n",
      "    \"num_hidden_layers\": 22,\n",
      "    \"num_key_value_heads\": 4,\n",
      "    \"pretraining_tp\": 1,\n",
      "    \"quantization_config\": {\n",
      "        \"bits\": 2,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 64,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:2.2.0\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\"\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true\n",
      "    },\n",
      "    \"rms_norm_eps\": 1e-05,\n",
      "    \"rope_scaling\": null,\n",
      "    \"rope_theta\": 10000.0,\n",
      "    \"tie_word_embeddings\": false,\n",
      "    \"torch_dtype\": \"float16\",\n",
      "    \"transformers_version\": \"4.53.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 32000\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 2098.20MB, 2.05GB                              \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 515.23MB, 0.50GB                                   \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1582.96MB, 1.55GB - 75.44%                              \n"
     ]
    }
   ],
   "source": [
    "QuantizeModelandSaveP(quantize_config_2, output_dir_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c444ce29",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native float16): `torch.float16`                     \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 4.2875 bpw, based on [bits: 4, group_size: 128]\n",
      "Quantizing mlp.gate_proj in layer     [2 of 21] | 0:09:10 / 1:07:13 [3/22] 13.6%"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`             \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                               \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                  \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.\n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.015610933303833008s                       \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`             \n",
      "Quantizing mlp.gate_proj in layer     [2 of 21] | 0:09:13 / 1:07:35 [3/22] 13.6%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=0 (token='<unk>').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Model: Auto-fixed `generation_config` mismatch between model and `generation_config.json`.\n",
      "\u001b[32mINFO\u001b[0m  Model: Updated `generation_config`: GenerationConfig {3/22] 13.6%\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"max_length\": 2048,\n",
      "  \"pad_token_id\": 0\n",
      "}\n",
      "\n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                   \n",
      "Quantizing mlp.gate_proj in layer     [2 of 21] | 0:09:13 / 1:07:35 [3/22] 13.6%"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 7\u001b[39m\n\u001b[32m      5\u001b[39m model = GPTQModel.load(output_dir_4, device=device)\n\u001b[32m      6\u001b[39m tokenizer = AutoTokenizer.from_pretrained(original_model_dir, use_fast=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m7\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokenizer.decode(model.generate(**tokenizer(\u001b[33m\"\u001b[39m\u001b[33mgptqmodel is\u001b[39m\u001b[33m\"\u001b[39m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device))[\u001b[32m0\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\gptqmodel\\models\\base.py:1175\u001b[39m, in \u001b[36mBaseGPTQModel.generate\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m   1172\u001b[39m     inputs = \u001b[38;5;28mself\u001b[39m.tokenizer(inputs, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, padding_side=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m).to(\u001b[38;5;28mself\u001b[39m.model.device)\n\u001b[32m   1173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.generate(**inputs, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.generate(inputs=inputs, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:2625\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2617\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2618\u001b[39m         input_ids=input_ids,\n\u001b[32m   2619\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2620\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2621\u001b[39m         **model_kwargs,\n\u001b[32m   2622\u001b[39m     )\n\u001b[32m   2624\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2625\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._sample(\n\u001b[32m   2626\u001b[39m         input_ids,\n\u001b[32m   2627\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   2628\u001b[39m         stopping_criteria=prepared_stopping_criteria,\n\u001b[32m   2629\u001b[39m         generation_config=generation_config,\n\u001b[32m   2630\u001b[39m         synced_gpus=synced_gpus,\n\u001b[32m   2631\u001b[39m         streamer=streamer,\n\u001b[32m   2632\u001b[39m         **model_kwargs,\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2635\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2636\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2637\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2638\u001b[39m         input_ids=input_ids,\n\u001b[32m   2639\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2640\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2641\u001b[39m         **model_kwargs,\n\u001b[32m   2642\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:3609\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3607\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3608\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3609\u001b[39m     outputs = model_forward(**model_inputs, return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3611\u001b[39m \u001b[38;5;66;03m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[39;00m\n\u001b[32m   3612\u001b[39m model_kwargs = \u001b[38;5;28mself\u001b[39m._update_model_kwargs_for_generation(\n\u001b[32m   3613\u001b[39m     outputs,\n\u001b[32m   3614\u001b[39m     model_kwargs,\n\u001b[32m   3615\u001b[39m     is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   3616\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:943\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     output = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    944\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    945\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:553\u001b[39m, in \u001b[36mLlamaForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    548\u001b[39m output_hidden_states = (\n\u001b[32m    549\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    550\u001b[39m )\n\u001b[32m    552\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m553\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28mself\u001b[39m.model(\n\u001b[32m    554\u001b[39m     input_ids=input_ids,\n\u001b[32m    555\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    556\u001b[39m     position_ids=position_ids,\n\u001b[32m    557\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    558\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    559\u001b[39m     use_cache=use_cache,\n\u001b[32m    560\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    561\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m    562\u001b[39m     cache_position=cache_position,\n\u001b[32m    563\u001b[39m     **kwargs,\n\u001b[32m    564\u001b[39m )\n\u001b[32m    566\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    567\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:943\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     output = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    944\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    945\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:441\u001b[39m, in \u001b[36mLlamaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    438\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    439\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m441\u001b[39m layer_outputs = decoder_layer(\n\u001b[32m    442\u001b[39m     hidden_states,\n\u001b[32m    443\u001b[39m     attention_mask=causal_mask,\n\u001b[32m    444\u001b[39m     position_ids=position_ids,\n\u001b[32m    445\u001b[39m     past_key_value=past_key_values,\n\u001b[32m    446\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    447\u001b[39m     use_cache=use_cache,\n\u001b[32m    448\u001b[39m     cache_position=cache_position,\n\u001b[32m    449\u001b[39m     position_embeddings=position_embeddings,\n\u001b[32m    450\u001b[39m     **flash_attn_kwargs,\n\u001b[32m    451\u001b[39m )\n\u001b[32m    453\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    455\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:83\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m         logger.warning(message)\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:306\u001b[39m, in \u001b[36mLlamaDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    304\u001b[39m residual = hidden_states\n\u001b[32m    305\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.post_attention_layernorm(hidden_states)\n\u001b[32m--> \u001b[39m\u001b[32m306\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.mlp(hidden_states)\n\u001b[32m    307\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    309\u001b[39m outputs = (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\models\\llama\\modeling_llama.py:151\u001b[39m, in \u001b[36mLlamaMLP.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     down_proj = \u001b[38;5;28mself\u001b[39m.down_proj(\u001b[38;5;28mself\u001b[39m.act_fn(\u001b[38;5;28mself\u001b[39m.gate_proj(x)) * \u001b[38;5;28mself\u001b[39m.up_proj(x))\n\u001b[32m    152\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m down_proj\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\gptqmodel\\nn_modules\\qlinear\\torch.py:154\u001b[39m, in \u001b[36mTorchQuantLinear.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    152\u001b[39m out_shape = x.shape[:-\u001b[32m1\u001b[39m] + (\u001b[38;5;28mself\u001b[39m.out_features,)\n\u001b[32m    153\u001b[39m x = x.reshape(-\u001b[32m1\u001b[39m, x.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m out = \u001b[38;5;28mself\u001b[39m._forward(x, out_shape)\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\gptqmodel\\nn_modules\\qlinear\\torch.py:160\u001b[39m, in \u001b[36mTorchQuantLinear._forward\u001b[39m\u001b[34m(self, x, out_shape)\u001b[39m\n\u001b[32m    158\u001b[39m num_itr = \u001b[38;5;28mself\u001b[39m.g_idx.shape[\u001b[32m0\u001b[39m] // x.shape[-\u001b[32m1\u001b[39m]\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# make sure dequant dtype matches input x\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m weights = \u001b[38;5;28mself\u001b[39m.dequantize_weight(num_itr=num_itr).to(x.dtype)\n\u001b[32m    162\u001b[39m out = torch.matmul(x, weights).reshape(out_shape)\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\gptqmodel\\nn_modules\\qlinear\\__init__.py:445\u001b[39m, in \u001b[36mPackableQuantLinear.dequantize_weight\u001b[39m\u001b[34m(self, num_itr)\u001b[39m\n\u001b[32m    440\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bits \u001b[38;5;129;01min\u001b[39;00m [\u001b[32m2\u001b[39m, \u001b[32m4\u001b[39m, \u001b[32m8\u001b[39m]:\n\u001b[32m    441\u001b[39m     zeros = t.bitwise_right_shift(\n\u001b[32m    442\u001b[39m         t.unsqueeze(\u001b[38;5;28mself\u001b[39m.qzeros, \u001b[32m2\u001b[39m).expand(-\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.pack_factor),\n\u001b[32m    443\u001b[39m         \u001b[38;5;28mself\u001b[39m.wf_unsqueeze_zero  \u001b[38;5;66;03m# self.wf.unsqueeze(0),\u001b[39;00m\n\u001b[32m    444\u001b[39m     ).to(\u001b[38;5;28mself\u001b[39m.dequant_dtype)\n\u001b[32m--> \u001b[39m\u001b[32m445\u001b[39m     zeros = t.bitwise_and(zeros, \u001b[38;5;28mself\u001b[39m.maxq).reshape(\u001b[38;5;28mself\u001b[39m.scales.shape)\n\u001b[32m    447\u001b[39m     weight = t.bitwise_and(\n\u001b[32m    448\u001b[39m         t.bitwise_right_shift(\n\u001b[32m    449\u001b[39m             t.unsqueeze(\u001b[38;5;28mself\u001b[39m.qweight, \u001b[32m1\u001b[39m).expand(-\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.pack_factor, -\u001b[32m1\u001b[39m),\n\u001b[32m   (...)\u001b[39m\u001b[32m    452\u001b[39m         \u001b[38;5;28mself\u001b[39m.maxq\n\u001b[32m    453\u001b[39m     )\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bits == \u001b[32m3\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from gptqmodel import get_best_device\n",
    "\n",
    "# load quantized model to the first GPU\n",
    "device = get_best_device()\n",
    "model = GPTQModel.load(output_dir_4, device=device)\n",
    "tokenizer = AutoTokenizer.from_pretrained(original_model_dir, use_fast=True)\n",
    "print(tokenizer.decode(model.generate(**tokenizer(\"gptqmodel is\", return_tensors=\"pt\").to(model.device))[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
