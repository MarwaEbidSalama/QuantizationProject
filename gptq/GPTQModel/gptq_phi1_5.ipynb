{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23681762",
   "metadata": {},
   "source": [
    "## Phi-1.5 Multi-bit GPTQ Quantization - Jupyter Notebook\n",
    "### This notebook will save the original model locally and create 8-bit, 4-bit, and 2-bit quantized versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9554e789",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' for memory saving.\n",
      "\u001b[32mINFO\u001b[0m  ENV: Auto setting CUDA_DEVICE_ORDER=PCI_BUS_ID for correctness.          \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from gptqmodel import GPTQModel, QuantizeConfig\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import logging\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "64883ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.7.1+cu128\n",
      "CUDA available: True\n",
      "GPU: Quadro RTX 3000\n"
     ]
    }
   ],
   "source": [
    "# Quantisation environment set up\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02ee451",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "### Model configuration\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c0cc76bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "Source model: microsoft/phi-1_5\n",
      "Base output directory: phi-1_5-models\n",
      "Q8 output directory: phi-1_5-models/phi-1_5-gptqmodel-8bit\n",
      "Q4 output directory: phi-1_5-models/phi-1_5-gptqmodel-4bit\n",
      "Q2 output directory: phi-1_5-models/phi-1_5-gptqmodel-2bit\n"
     ]
    }
   ],
   "source": [
    "model_id = \"microsoft/phi-1_5\"\n",
    "base_output_dir = \"phi-1_5-models\"\n",
    "original_model_dir = f\"{base_output_dir}/original\"\n",
    "output_dir_8= f\"{base_output_dir}/phi-1_5-gptqmodel-8bit\"\n",
    "output_dir_4= f\"{base_output_dir}/phi-1_5-gptqmodel-4bit\"\n",
    "output_dir_2= f\"{base_output_dir}/phi-1_5-gptqmodel-2bit\"\n",
    "\n",
    "# Quantization configurations\n",
    "quantize_config_8 = QuantizeConfig(\n",
    "        bits=8,  # quantize model to 8-bit\n",
    "        group_size=128,  # it is recommended to set the value to 128\n",
    "    )\n",
    "\n",
    "quantize_config_4 = QuantizeConfig(\n",
    "        bits=4,  # quantize model to 4-bit\n",
    "        group_size=128,  # it is recommended to set the value to 128\n",
    "    )\n",
    "\n",
    "quantize_config_2 = QuantizeConfig(\n",
    "        bits=2,  # quantize model to 2-bit\n",
    "        group_size=64,  # it is recommended to set the value to 128\n",
    "    )\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"Source model: {model_id}\")\n",
    "print(f\"Base output directory: {base_output_dir}\")\n",
    "print(f\"Q8 output directory: {output_dir_8}\")\n",
    "print(f\"Q4 output directory: {output_dir_4}\")\n",
    "print(f\"Q2 output directory: {output_dir_2}\")\n",
    "\n",
    "# Create output directories\n",
    "os.makedirs(base_output_dir, exist_ok=True)\n",
    "os.makedirs(output_dir_8, exist_ok=True)\n",
    "os.makedirs(output_dir_4, exist_ok=True)\n",
    "os.makedirs(output_dir_2, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f323d4",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 8.31875 bpw, based on [bits: 8, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native float16): `torch.float16`                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=50256 (token='<|endoftext|>').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {}\n",
      "                  \n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                   \n"
     ]
    }
   ],
   "source": [
    "#Load un-quantized model, by default, the model will always be loaded into CPU memory\n",
    "model = GPTQModel.load(original_model_dir, quantize_config_8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2ecb0c08",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original model size: 2.65 GB\n"
     ]
    }
   ],
   "source": [
    "# Get original model size\n",
    "original_size = sum(os.path.getsize(os.path.join(original_model_dir, f)) \n",
    "                   for f in os.listdir(original_model_dir) \n",
    "                   if os.path.isfile(os.path.join(original_model_dir, f)))\n",
    "\n",
    "print(f\"Original model size: {original_size / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ab73e29f",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "    tokenizer = AutoTokenizer.from_pretrained(original_model_dir, use_fast=True)\n",
    "    calibration_dataset = [\n",
    "        tokenizer(\n",
    "            \"gptqmodel is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"\n",
    "        )\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974bb3be",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`      \n",
      "\u001b[33mWARN\u001b[0m  Calibration dataset size should be more than 256. Current: 1.            \n",
      "\u001b[33mWARN\u001b[0m  The average length of input_ids of calibration_dataset should be greater than 256: actual avg: 29.0.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1341: UserWarning: expandable_segments not supported on this platform (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\c10/cuda/CUDAAllocatorConfig.h:28.)\n",
      "  return t.to(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_garapato_time_07_27_2025_17h_04m_08s.log`\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.q_proj     | \u001b[92m0.01595473\u001b[0m | 1           | 0.01000     | 2.191     | 0.522        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.k_proj     | \u001b[92m0.01693969\u001b[0m | 1           | 0.01000     | 1.723     | 0.005        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.v_proj     | \u001b[92m0.00518053\u001b[0m | 1           | 0.01000     | 1.657     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.dense      | \u001b[92m0.00280963\u001b[0m | 1           | 0.01000     | 1.682     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.fc1              | \u001b[92m0.01969288\u001b[0m | 1           | 0.01000     | 2.276     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.fc2              | \u001b[33m19.75485229\u001b[0m | 1           | 0.01250     | 11.830     | 0.016        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.q_proj     | \u001b[92m0.01593773\u001b[0m | 1           | 0.01000     | 1.580      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.k_proj     | \u001b[92m0.01612364\u001b[0m | 1           | 0.01000     | 1.628      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.v_proj     | \u001b[92m0.00348048\u001b[0m | 1           | 0.01000     | 1.627      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.dense      | \u001b[92m0.00097780\u001b[0m | 1           | 0.01000     | 1.699      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.fc1              | \u001b[92m0.05221809\u001b[0m | 1           | 0.01000     | 2.089      | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.fc2              | \u001b[91m23.08218002\u001b[0m | 1           | 0.01250     | 11.408     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.q_proj     | \u001b[92m0.01985274\u001b[0m | 1           | 0.01000     | 1.738      | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.k_proj     | \u001b[92m0.01969804\u001b[0m | 1           | 0.01000     | 1.590      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.v_proj     | \u001b[92m0.00715708\u001b[0m | 1           | 0.01000     | 1.720      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.dense      | \u001b[92m0.00069260\u001b[0m | 1           | 0.01000     | 1.638      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.fc1              | \u001b[92m0.06927988\u001b[0m | 1           | 0.01000     | 2.340      | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.fc2              | \u001b[33m16.56796646\u001b[0m | 1           | 0.01250     | 11.012     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.q_proj     | \u001b[92m0.02220270\u001b[0m | 1           | 0.01000     | 1.679      | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.k_proj     | \u001b[92m0.02132372\u001b[0m | 1           | 0.01000     | 1.679      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.v_proj     | \u001b[92m0.01007590\u001b[0m | 1           | 0.01000     | 1.665      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.dense      | \u001b[92m0.00066503\u001b[0m | 1           | 0.01000     | 1.683      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.fc1              | \u001b[92m0.07844412\u001b[0m | 1           | 0.01000     | 2.261      | 0.024        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.fc2              | \u001b[91m21.68490219\u001b[0m | 1           | 0.01250     | 11.103     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.q_proj     | \u001b[92m0.02196059\u001b[0m | 1           | 0.01000     | 1.886      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.k_proj     | \u001b[92m0.02036222\u001b[0m | 1           | 0.01000     | 1.768      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.v_proj     | \u001b[92m0.01133000\u001b[0m | 1           | 0.01000     | 1.692      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.dense      | \u001b[92m0.00111006\u001b[0m | 1           | 0.01000     | 1.724      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.fc1              | \u001b[92m0.07329569\u001b[0m | 1           | 0.01000     | 2.219      | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.fc2              | \u001b[33m17.85716248\u001b[0m | 1           | 0.01250     | 10.304     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.q_proj     | \u001b[92m0.02517950\u001b[0m | 1           | 0.01000     | 1.655      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.k_proj     | \u001b[92m0.02352443\u001b[0m | 1           | 0.01000     | 1.611      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.v_proj     | \u001b[92m0.01266266\u001b[0m | 1           | 0.01000     | 1.654      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.dense      | \u001b[92m0.00076688\u001b[0m | 1           | 0.01000     | 1.664      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.fc1              | \u001b[92m0.07780283\u001b[0m | 1           | 0.01000     | 2.236      | 0.024        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.fc2              | \u001b[33m19.38263130\u001b[0m | 1           | 0.01250     | 10.144     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.q_proj     | \u001b[92m0.02425709\u001b[0m | 1           | 0.01000     | 2.030      | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.k_proj     | \u001b[92m0.02248980\u001b[0m | 1           | 0.01000     | 1.680      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.v_proj     | \u001b[92m0.01351467\u001b[0m | 1           | 0.01000     | 1.618      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.dense      | \u001b[92m0.00091098\u001b[0m | 1           | 0.01000     | 1.639      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.fc1              | \u001b[92m0.07670280\u001b[0m | 1           | 0.01000     | 2.187      | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.fc2              | \u001b[33m18.12718201\u001b[0m | 1           | 0.01250     | 10.237     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.q_proj     | \u001b[92m0.02466999\u001b[0m | 1           | 0.01000     | 1.671      | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.k_proj     | \u001b[92m0.02318174\u001b[0m | 1           | 0.01000     | 1.578      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.v_proj     | \u001b[92m0.01490786\u001b[0m | 1           | 0.01000     | 1.880      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.dense      | \u001b[92m0.00099493\u001b[0m | 1           | 0.01000     | 1.723      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.fc1              | \u001b[92m0.07720293\u001b[0m | 1           | 0.01000     | 2.193      | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.fc2              | \u001b[33m16.61933899\u001b[0m | 1           | 0.01250     | 10.605     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.q_proj     | \u001b[92m0.02371854\u001b[0m | 1           | 0.01000     | 1.618      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.k_proj     | \u001b[92m0.02240760\u001b[0m | 1           | 0.01000     | 1.533      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.v_proj     | \u001b[92m0.01443617\u001b[0m | 1           | 0.01000     | 1.544      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.dense      | \u001b[92m0.00119963\u001b[0m | 1           | 0.01000     | 1.916      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.fc1              | \u001b[92m0.07011247\u001b[0m | 1           | 0.01000     | 2.342      | 0.024        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.fc2              | \u001b[33m15.08327675\u001b[0m | 1           | 0.01250     | 10.413     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.q_proj     | \u001b[92m0.02400612\u001b[0m | 1           | 0.01000     | 1.660      | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.k_proj     | \u001b[92m0.02306258\u001b[0m | 1           | 0.01000     | 1.606      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.v_proj     | \u001b[92m0.01570185\u001b[0m | 1           | 0.01000     | 1.618      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.dense      | \u001b[92m0.00123014\u001b[0m | 1           | 0.01000     | 1.811      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.fc1              | \u001b[92m0.07213342\u001b[0m | 1           | 0.01000     | 2.165      | 0.024        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.fc2              | \u001b[33m16.68219757\u001b[0m | 1           | 0.01250     | 10.290     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.q_proj     | \u001b[92m0.02462451\u001b[0m | 1           | 0.01000     | 1.649      | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.k_proj     | \u001b[92m0.02329665\u001b[0m | 1           | 0.01000     | 1.576      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.v_proj     | \u001b[92m0.01630257\u001b[0m | 1           | 0.01000     | 1.581      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.dense      | \u001b[92m0.00109919\u001b[0m | 1           | 0.01000     | 1.558      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.fc1              | \u001b[92m0.07546999\u001b[0m | 1           | 0.01000     | 2.270      | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.fc2              | \u001b[33m17.92737198\u001b[0m | 1           | 0.01250     | 10.294     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.q_proj     | \u001b[92m0.02328905\u001b[0m | 1           | 0.01000     | 1.772      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.k_proj     | \u001b[92m0.02210120\u001b[0m | 1           | 0.01000     | 1.542      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.v_proj     | \u001b[92m0.01543697\u001b[0m | 1           | 0.01000     | 1.559      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.dense      | \u001b[92m0.00156830\u001b[0m | 1           | 0.01000     | 1.653      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.fc1              | \u001b[92m0.06894865\u001b[0m | 1           | 0.01000     | 2.168      | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.fc2              | \u001b[33m15.57287312\u001b[0m | 1           | 0.01250     | 10.504     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.q_proj     | \u001b[92m0.02478827\u001b[0m | 1           | 0.01000     | 1.668      | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.k_proj     | \u001b[92m0.02361409\u001b[0m | 1           | 0.01000     | 1.639      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.v_proj     | \u001b[92m0.01636356\u001b[0m | 1           | 0.01000     | 1.568      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.dense      | \u001b[92m0.00126124\u001b[0m | 1           | 0.01000     | 1.686      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.fc1              | \u001b[92m0.07375889\u001b[0m | 1           | 0.01000     | 2.227      | 0.025        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.fc2              | \u001b[33m18.16734886\u001b[0m | 1           | 0.01250     | 10.847     | 0.013        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.q_proj     | \u001b[92m0.02468135\u001b[0m | 1           | 0.01000     | 1.641      | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.k_proj     | \u001b[92m0.02368193\u001b[0m | 1           | 0.01000     | 1.645      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.v_proj     | \u001b[92m0.01648459\u001b[0m | 1           | 0.01000     | 1.603      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.dense      | \u001b[92m0.00167232\u001b[0m | 1           | 0.01000     | 1.552      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.fc1              | \u001b[92m0.06864940\u001b[0m | 1           | 0.01000     | 2.239      | 0.024        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.fc2              | \u001b[33m17.67602539\u001b[0m | 1           | 0.01250     | 10.999     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.q_proj     | \u001b[92m0.02589894\u001b[0m | 1           | 0.01000     | 1.679      | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.k_proj     | \u001b[92m0.02414727\u001b[0m | 1           | 0.01000     | 1.603      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.v_proj     | \u001b[92m0.01876141\u001b[0m | 1           | 0.01000     | 1.551      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.dense      | \u001b[92m0.00209781\u001b[0m | 1           | 0.01000     | 1.581      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.fc1              | \u001b[92m0.07837932\u001b[0m | 1           | 0.01000     | 2.139      | 0.013        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.fc2              | \u001b[91m20.18173409\u001b[0m | 1           | 0.01250     | 11.774     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.q_proj     | \u001b[92m0.02818628\u001b[0m | 1           | 0.01000     | 1.683      | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.k_proj     | \u001b[92m0.02424045\u001b[0m | 1           | 0.01000     | 1.643      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.v_proj     | \u001b[92m0.01767917\u001b[0m | 1           | 0.01000     | 1.610      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.dense      | \u001b[92m0.00256458\u001b[0m | 1           | 0.01000     | 2.074      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.fc1              | \u001b[92m0.07891945\u001b[0m | 1           | 0.01000     | 2.289      | 0.024        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.fc2              | \u001b[33m19.21055603\u001b[0m | 1           | 0.01250     | 12.551     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.q_proj     | \u001b[92m0.02893451\u001b[0m | 1           | 0.01000     | 1.641      | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.k_proj     | \u001b[92m0.02551915\u001b[0m | 1           | 0.01000     | 1.602      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.v_proj     | \u001b[92m0.02132681\u001b[0m | 1           | 0.01000     | 1.692      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.dense      | \u001b[92m0.00188350\u001b[0m | 1           | 0.01000     | 1.800      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.fc1              | \u001b[92m0.08555937\u001b[0m | 1           | 0.01000     | 2.172      | 0.024        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.fc2              | \u001b[33m19.50741577\u001b[0m | 1           | 0.01250     | 13.104     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.q_proj     | \u001b[92m0.02706281\u001b[0m | 1           | 0.01000     | 1.685      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.k_proj     | \u001b[92m0.02431571\u001b[0m | 1           | 0.01000     | 1.959      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.v_proj     | \u001b[92m0.02146942\u001b[0m | 1           | 0.01000     | 1.627      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.dense      | \u001b[92m0.00182110\u001b[0m | 1           | 0.01000     | 1.749      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.fc1              | \u001b[92m0.08827214\u001b[0m | 1           | 0.01000     | 2.348      | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.fc2              | \u001b[91m20.00148010\u001b[0m | 1           | 0.01250     | 13.245     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.q_proj     | \u001b[92m0.03165316\u001b[0m | 1           | 0.01000     | 1.820      | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.k_proj     | \u001b[92m0.02578793\u001b[0m | 1           | 0.01000     | 1.614      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.v_proj     | \u001b[92m0.02418635\u001b[0m | 1           | 0.01000     | 1.699      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.dense      | \u001b[92m0.00298061\u001b[0m | 1           | 0.01000     | 1.647      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.fc1              | \u001b[92m0.09763559\u001b[0m | 1           | 0.01000     | 2.647      | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.fc2              | \u001b[33m19.98077202\u001b[0m | 1           | 0.01250     | 14.150     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.q_proj     | \u001b[92m0.03174010\u001b[0m | 1           | 0.01000     | 1.927      | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.k_proj     | \u001b[92m0.02682114\u001b[0m | 1           | 0.01000     | 1.682      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.v_proj     | \u001b[92m0.02519226\u001b[0m | 1           | 0.01000     | 1.555      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.dense      | \u001b[92m0.00307553\u001b[0m | 1           | 0.01000     | 1.731      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.fc1              | \u001b[96m0.10103138\u001b[0m | 1           | 0.01000     | 2.313      | 0.025        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.fc2              | \u001b[33m19.39968872\u001b[0m | 1           | 0.01250     | 13.836     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.q_proj     | \u001b[92m0.03148966\u001b[0m | 1           | 0.01000     | 1.805      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.k_proj     | \u001b[92m0.02550388\u001b[0m | 1           | 0.01000     | 2.084      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.v_proj     | \u001b[92m0.02665946\u001b[0m | 1           | 0.01000     | 1.681      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.dense      | \u001b[92m0.00212079\u001b[0m | 1           | 0.01000     | 1.672      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.fc1              | \u001b[96m0.10348737\u001b[0m | 1           | 0.01000     | 2.397      | 0.025        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.fc2              | \u001b[33m17.69162560\u001b[0m | 1           | 0.01250     | 15.235     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.q_proj     | \u001b[92m0.03263386\u001b[0m | 1           | 0.01000     | 1.768      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.k_proj     | \u001b[92m0.02773156\u001b[0m | 1           | 0.01000     | 1.614      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.v_proj     | \u001b[92m0.02548177\u001b[0m | 1           | 0.01000     | 1.546      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.dense      | \u001b[92m0.00413856\u001b[0m | 1           | 0.01000     | 1.596      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.fc1              | \u001b[96m0.10574476\u001b[0m | 1           | 0.01000     | 2.128      | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.fc2              | \u001b[33m15.77829456\u001b[0m | 1           | 0.01250     | 14.986     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.q_proj     | \u001b[92m0.04266209\u001b[0m | 1           | 0.01000     | 1.722      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.k_proj     | \u001b[92m0.02868354\u001b[0m | 1           | 0.01000     | 1.822      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.v_proj     | \u001b[92m0.02343475\u001b[0m | 1           | 0.01000     | 1.535      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.dense      | \u001b[92m0.00416054\u001b[0m | 1           | 0.01000     | 1.778      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | mlp.fc1              | \u001b[96m0.10778673\u001b[0m | 1           | 0.01000     | 2.112      | 0.024        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | mlp.fc2              | \u001b[33m18.11469650\u001b[0m | 1           | 0.01250     | 15.185     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss            | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.q_proj     | \u001b[92m0.06705652\u001b[0m | 1           | 0.01000     | 1.645      | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.k_proj     | \u001b[92m0.03103530\u001b[0m | 1           | 0.01000     | 1.586      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.v_proj     | \u001b[92m0.01212827\u001b[0m | 1           | 0.01000     | 1.618      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.dense      | \u001b[92m0.00364444\u001b[0m | 1           | 0.01000     | 1.606      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | mlp.fc1              | \u001b[92m0.09138007\u001b[0m | 1           | 0.01000     | 2.113      | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | mlp.fc2              | \u001b[91m22.08231544\u001b[0m | 1           | 0.01250     | 14.972     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.q_proj', 'loss': '0.01595473', 'samples': '1', 'damp': '0.01000', 'time': '2.191', 'fwd_time': '0.522'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.k_proj', 'loss': '0.01693969', 'samples': '1', 'damp': '0.01000', 'time': '1.723', 'fwd_time': '0.005'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.v_proj', 'loss': '0.00518053', 'samples': '1', 'damp': '0.01000', 'time': '1.657', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.dense', 'loss': '0.00280963', 'samples': '1', 'damp': '0.01000', 'time': '1.682', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.fc1', 'loss': '0.01969288', 'samples': '1', 'damp': '0.01000', 'time': '2.276', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.fc2', 'loss': '19.75485229', 'samples': '1', 'damp': '0.01250', 'time': '11.830', 'fwd_time': '0.016'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.q_proj', 'loss': '0.01593773', 'samples': '1', 'damp': '0.01000', 'time': '1.580', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.k_proj', 'loss': '0.01612364', 'samples': '1', 'damp': '0.01000', 'time': '1.628', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.v_proj', 'loss': '0.00348048', 'samples': '1', 'damp': '0.01000', 'time': '1.627', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.dense', 'loss': '0.00097780', 'samples': '1', 'damp': '0.01000', 'time': '1.699', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.fc1', 'loss': '0.05221809', 'samples': '1', 'damp': '0.01000', 'time': '2.089', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.fc2', 'loss': '23.08218002', 'samples': '1', 'damp': '0.01250', 'time': '11.408', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.q_proj', 'loss': '0.01985274', 'samples': '1', 'damp': '0.01000', 'time': '1.738', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.k_proj', 'loss': '0.01969804', 'samples': '1', 'damp': '0.01000', 'time': '1.590', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.v_proj', 'loss': '0.00715708', 'samples': '1', 'damp': '0.01000', 'time': '1.720', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.dense', 'loss': '0.00069260', 'samples': '1', 'damp': '0.01000', 'time': '1.638', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.fc1', 'loss': '0.06927988', 'samples': '1', 'damp': '0.01000', 'time': '2.340', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.fc2', 'loss': '16.56796646', 'samples': '1', 'damp': '0.01250', 'time': '11.012', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.q_proj', 'loss': '0.02220270', 'samples': '1', 'damp': '0.01000', 'time': '1.679', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.k_proj', 'loss': '0.02132372', 'samples': '1', 'damp': '0.01000', 'time': '1.679', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.v_proj', 'loss': '0.01007590', 'samples': '1', 'damp': '0.01000', 'time': '1.665', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.dense', 'loss': '0.00066503', 'samples': '1', 'damp': '0.01000', 'time': '1.683', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.fc1', 'loss': '0.07844412', 'samples': '1', 'damp': '0.01000', 'time': '2.261', 'fwd_time': '0.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.fc2', 'loss': '21.68490219', 'samples': '1', 'damp': '0.01250', 'time': '11.103', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.q_proj', 'loss': '0.02196059', 'samples': '1', 'damp': '0.01000', 'time': '1.886', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.k_proj', 'loss': '0.02036222', 'samples': '1', 'damp': '0.01000', 'time': '1.768', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.v_proj', 'loss': '0.01133000', 'samples': '1', 'damp': '0.01000', 'time': '1.692', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.dense', 'loss': '0.00111006', 'samples': '1', 'damp': '0.01000', 'time': '1.724', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.fc1', 'loss': '0.07329569', 'samples': '1', 'damp': '0.01000', 'time': '2.219', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.fc2', 'loss': '17.85716248', 'samples': '1', 'damp': '0.01250', 'time': '10.304', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.q_proj', 'loss': '0.02517950', 'samples': '1', 'damp': '0.01000', 'time': '1.655', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.k_proj', 'loss': '0.02352443', 'samples': '1', 'damp': '0.01000', 'time': '1.611', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.v_proj', 'loss': '0.01266266', 'samples': '1', 'damp': '0.01000', 'time': '1.654', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.dense', 'loss': '0.00076688', 'samples': '1', 'damp': '0.01000', 'time': '1.664', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.fc1', 'loss': '0.07780283', 'samples': '1', 'damp': '0.01000', 'time': '2.236', 'fwd_time': '0.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.fc2', 'loss': '19.38263130', 'samples': '1', 'damp': '0.01250', 'time': '10.144', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.q_proj', 'loss': '0.02425709', 'samples': '1', 'damp': '0.01000', 'time': '2.030', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.k_proj', 'loss': '0.02248980', 'samples': '1', 'damp': '0.01000', 'time': '1.680', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.v_proj', 'loss': '0.01351467', 'samples': '1', 'damp': '0.01000', 'time': '1.618', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.dense', 'loss': '0.00091098', 'samples': '1', 'damp': '0.01000', 'time': '1.639', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.fc1', 'loss': '0.07670280', 'samples': '1', 'damp': '0.01000', 'time': '2.187', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.fc2', 'loss': '18.12718201', 'samples': '1', 'damp': '0.01250', 'time': '10.237', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.q_proj', 'loss': '0.02466999', 'samples': '1', 'damp': '0.01000', 'time': '1.671', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.k_proj', 'loss': '0.02318174', 'samples': '1', 'damp': '0.01000', 'time': '1.578', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.v_proj', 'loss': '0.01490786', 'samples': '1', 'damp': '0.01000', 'time': '1.880', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.dense', 'loss': '0.00099493', 'samples': '1', 'damp': '0.01000', 'time': '1.723', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.fc1', 'loss': '0.07720293', 'samples': '1', 'damp': '0.01000', 'time': '2.193', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.fc2', 'loss': '16.61933899', 'samples': '1', 'damp': '0.01250', 'time': '10.605', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.q_proj', 'loss': '0.02371854', 'samples': '1', 'damp': '0.01000', 'time': '1.618', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.k_proj', 'loss': '0.02240760', 'samples': '1', 'damp': '0.01000', 'time': '1.533', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.v_proj', 'loss': '0.01443617', 'samples': '1', 'damp': '0.01000', 'time': '1.544', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.dense', 'loss': '0.00119963', 'samples': '1', 'damp': '0.01000', 'time': '1.916', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.fc1', 'loss': '0.07011247', 'samples': '1', 'damp': '0.01000', 'time': '2.342', 'fwd_time': '0.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.fc2', 'loss': '15.08327675', 'samples': '1', 'damp': '0.01250', 'time': '10.413', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.q_proj', 'loss': '0.02400612', 'samples': '1', 'damp': '0.01000', 'time': '1.660', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.k_proj', 'loss': '0.02306258', 'samples': '1', 'damp': '0.01000', 'time': '1.606', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.v_proj', 'loss': '0.01570185', 'samples': '1', 'damp': '0.01000', 'time': '1.618', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.dense', 'loss': '0.00123014', 'samples': '1', 'damp': '0.01000', 'time': '1.811', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.fc1', 'loss': '0.07213342', 'samples': '1', 'damp': '0.01000', 'time': '2.165', 'fwd_time': '0.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.fc2', 'loss': '16.68219757', 'samples': '1', 'damp': '0.01250', 'time': '10.290', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.q_proj', 'loss': '0.02462451', 'samples': '1', 'damp': '0.01000', 'time': '1.649', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.k_proj', 'loss': '0.02329665', 'samples': '1', 'damp': '0.01000', 'time': '1.576', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.v_proj', 'loss': '0.01630257', 'samples': '1', 'damp': '0.01000', 'time': '1.581', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.dense', 'loss': '0.00109919', 'samples': '1', 'damp': '0.01000', 'time': '1.558', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.fc1', 'loss': '0.07546999', 'samples': '1', 'damp': '0.01000', 'time': '2.270', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.fc2', 'loss': '17.92737198', 'samples': '1', 'damp': '0.01250', 'time': '10.294', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.q_proj', 'loss': '0.02328905', 'samples': '1', 'damp': '0.01000', 'time': '1.772', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.k_proj', 'loss': '0.02210120', 'samples': '1', 'damp': '0.01000', 'time': '1.542', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.v_proj', 'loss': '0.01543697', 'samples': '1', 'damp': '0.01000', 'time': '1.559', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.dense', 'loss': '0.00156830', 'samples': '1', 'damp': '0.01000', 'time': '1.653', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.fc1', 'loss': '0.06894865', 'samples': '1', 'damp': '0.01000', 'time': '2.168', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.fc2', 'loss': '15.57287312', 'samples': '1', 'damp': '0.01250', 'time': '10.504', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.q_proj', 'loss': '0.02478827', 'samples': '1', 'damp': '0.01000', 'time': '1.668', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.k_proj', 'loss': '0.02361409', 'samples': '1', 'damp': '0.01000', 'time': '1.639', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.v_proj', 'loss': '0.01636356', 'samples': '1', 'damp': '0.01000', 'time': '1.568', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.dense', 'loss': '0.00126124', 'samples': '1', 'damp': '0.01000', 'time': '1.686', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.fc1', 'loss': '0.07375889', 'samples': '1', 'damp': '0.01000', 'time': '2.227', 'fwd_time': '0.025'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.fc2', 'loss': '18.16734886', 'samples': '1', 'damp': '0.01250', 'time': '10.847', 'fwd_time': '0.013'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.q_proj', 'loss': '0.02468135', 'samples': '1', 'damp': '0.01000', 'time': '1.641', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.k_proj', 'loss': '0.02368193', 'samples': '1', 'damp': '0.01000', 'time': '1.645', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.v_proj', 'loss': '0.01648459', 'samples': '1', 'damp': '0.01000', 'time': '1.603', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.dense', 'loss': '0.00167232', 'samples': '1', 'damp': '0.01000', 'time': '1.552', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.fc1', 'loss': '0.06864940', 'samples': '1', 'damp': '0.01000', 'time': '2.239', 'fwd_time': '0.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.fc2', 'loss': '17.67602539', 'samples': '1', 'damp': '0.01250', 'time': '10.999', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.q_proj', 'loss': '0.02589894', 'samples': '1', 'damp': '0.01000', 'time': '1.679', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.k_proj', 'loss': '0.02414727', 'samples': '1', 'damp': '0.01000', 'time': '1.603', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.v_proj', 'loss': '0.01876141', 'samples': '1', 'damp': '0.01000', 'time': '1.551', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.dense', 'loss': '0.00209781', 'samples': '1', 'damp': '0.01000', 'time': '1.581', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.fc1', 'loss': '0.07837932', 'samples': '1', 'damp': '0.01000', 'time': '2.139', 'fwd_time': '0.013'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.fc2', 'loss': '20.18173409', 'samples': '1', 'damp': '0.01250', 'time': '11.774', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.q_proj', 'loss': '0.02818628', 'samples': '1', 'damp': '0.01000', 'time': '1.683', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.k_proj', 'loss': '0.02424045', 'samples': '1', 'damp': '0.01000', 'time': '1.643', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.v_proj', 'loss': '0.01767917', 'samples': '1', 'damp': '0.01000', 'time': '1.610', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.dense', 'loss': '0.00256458', 'samples': '1', 'damp': '0.01000', 'time': '2.074', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.fc1', 'loss': '0.07891945', 'samples': '1', 'damp': '0.01000', 'time': '2.289', 'fwd_time': '0.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.fc2', 'loss': '19.21055603', 'samples': '1', 'damp': '0.01250', 'time': '12.551', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.q_proj', 'loss': '0.02893451', 'samples': '1', 'damp': '0.01000', 'time': '1.641', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.k_proj', 'loss': '0.02551915', 'samples': '1', 'damp': '0.01000', 'time': '1.602', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.v_proj', 'loss': '0.02132681', 'samples': '1', 'damp': '0.01000', 'time': '1.692', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.dense', 'loss': '0.00188350', 'samples': '1', 'damp': '0.01000', 'time': '1.800', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.fc1', 'loss': '0.08555937', 'samples': '1', 'damp': '0.01000', 'time': '2.172', 'fwd_time': '0.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.fc2', 'loss': '19.50741577', 'samples': '1', 'damp': '0.01250', 'time': '13.104', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.q_proj', 'loss': '0.02706281', 'samples': '1', 'damp': '0.01000', 'time': '1.685', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.k_proj', 'loss': '0.02431571', 'samples': '1', 'damp': '0.01000', 'time': '1.959', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.v_proj', 'loss': '0.02146942', 'samples': '1', 'damp': '0.01000', 'time': '1.627', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.dense', 'loss': '0.00182110', 'samples': '1', 'damp': '0.01000', 'time': '1.749', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.fc1', 'loss': '0.08827214', 'samples': '1', 'damp': '0.01000', 'time': '2.348', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.fc2', 'loss': '20.00148010', 'samples': '1', 'damp': '0.01250', 'time': '13.245', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.q_proj', 'loss': '0.03165316', 'samples': '1', 'damp': '0.01000', 'time': '1.820', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.k_proj', 'loss': '0.02578793', 'samples': '1', 'damp': '0.01000', 'time': '1.614', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.v_proj', 'loss': '0.02418635', 'samples': '1', 'damp': '0.01000', 'time': '1.699', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.dense', 'loss': '0.00298061', 'samples': '1', 'damp': '0.01000', 'time': '1.647', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.fc1', 'loss': '0.09763559', 'samples': '1', 'damp': '0.01000', 'time': '2.647', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.fc2', 'loss': '19.98077202', 'samples': '1', 'damp': '0.01250', 'time': '14.150', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.q_proj', 'loss': '0.03174010', 'samples': '1', 'damp': '0.01000', 'time': '1.927', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.k_proj', 'loss': '0.02682114', 'samples': '1', 'damp': '0.01000', 'time': '1.682', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.v_proj', 'loss': '0.02519226', 'samples': '1', 'damp': '0.01000', 'time': '1.555', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.dense', 'loss': '0.00307553', 'samples': '1', 'damp': '0.01000', 'time': '1.731', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.fc1', 'loss': '0.10103138', 'samples': '1', 'damp': '0.01000', 'time': '2.313', 'fwd_time': '0.025'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.fc2', 'loss': '19.39968872', 'samples': '1', 'damp': '0.01250', 'time': '13.836', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.q_proj', 'loss': '0.03148966', 'samples': '1', 'damp': '0.01000', 'time': '1.805', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.k_proj', 'loss': '0.02550388', 'samples': '1', 'damp': '0.01000', 'time': '2.084', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.v_proj', 'loss': '0.02665946', 'samples': '1', 'damp': '0.01000', 'time': '1.681', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.dense', 'loss': '0.00212079', 'samples': '1', 'damp': '0.01000', 'time': '1.672', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.fc1', 'loss': '0.10348737', 'samples': '1', 'damp': '0.01000', 'time': '2.397', 'fwd_time': '0.025'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.fc2', 'loss': '17.69162560', 'samples': '1', 'damp': '0.01250', 'time': '15.235', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.q_proj', 'loss': '0.03263386', 'samples': '1', 'damp': '0.01000', 'time': '1.768', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.k_proj', 'loss': '0.02773156', 'samples': '1', 'damp': '0.01000', 'time': '1.614', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.v_proj', 'loss': '0.02548177', 'samples': '1', 'damp': '0.01000', 'time': '1.546', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.dense', 'loss': '0.00413856', 'samples': '1', 'damp': '0.01000', 'time': '1.596', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.fc1', 'loss': '0.10574476', 'samples': '1', 'damp': '0.01000', 'time': '2.128', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.fc2', 'loss': '15.77829456', 'samples': '1', 'damp': '0.01250', 'time': '14.986', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.q_proj', 'loss': '0.04266209', 'samples': '1', 'damp': '0.01000', 'time': '1.722', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.k_proj', 'loss': '0.02868354', 'samples': '1', 'damp': '0.01000', 'time': '1.822', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.v_proj', 'loss': '0.02343475', 'samples': '1', 'damp': '0.01000', 'time': '1.535', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.dense', 'loss': '0.00416054', 'samples': '1', 'damp': '0.01000', 'time': '1.778', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'mlp.fc1', 'loss': '0.10778673', 'samples': '1', 'damp': '0.01000', 'time': '2.112', 'fwd_time': '0.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'mlp.fc2', 'loss': '18.11469650', 'samples': '1', 'damp': '0.01250', 'time': '15.185', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.q_proj', 'loss': '0.06705652', 'samples': '1', 'damp': '0.01000', 'time': '1.645', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.k_proj', 'loss': '0.03103530', 'samples': '1', 'damp': '0.01000', 'time': '1.586', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.v_proj', 'loss': '0.01212827', 'samples': '1', 'damp': '0.01000', 'time': '1.618', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.dense', 'loss': '0.00364444', 'samples': '1', 'damp': '0.01000', 'time': '1.606', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'mlp.fc1', 'loss': '0.09138007', 'samples': '1', 'damp': '0.01000', 'time': '2.113', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'mlp.fc2', 'loss': '22.08231544', 'samples': '1', 'damp': '0.01250', 'time': '14.972', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                         \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`      \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                               \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                  \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                            0%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'quantized_model_id' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m model.quantize(calibration_dataset)\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# save quantized model\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m model.save(quantized_model_id)\n",
      "\u001b[31mNameError\u001b[39m: name 'quantized_model_id' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "    # quantize model, the calibration_dataset should be list of dict whose keys can only be \"input_ids\" and \"attention_mask\"\n",
    "    model.quantize(calibration_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4bf5a2e9",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Format: Converting GPTQ v2 to v1                                         \n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 8,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:2.2.0\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0\n",
      "  }\n",
      "}\n",
      "Files in directory:\n",
      "config.json\n",
      "generation_config.json\n",
      "quantize_config.json\n",
      "quant_log.csv\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"transformers_version\": \"4.53.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"architectures\": [\n",
      "        \"PhiForCausalLM\"\n",
      "    ],\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bos_token_id\": null,\n",
      "    \"embd_pdrop\": 0.0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"hidden_act\": \"gelu_new\",\n",
      "    \"hidden_size\": 2048,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 8192,\n",
      "    \"layer_norm_eps\": 1e-05,\n",
      "    \"max_position_embeddings\": 2048,\n",
      "    \"model_type\": \"phi\",\n",
      "    \"num_attention_heads\": 32,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"num_key_value_heads\": 32,\n",
      "    \"partial_rotary_factor\": 0.5,\n",
      "    \"qk_layernorm\": false,\n",
      "    \"quantization_config\": {\n",
      "        \"bits\": 8,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:2.2.0\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\"\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true\n",
      "    },\n",
      "    \"resid_pdrop\": 0.0,\n",
      "    \"rope_scaling\": null,\n",
      "    \"rope_theta\": 10000.0,\n",
      "    \"tie_word_embeddings\": false,\n",
      "    \"torch_dtype\": \"float16\",\n",
      "    \"transformers_version\": \"4.53.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 51200\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 2705.17MB, 2.64GB                              \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 1581.90MB, 1.54GB                                  \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1123.27MB, 1.10GB - 41.52%                              \n"
     ]
    }
   ],
   "source": [
    "\n",
    "    # save quantized model\n",
    "    model.save(output_dir_8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82196f36",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [],
   "source": [
    "def QuantizeModelandSaveP(quantization_config, output_dir):\n",
    "    model = GPTQModel.load(original_model_dir, quantization_config)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(original_model_dir, use_fast=True)\n",
    "    calibration_dataset = [\n",
    "        tokenizer(\n",
    "            \"gptqmodel is an easy-to-use model quantization library with user-friendly apis, based on GPTQ algorithm.\"\n",
    "        )\n",
    "    ]\n",
    "\n",
    "    model.quantize(calibration_dataset)\n",
    "    model.save(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5911dd4e",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 4.2875 bpw, based on [bits: 4, group_size: 128]\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native float16): `torch.float16`                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=50256 (token='<|endoftext|>').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {}\n",
      "                  \n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`      \n",
      "\u001b[33mWARN\u001b[0m  Calibration dataset size should be more than 256. Current: 1.            \n",
      "\u001b[33mWARN\u001b[0m  The average length of input_ids of calibration_dataset should be greater than 256: actual avg: 29.0.\n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_titulus_time_07_27_2025_17h_39m_21s.log`\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss           | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.q_proj     | \u001b[93m4.61532116\u001b[0m | 1           | 0.01000     | 1.795     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.k_proj     | \u001b[93m4.82750702\u001b[0m | 1           | 0.01000     | 1.756     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.v_proj     | \u001b[93m1.49578309\u001b[0m | 1           | 0.01000     | 1.811     | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.dense      | \u001b[96m0.80343550\u001b[0m | 1           | 0.01000     | 1.747     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.fc1              | \u001b[33m5.71338367\u001b[0m | 1           | 0.01000     | 2.195     | 0.024        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.fc2              | \u001b[91m5565.74609375\u001b[0m | 1           | 0.01250     | 10.811     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.q_proj     | \u001b[93m4.63297701\u001b[0m | 1           | 0.01000     | 1.711      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.k_proj     | \u001b[93m4.68833017\u001b[0m | 1           | 0.01000     | 1.602      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.v_proj     | \u001b[93m1.00771189\u001b[0m | 1           | 0.01000     | 1.700      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.dense      | \u001b[96m0.27555346\u001b[0m | 1           | 0.01000     | 1.623      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.fc1              | \u001b[33m15.17752838\u001b[0m | 1           | 0.01000     | 2.371      | 0.025        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.fc2              | \u001b[91m6925.86669922\u001b[0m | 1           | 0.01250     | 10.373     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.q_proj     | \u001b[33m5.85178900\u001b[0m | 1           | 0.01000     | 1.715      | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.k_proj     | \u001b[33m5.79046297\u001b[0m | 1           | 0.01000     | 1.685      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.v_proj     | \u001b[93m2.11511135\u001b[0m | 1           | 0.01000     | 1.622      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.dense      | \u001b[96m0.17937197\u001b[0m | 1           | 0.01000     | 1.956      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.fc1              | \u001b[91m20.39612961\u001b[0m | 1           | 0.01000     | 2.238      | 0.025        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.fc2              | \u001b[91m5482.84619141\u001b[0m | 1           | 0.01250     | 10.753     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.q_proj     | \u001b[33m6.39534378\u001b[0m | 1           | 0.01000     | 1.603      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.k_proj     | \u001b[33m6.13710117\u001b[0m | 1           | 0.01000     | 1.619      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.v_proj     | \u001b[93m2.90941811\u001b[0m | 1           | 0.01000     | 1.811      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.dense      | \u001b[96m0.16641206\u001b[0m | 1           | 0.01000     | 1.639      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.fc1              | \u001b[91m22.68497849\u001b[0m | 1           | 0.01000     | 2.458      | 0.024        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.fc2              | \u001b[91m5748.96728516\u001b[0m | 1           | 0.01250     | 10.183     | 0.014        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.q_proj     | \u001b[33m6.36697388\u001b[0m | 1           | 0.01000     | 1.798      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.k_proj     | \u001b[33m5.89824486\u001b[0m | 1           | 0.01000     | 1.626      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.v_proj     | \u001b[93m3.27431107\u001b[0m | 1           | 0.01000     | 1.736      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.dense      | \u001b[96m0.31741595\u001b[0m | 1           | 0.01000     | 1.692      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.fc1              | \u001b[91m21.16718292\u001b[0m | 1           | 0.01000     | 2.124      | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.fc2              | \u001b[91m5206.37744141\u001b[0m | 1           | 0.01250     | 10.138     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.q_proj     | \u001b[33m7.07351780\u001b[0m | 1           | 0.01000     | 1.756      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.k_proj     | \u001b[33m6.65992355\u001b[0m | 1           | 0.01000     | 1.553      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.v_proj     | \u001b[93m3.56932306\u001b[0m | 1           | 0.01000     | 1.696      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.dense      | \u001b[96m0.23503068\u001b[0m | 1           | 0.01000     | 1.634      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.fc1              | \u001b[91m22.00125504\u001b[0m | 1           | 0.01000     | 2.123      | 0.013        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.fc2              | \u001b[91m5721.78808594\u001b[0m | 1           | 0.01250     | 10.432     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.q_proj     | \u001b[33m6.92100906\u001b[0m | 1           | 0.01000     | 1.608      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.k_proj     | \u001b[33m6.43927526\u001b[0m | 1           | 0.01000     | 1.674      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.v_proj     | \u001b[93m3.86567354\u001b[0m | 1           | 0.01000     | 1.737      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.dense      | \u001b[96m0.24750352\u001b[0m | 1           | 0.01000     | 1.531      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.fc1              | \u001b[91m21.94090462\u001b[0m | 1           | 0.01000     | 2.166      | 0.013        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.fc2              | \u001b[91m5631.04882812\u001b[0m | 1           | 0.01250     | 10.273     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.q_proj     | \u001b[33m7.01295519\u001b[0m | 1           | 0.01000     | 1.963      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.k_proj     | \u001b[33m6.57554626\u001b[0m | 1           | 0.01000     | 1.624      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.v_proj     | \u001b[93m4.23673344\u001b[0m | 1           | 0.01000     | 2.203      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.dense      | \u001b[96m0.25063556\u001b[0m | 1           | 0.01000     | 1.736      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.fc1              | \u001b[91m21.92925644\u001b[0m | 1           | 0.01000     | 2.177      | 0.024        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.fc2              | \u001b[91m5400.57031250\u001b[0m | 1           | 0.01250     | 10.537     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.q_proj     | \u001b[33m6.79898834\u001b[0m | 1           | 0.01000     | 1.645      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.k_proj     | \u001b[33m6.41143799\u001b[0m | 1           | 0.01000     | 1.533      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.v_proj     | \u001b[93m4.11173439\u001b[0m | 1           | 0.01000     | 1.600      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.dense      | \u001b[96m0.31145352\u001b[0m | 1           | 0.01000     | 1.564      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.fc1              | \u001b[91m20.06121445\u001b[0m | 1           | 0.01000     | 2.101      | 0.013        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.fc2              | \u001b[91m4580.79785156\u001b[0m | 1           | 0.01250     | 10.817     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.q_proj     | \u001b[33m6.84061813\u001b[0m | 1           | 0.01000     | 1.819      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.k_proj     | \u001b[33m6.55779600\u001b[0m | 1           | 0.01000     | 1.623      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.v_proj     | \u001b[93m4.47617912\u001b[0m | 1           | 0.01000     | 1.581      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.dense      | \u001b[96m0.35718733\u001b[0m | 1           | 0.01000     | 1.599      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.fc1              | \u001b[91m20.59479523\u001b[0m | 1           | 0.01000     | 2.096      | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.fc2              | \u001b[91m4751.00683594\u001b[0m | 1           | 0.01250     | 11.198     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.q_proj     | \u001b[33m7.08456993\u001b[0m | 1           | 0.01000     | 1.690      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.k_proj     | \u001b[33m6.69084167\u001b[0m | 1           | 0.01000     | 1.569      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.v_proj     | \u001b[93m4.67247391\u001b[0m | 1           | 0.01000     | 1.619      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.dense      | \u001b[96m0.33503735\u001b[0m | 1           | 0.01000     | 1.685      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.fc1              | \u001b[91m21.67586899\u001b[0m | 1           | 0.01000     | 2.163      | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.fc2              | \u001b[91m4714.36474609\u001b[0m | 1           | 0.01250     | 11.715     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.q_proj     | \u001b[33m6.80221462\u001b[0m | 1           | 0.01000     | 1.816      | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.k_proj     | \u001b[33m6.46317148\u001b[0m | 1           | 0.01000     | 1.710      | 0.005        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.v_proj     | \u001b[93m4.50313377\u001b[0m | 1           | 0.01000     | 1.548      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.dense      | \u001b[96m0.45004815\u001b[0m | 1           | 0.01000     | 1.598      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.fc1              | \u001b[91m20.19045639\u001b[0m | 1           | 0.01000     | 2.103      | 0.013        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.fc2              | \u001b[91m4504.46728516\u001b[0m | 1           | 0.01250     | 12.648     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.q_proj     | \u001b[33m7.15744114\u001b[0m | 1           | 0.01000     | 1.635      | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.k_proj     | \u001b[33m6.82752228\u001b[0m | 1           | 0.01000     | 1.578      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.v_proj     | \u001b[93m4.73266697\u001b[0m | 1           | 0.01000     | 1.651      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.dense      | \u001b[96m0.37881920\u001b[0m | 1           | 0.01000     | 1.525      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.fc1              | \u001b[91m21.31790733\u001b[0m | 1           | 0.01000     | 2.152      | 0.013        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.fc2              | \u001b[91m5240.15332031\u001b[0m | 1           | 0.01250     | 13.088     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.q_proj     | \u001b[33m7.12524796\u001b[0m | 1           | 0.01000     | 1.645      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.k_proj     | \u001b[33m6.80874205\u001b[0m | 1           | 0.01000     | 1.563      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.v_proj     | \u001b[93m4.72861481\u001b[0m | 1           | 0.01000     | 1.727      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.dense      | \u001b[96m0.46390644\u001b[0m | 1           | 0.01000     | 1.666      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.fc1              | \u001b[33m19.70883369\u001b[0m | 1           | 0.01000     | 2.075      | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.fc2              | \u001b[91m5000.26171875\u001b[0m | 1           | 0.01250     | 13.476     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.q_proj     | \u001b[33m7.39533997\u001b[0m | 1           | 0.01000     | 1.638      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.k_proj     | \u001b[33m6.87152815\u001b[0m | 1           | 0.01000     | 1.559      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.v_proj     | \u001b[33m5.35205507\u001b[0m | 1           | 0.01000     | 1.701      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.dense      | \u001b[96m0.54161996\u001b[0m | 1           | 0.01000     | 1.567      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.fc1              | \u001b[91m22.45143890\u001b[0m | 1           | 0.01000     | 2.089      | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.fc2              | \u001b[91m5887.35205078\u001b[0m | 1           | 0.01250     | 13.976     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.q_proj     | \u001b[33m8.01550770\u001b[0m | 1           | 0.01000     | 1.899      | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.k_proj     | \u001b[33m6.88472843\u001b[0m | 1           | 0.01000     | 1.843      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.v_proj     | \u001b[33m5.02801132\u001b[0m | 1           | 0.01000     | 1.795      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.dense      | \u001b[96m0.65421075\u001b[0m | 1           | 0.01000     | 1.846      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.fc1              | \u001b[91m22.43717575\u001b[0m | 1           | 0.01000     | 2.149      | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.fc2              | \u001b[91m5830.51367188\u001b[0m | 1           | 0.01250     | 14.252     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.q_proj     | \u001b[33m8.25330162\u001b[0m | 1           | 0.01000     | 1.949      | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.k_proj     | \u001b[33m7.25332451\u001b[0m | 1           | 0.01000     | 1.869      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.v_proj     | \u001b[33m6.06584358\u001b[0m | 1           | 0.01000     | 1.725      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.dense      | \u001b[96m0.60731566\u001b[0m | 1           | 0.01000     | 1.602      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.fc1              | \u001b[91m24.41004944\u001b[0m | 1           | 0.01000     | 2.071      | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.fc2              | \u001b[91m6205.57714844\u001b[0m | 1           | 0.01250     | 14.632     | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.q_proj     | \u001b[33m7.70821381\u001b[0m | 1           | 0.01000     | 1.620      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.k_proj     | \u001b[33m6.94354057\u001b[0m | 1           | 0.01000     | 1.541      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.v_proj     | \u001b[33m6.10996628\u001b[0m | 1           | 0.01000     | 1.596      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.dense      | \u001b[96m0.53726637\u001b[0m | 1           | 0.01000     | 1.588      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.fc1              | \u001b[91m25.26573944\u001b[0m | 1           | 0.01000     | 2.394      | 0.013        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.fc2              | \u001b[91m6616.49316406\u001b[0m | 1           | 0.01250     | 15.000     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.q_proj     | \u001b[33m8.97389984\u001b[0m | 1           | 0.01000     | 1.597      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.k_proj     | \u001b[33m7.36906528\u001b[0m | 1           | 0.01000     | 1.586      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.v_proj     | \u001b[33m6.90743923\u001b[0m | 1           | 0.01000     | 1.813      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.dense      | \u001b[96m0.83863020\u001b[0m | 1           | 0.01000     | 1.623      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.fc1              | \u001b[91m27.97096825\u001b[0m | 1           | 0.01000     | 2.148      | 0.024        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.fc2              | \u001b[91m6618.49804688\u001b[0m | 1           | 0.01250     | 15.241     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.q_proj     | \u001b[33m9.03052616\u001b[0m | 1           | 0.01000     | 1.665      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.k_proj     | \u001b[33m7.62771130\u001b[0m | 1           | 0.01000     | 2.146      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.v_proj     | \u001b[33m7.18200970\u001b[0m | 1           | 0.01000     | 1.625      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.dense      | \u001b[96m0.93966275\u001b[0m | 1           | 0.01000     | 3.016      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.fc1              | \u001b[91m28.91572571\u001b[0m | 1           | 0.01000     | 2.445      | 0.025        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.fc2              | \u001b[91m6089.71484375\u001b[0m | 1           | 0.01250     | 15.903     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.q_proj     | \u001b[33m8.99487209\u001b[0m | 1           | 0.01000     | 1.649      | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.k_proj     | \u001b[33m7.31513500\u001b[0m | 1           | 0.01000     | 1.594      | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.v_proj     | \u001b[33m7.71275711\u001b[0m | 1           | 0.01000     | 1.680      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.dense      | \u001b[96m0.69048375\u001b[0m | 1           | 0.01000     | 2.115      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.fc1              | \u001b[91m29.84140396\u001b[0m | 1           | 0.01000     | 2.182      | 0.024        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.fc2              | \u001b[91m5670.08642578\u001b[0m | 1           | 0.01250     | 15.431     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.q_proj     | \u001b[33m9.36599922\u001b[0m | 1           | 0.01000     | 1.671      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.k_proj     | \u001b[33m7.90890694\u001b[0m | 1           | 0.01000     | 1.567      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.v_proj     | \u001b[33m7.28874493\u001b[0m | 1           | 0.01000     | 1.616      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.dense      | \u001b[93m1.19903409\u001b[0m | 1           | 0.01000     | 1.711      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.fc1              | \u001b[91m30.33419037\u001b[0m | 1           | 0.01000     | 2.425      | 0.013        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.fc2              | \u001b[91m4982.22363281\u001b[0m | 1           | 0.01250     | 15.994     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.q_proj     | \u001b[33m12.03855324\u001b[0m | 1           | 0.01000     | 1.757      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.k_proj     | \u001b[33m8.11512375\u001b[0m | 1           | 0.01000     | 1.591      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.v_proj     | \u001b[33m6.61344004\u001b[0m | 1           | 0.01000     | 1.541      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.dense      | \u001b[93m1.15910769\u001b[0m | 1           | 0.01000     | 1.716      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | mlp.fc1              | \u001b[91m30.49663162\u001b[0m | 1           | 0.01000     | 2.064      | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | mlp.fc2              | \u001b[91m5694.48828125\u001b[0m | 1           | 0.01250     | 15.717     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss              | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.q_proj     | \u001b[33m18.27921677\u001b[0m | 1           | 0.01000     | 1.729      | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.k_proj     | \u001b[33m8.51222801\u001b[0m | 1           | 0.01000     | 1.645      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.v_proj     | \u001b[93m3.34947085\u001b[0m | 1           | 0.01000     | 1.640      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.dense      | \u001b[93m1.36634028\u001b[0m | 1           | 0.01000     | 1.651      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | mlp.fc1              | \u001b[91m25.25232697\u001b[0m | 1           | 0.01000     | 2.357      | 0.025        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | mlp.fc2              | \u001b[91m7196.59570312\u001b[0m | 1           | 0.01250     | 15.610     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.q_proj', 'loss': '4.61532116', 'samples': '1', 'damp': '0.01000', 'time': '1.795', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.k_proj', 'loss': '4.82750702', 'samples': '1', 'damp': '0.01000', 'time': '1.756', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.v_proj', 'loss': '1.49578309', 'samples': '1', 'damp': '0.01000', 'time': '1.811', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.dense', 'loss': '0.80343550', 'samples': '1', 'damp': '0.01000', 'time': '1.747', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.fc1', 'loss': '5.71338367', 'samples': '1', 'damp': '0.01000', 'time': '2.195', 'fwd_time': '0.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.fc2', 'loss': '5565.74609375', 'samples': '1', 'damp': '0.01250', 'time': '10.811', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.q_proj', 'loss': '4.63297701', 'samples': '1', 'damp': '0.01000', 'time': '1.711', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.k_proj', 'loss': '4.68833017', 'samples': '1', 'damp': '0.01000', 'time': '1.602', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.v_proj', 'loss': '1.00771189', 'samples': '1', 'damp': '0.01000', 'time': '1.700', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.dense', 'loss': '0.27555346', 'samples': '1', 'damp': '0.01000', 'time': '1.623', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.fc1', 'loss': '15.17752838', 'samples': '1', 'damp': '0.01000', 'time': '2.371', 'fwd_time': '0.025'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.fc2', 'loss': '6925.86669922', 'samples': '1', 'damp': '0.01250', 'time': '10.373', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.q_proj', 'loss': '5.85178900', 'samples': '1', 'damp': '0.01000', 'time': '1.715', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.k_proj', 'loss': '5.79046297', 'samples': '1', 'damp': '0.01000', 'time': '1.685', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.v_proj', 'loss': '2.11511135', 'samples': '1', 'damp': '0.01000', 'time': '1.622', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.dense', 'loss': '0.17937197', 'samples': '1', 'damp': '0.01000', 'time': '1.956', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.fc1', 'loss': '20.39612961', 'samples': '1', 'damp': '0.01000', 'time': '2.238', 'fwd_time': '0.025'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.fc2', 'loss': '5482.84619141', 'samples': '1', 'damp': '0.01250', 'time': '10.753', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.q_proj', 'loss': '6.39534378', 'samples': '1', 'damp': '0.01000', 'time': '1.603', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.k_proj', 'loss': '6.13710117', 'samples': '1', 'damp': '0.01000', 'time': '1.619', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.v_proj', 'loss': '2.90941811', 'samples': '1', 'damp': '0.01000', 'time': '1.811', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.dense', 'loss': '0.16641206', 'samples': '1', 'damp': '0.01000', 'time': '1.639', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.fc1', 'loss': '22.68497849', 'samples': '1', 'damp': '0.01000', 'time': '2.458', 'fwd_time': '0.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.fc2', 'loss': '5748.96728516', 'samples': '1', 'damp': '0.01250', 'time': '10.183', 'fwd_time': '0.014'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.q_proj', 'loss': '6.36697388', 'samples': '1', 'damp': '0.01000', 'time': '1.798', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.k_proj', 'loss': '5.89824486', 'samples': '1', 'damp': '0.01000', 'time': '1.626', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.v_proj', 'loss': '3.27431107', 'samples': '1', 'damp': '0.01000', 'time': '1.736', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.dense', 'loss': '0.31741595', 'samples': '1', 'damp': '0.01000', 'time': '1.692', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.fc1', 'loss': '21.16718292', 'samples': '1', 'damp': '0.01000', 'time': '2.124', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.fc2', 'loss': '5206.37744141', 'samples': '1', 'damp': '0.01250', 'time': '10.138', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.q_proj', 'loss': '7.07351780', 'samples': '1', 'damp': '0.01000', 'time': '1.756', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.k_proj', 'loss': '6.65992355', 'samples': '1', 'damp': '0.01000', 'time': '1.553', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.v_proj', 'loss': '3.56932306', 'samples': '1', 'damp': '0.01000', 'time': '1.696', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.dense', 'loss': '0.23503068', 'samples': '1', 'damp': '0.01000', 'time': '1.634', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.fc1', 'loss': '22.00125504', 'samples': '1', 'damp': '0.01000', 'time': '2.123', 'fwd_time': '0.013'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.fc2', 'loss': '5721.78808594', 'samples': '1', 'damp': '0.01250', 'time': '10.432', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.q_proj', 'loss': '6.92100906', 'samples': '1', 'damp': '0.01000', 'time': '1.608', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.k_proj', 'loss': '6.43927526', 'samples': '1', 'damp': '0.01000', 'time': '1.674', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.v_proj', 'loss': '3.86567354', 'samples': '1', 'damp': '0.01000', 'time': '1.737', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.dense', 'loss': '0.24750352', 'samples': '1', 'damp': '0.01000', 'time': '1.531', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.fc1', 'loss': '21.94090462', 'samples': '1', 'damp': '0.01000', 'time': '2.166', 'fwd_time': '0.013'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.fc2', 'loss': '5631.04882812', 'samples': '1', 'damp': '0.01250', 'time': '10.273', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.q_proj', 'loss': '7.01295519', 'samples': '1', 'damp': '0.01000', 'time': '1.963', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.k_proj', 'loss': '6.57554626', 'samples': '1', 'damp': '0.01000', 'time': '1.624', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.v_proj', 'loss': '4.23673344', 'samples': '1', 'damp': '0.01000', 'time': '2.203', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.dense', 'loss': '0.25063556', 'samples': '1', 'damp': '0.01000', 'time': '1.736', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.fc1', 'loss': '21.92925644', 'samples': '1', 'damp': '0.01000', 'time': '2.177', 'fwd_time': '0.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.fc2', 'loss': '5400.57031250', 'samples': '1', 'damp': '0.01250', 'time': '10.537', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.q_proj', 'loss': '6.79898834', 'samples': '1', 'damp': '0.01000', 'time': '1.645', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.k_proj', 'loss': '6.41143799', 'samples': '1', 'damp': '0.01000', 'time': '1.533', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.v_proj', 'loss': '4.11173439', 'samples': '1', 'damp': '0.01000', 'time': '1.600', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.dense', 'loss': '0.31145352', 'samples': '1', 'damp': '0.01000', 'time': '1.564', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.fc1', 'loss': '20.06121445', 'samples': '1', 'damp': '0.01000', 'time': '2.101', 'fwd_time': '0.013'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.fc2', 'loss': '4580.79785156', 'samples': '1', 'damp': '0.01250', 'time': '10.817', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.q_proj', 'loss': '6.84061813', 'samples': '1', 'damp': '0.01000', 'time': '1.819', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.k_proj', 'loss': '6.55779600', 'samples': '1', 'damp': '0.01000', 'time': '1.623', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.v_proj', 'loss': '4.47617912', 'samples': '1', 'damp': '0.01000', 'time': '1.581', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.dense', 'loss': '0.35718733', 'samples': '1', 'damp': '0.01000', 'time': '1.599', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.fc1', 'loss': '20.59479523', 'samples': '1', 'damp': '0.01000', 'time': '2.096', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.fc2', 'loss': '4751.00683594', 'samples': '1', 'damp': '0.01250', 'time': '11.198', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.q_proj', 'loss': '7.08456993', 'samples': '1', 'damp': '0.01000', 'time': '1.690', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.k_proj', 'loss': '6.69084167', 'samples': '1', 'damp': '0.01000', 'time': '1.569', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.v_proj', 'loss': '4.67247391', 'samples': '1', 'damp': '0.01000', 'time': '1.619', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.dense', 'loss': '0.33503735', 'samples': '1', 'damp': '0.01000', 'time': '1.685', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.fc1', 'loss': '21.67586899', 'samples': '1', 'damp': '0.01000', 'time': '2.163', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.fc2', 'loss': '4714.36474609', 'samples': '1', 'damp': '0.01250', 'time': '11.715', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.q_proj', 'loss': '6.80221462', 'samples': '1', 'damp': '0.01000', 'time': '1.816', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.k_proj', 'loss': '6.46317148', 'samples': '1', 'damp': '0.01000', 'time': '1.710', 'fwd_time': '0.005'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.v_proj', 'loss': '4.50313377', 'samples': '1', 'damp': '0.01000', 'time': '1.548', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.dense', 'loss': '0.45004815', 'samples': '1', 'damp': '0.01000', 'time': '1.598', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.fc1', 'loss': '20.19045639', 'samples': '1', 'damp': '0.01000', 'time': '2.103', 'fwd_time': '0.013'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.fc2', 'loss': '4504.46728516', 'samples': '1', 'damp': '0.01250', 'time': '12.648', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.q_proj', 'loss': '7.15744114', 'samples': '1', 'damp': '0.01000', 'time': '1.635', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.k_proj', 'loss': '6.82752228', 'samples': '1', 'damp': '0.01000', 'time': '1.578', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.v_proj', 'loss': '4.73266697', 'samples': '1', 'damp': '0.01000', 'time': '1.651', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.dense', 'loss': '0.37881920', 'samples': '1', 'damp': '0.01000', 'time': '1.525', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.fc1', 'loss': '21.31790733', 'samples': '1', 'damp': '0.01000', 'time': '2.152', 'fwd_time': '0.013'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.fc2', 'loss': '5240.15332031', 'samples': '1', 'damp': '0.01250', 'time': '13.088', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.q_proj', 'loss': '7.12524796', 'samples': '1', 'damp': '0.01000', 'time': '1.645', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.k_proj', 'loss': '6.80874205', 'samples': '1', 'damp': '0.01000', 'time': '1.563', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.v_proj', 'loss': '4.72861481', 'samples': '1', 'damp': '0.01000', 'time': '1.727', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.dense', 'loss': '0.46390644', 'samples': '1', 'damp': '0.01000', 'time': '1.666', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.fc1', 'loss': '19.70883369', 'samples': '1', 'damp': '0.01000', 'time': '2.075', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.fc2', 'loss': '5000.26171875', 'samples': '1', 'damp': '0.01250', 'time': '13.476', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.q_proj', 'loss': '7.39533997', 'samples': '1', 'damp': '0.01000', 'time': '1.638', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.k_proj', 'loss': '6.87152815', 'samples': '1', 'damp': '0.01000', 'time': '1.559', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.v_proj', 'loss': '5.35205507', 'samples': '1', 'damp': '0.01000', 'time': '1.701', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.dense', 'loss': '0.54161996', 'samples': '1', 'damp': '0.01000', 'time': '1.567', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.fc1', 'loss': '22.45143890', 'samples': '1', 'damp': '0.01000', 'time': '2.089', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.fc2', 'loss': '5887.35205078', 'samples': '1', 'damp': '0.01250', 'time': '13.976', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.q_proj', 'loss': '8.01550770', 'samples': '1', 'damp': '0.01000', 'time': '1.899', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.k_proj', 'loss': '6.88472843', 'samples': '1', 'damp': '0.01000', 'time': '1.843', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.v_proj', 'loss': '5.02801132', 'samples': '1', 'damp': '0.01000', 'time': '1.795', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.dense', 'loss': '0.65421075', 'samples': '1', 'damp': '0.01000', 'time': '1.846', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.fc1', 'loss': '22.43717575', 'samples': '1', 'damp': '0.01000', 'time': '2.149', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.fc2', 'loss': '5830.51367188', 'samples': '1', 'damp': '0.01250', 'time': '14.252', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.q_proj', 'loss': '8.25330162', 'samples': '1', 'damp': '0.01000', 'time': '1.949', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.k_proj', 'loss': '7.25332451', 'samples': '1', 'damp': '0.01000', 'time': '1.869', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.v_proj', 'loss': '6.06584358', 'samples': '1', 'damp': '0.01000', 'time': '1.725', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.dense', 'loss': '0.60731566', 'samples': '1', 'damp': '0.01000', 'time': '1.602', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.fc1', 'loss': '24.41004944', 'samples': '1', 'damp': '0.01000', 'time': '2.071', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.fc2', 'loss': '6205.57714844', 'samples': '1', 'damp': '0.01250', 'time': '14.632', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.q_proj', 'loss': '7.70821381', 'samples': '1', 'damp': '0.01000', 'time': '1.620', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.k_proj', 'loss': '6.94354057', 'samples': '1', 'damp': '0.01000', 'time': '1.541', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.v_proj', 'loss': '6.10996628', 'samples': '1', 'damp': '0.01000', 'time': '1.596', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.dense', 'loss': '0.53726637', 'samples': '1', 'damp': '0.01000', 'time': '1.588', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.fc1', 'loss': '25.26573944', 'samples': '1', 'damp': '0.01000', 'time': '2.394', 'fwd_time': '0.013'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.fc2', 'loss': '6616.49316406', 'samples': '1', 'damp': '0.01250', 'time': '15.000', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.q_proj', 'loss': '8.97389984', 'samples': '1', 'damp': '0.01000', 'time': '1.597', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.k_proj', 'loss': '7.36906528', 'samples': '1', 'damp': '0.01000', 'time': '1.586', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.v_proj', 'loss': '6.90743923', 'samples': '1', 'damp': '0.01000', 'time': '1.813', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.dense', 'loss': '0.83863020', 'samples': '1', 'damp': '0.01000', 'time': '1.623', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.fc1', 'loss': '27.97096825', 'samples': '1', 'damp': '0.01000', 'time': '2.148', 'fwd_time': '0.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.fc2', 'loss': '6618.49804688', 'samples': '1', 'damp': '0.01250', 'time': '15.241', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.q_proj', 'loss': '9.03052616', 'samples': '1', 'damp': '0.01000', 'time': '1.665', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.k_proj', 'loss': '7.62771130', 'samples': '1', 'damp': '0.01000', 'time': '2.146', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.v_proj', 'loss': '7.18200970', 'samples': '1', 'damp': '0.01000', 'time': '1.625', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.dense', 'loss': '0.93966275', 'samples': '1', 'damp': '0.01000', 'time': '3.016', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.fc1', 'loss': '28.91572571', 'samples': '1', 'damp': '0.01000', 'time': '2.445', 'fwd_time': '0.025'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.fc2', 'loss': '6089.71484375', 'samples': '1', 'damp': '0.01250', 'time': '15.903', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.q_proj', 'loss': '8.99487209', 'samples': '1', 'damp': '0.01000', 'time': '1.649', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.k_proj', 'loss': '7.31513500', 'samples': '1', 'damp': '0.01000', 'time': '1.594', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.v_proj', 'loss': '7.71275711', 'samples': '1', 'damp': '0.01000', 'time': '1.680', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.dense', 'loss': '0.69048375', 'samples': '1', 'damp': '0.01000', 'time': '2.115', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.fc1', 'loss': '29.84140396', 'samples': '1', 'damp': '0.01000', 'time': '2.182', 'fwd_time': '0.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.fc2', 'loss': '5670.08642578', 'samples': '1', 'damp': '0.01250', 'time': '15.431', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.q_proj', 'loss': '9.36599922', 'samples': '1', 'damp': '0.01000', 'time': '1.671', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.k_proj', 'loss': '7.90890694', 'samples': '1', 'damp': '0.01000', 'time': '1.567', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.v_proj', 'loss': '7.28874493', 'samples': '1', 'damp': '0.01000', 'time': '1.616', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.dense', 'loss': '1.19903409', 'samples': '1', 'damp': '0.01000', 'time': '1.711', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.fc1', 'loss': '30.33419037', 'samples': '1', 'damp': '0.01000', 'time': '2.425', 'fwd_time': '0.013'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.fc2', 'loss': '4982.22363281', 'samples': '1', 'damp': '0.01250', 'time': '15.994', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.q_proj', 'loss': '12.03855324', 'samples': '1', 'damp': '0.01000', 'time': '1.757', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.k_proj', 'loss': '8.11512375', 'samples': '1', 'damp': '0.01000', 'time': '1.591', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.v_proj', 'loss': '6.61344004', 'samples': '1', 'damp': '0.01000', 'time': '1.541', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.dense', 'loss': '1.15910769', 'samples': '1', 'damp': '0.01000', 'time': '1.716', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'mlp.fc1', 'loss': '30.49663162', 'samples': '1', 'damp': '0.01000', 'time': '2.064', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'mlp.fc2', 'loss': '5694.48828125', 'samples': '1', 'damp': '0.01250', 'time': '15.717', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.q_proj', 'loss': '18.27921677', 'samples': '1', 'damp': '0.01000', 'time': '1.729', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.k_proj', 'loss': '8.51222801', 'samples': '1', 'damp': '0.01000', 'time': '1.645', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.v_proj', 'loss': '3.34947085', 'samples': '1', 'damp': '0.01000', 'time': '1.640', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.dense', 'loss': '1.36634028', 'samples': '1', 'damp': '0.01000', 'time': '1.651', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'mlp.fc1', 'loss': '25.25232697', 'samples': '1', 'damp': '0.01000', 'time': '2.357', 'fwd_time': '0.025'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'mlp.fc2', 'loss': '7196.59570312', 'samples': '1', 'damp': '0.01250', 'time': '15.610', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                         \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`      \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                               \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                  \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                            0%\n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 4,\n",
      "  \"group_size\": 128,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:2.2.0\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0\n",
      "  }\n",
      "}\n",
      "Files in directory:\n",
      "config.json\n",
      "generation_config.json\n",
      "quantize_config.json\n",
      "quant_log.csv\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"transformers_version\": \"4.53.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"architectures\": [\n",
      "        \"PhiForCausalLM\"\n",
      "    ],\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bos_token_id\": null,\n",
      "    \"embd_pdrop\": 0.0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"hidden_act\": \"gelu_new\",\n",
      "    \"hidden_size\": 2048,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 8192,\n",
      "    \"layer_norm_eps\": 1e-05,\n",
      "    \"max_position_embeddings\": 2048,\n",
      "    \"model_type\": \"phi\",\n",
      "    \"num_attention_heads\": 32,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"num_key_value_heads\": 32,\n",
      "    \"partial_rotary_factor\": 0.5,\n",
      "    \"qk_layernorm\": false,\n",
      "    \"quantization_config\": {\n",
      "        \"bits\": 4,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 128,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:2.2.0\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\"\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true\n",
      "    },\n",
      "    \"resid_pdrop\": 0.0,\n",
      "    \"rope_scaling\": null,\n",
      "    \"rope_theta\": 10000.0,\n",
      "    \"tie_word_embeddings\": false,\n",
      "    \"torch_dtype\": \"float16\",\n",
      "    \"transformers_version\": \"4.53.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 51200\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 2705.17MB, 2.64GB                              \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 1001.40MB, 0.98GB                                  \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1703.77MB, 1.66GB - 62.98%                              \n"
     ]
    }
   ],
   "source": [
    "QuantizeModelandSaveP(quantize_config_4, output_dir_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e34ffd0b",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 2.44375 bpw, based on [bits: 2, group_size: 64]\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native float16): `torch.float16`                     \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=50256 (token='<|endoftext|>').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {}\n",
      "                  \n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[]`                                                   \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`      \n",
      "\u001b[33mWARN\u001b[0m  Calibration dataset size should be more than 256. Current: 1.            \n",
      "\u001b[33mWARN\u001b[0m  The average length of input_ids of calibration_dataset should be greater than 256: actual avg: 29.0.\n",
      "\u001b[32mINFO\u001b[0m  Process: progress logs for `gptq` will be streamed to file: `gptq_log_undegraded_time_07_27_2025_17h_49m_06s.log`\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss             | samples     | damp        | time      | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.q_proj     | \u001b[91m101.11450195\u001b[0m | 1           | 0.01000     | 1.753     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.k_proj     | \u001b[91m100.74877930\u001b[0m | 1           | 0.01000     | 1.738     | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.v_proj     | \u001b[91m32.58218765\u001b[0m | 1           | 0.01000     | 1.603     | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | self_attn.dense      | \u001b[33m16.47297478\u001b[0m | 1           | 0.01000     | 1.622     | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.fc1              | \u001b[91m126.84600830\u001b[0m | 1           | 0.01000     | 2.075     | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 0         | mlp.fc2              | \u001b[91m106996.78906250\u001b[0m | 1           | 0.01250     | 15.632     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time       | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.q_proj     | \u001b[91m106.33714294\u001b[0m | 1           | 0.01000     | 1.811      | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.k_proj     | \u001b[91m107.21305847\u001b[0m | 1           | 0.01000     | 1.660      | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.v_proj     | \u001b[91m22.91265106\u001b[0m | 1           | 0.01000     | 1.560      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | self_attn.dense      | \u001b[93m4.92205477\u001b[0m | 1           | 0.01000     | 1.722      | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.fc1              | \u001b[91m351.60754395\u001b[0m | 1           | 0.01000     | 2.182      | 0.024        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 1         | mlp.fc2              | \u001b[91m115594.28125000\u001b[0m | 1           | 0.01250     | 459.037     | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.q_proj     | \u001b[91m130.03463745\u001b[0m | 1           | 0.01000     | 1.622       | 0.008        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.k_proj     | \u001b[91m129.76574707\u001b[0m | 1           | 0.01000     | 1.367       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.v_proj     | \u001b[91m46.67893982\u001b[0m | 1           | 0.01000     | 1.390       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | self_attn.dense      | \u001b[33m10.42765903\u001b[0m | 1           | 0.01000     | 1.495       | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.fc1              | \u001b[91m460.92077637\u001b[0m | 1           | 0.01000     | 2.128       | 0.024        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 2         | mlp.fc2              | \u001b[91m94196.98437500\u001b[0m | 1           | 0.01250     | 8.948       | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.q_proj     | \u001b[91m141.92184448\u001b[0m | 1           | 0.01000     | 1.342       | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.k_proj     | \u001b[91m137.03282166\u001b[0m | 1           | 0.01000     | 1.212       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.v_proj     | \u001b[91m63.80435562\u001b[0m | 1           | 0.01000     | 1.234       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | self_attn.dense      | \u001b[33m16.43011284\u001b[0m | 1           | 0.01000     | 1.264       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.fc1              | \u001b[91m511.16094971\u001b[0m | 1           | 0.01000     | 1.807       | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 3         | mlp.fc2              | \u001b[91m63795.21484375\u001b[0m | 1           | 0.01250     | 9.502       | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.q_proj     | \u001b[91m138.67892456\u001b[0m | 1           | 0.01000     | 1.515       | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.k_proj     | \u001b[91m132.08502197\u001b[0m | 1           | 0.01000     | 1.340       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.v_proj     | \u001b[91m71.12254333\u001b[0m | 1           | 0.01000     | 1.243       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | self_attn.dense      | \u001b[91m20.34513474\u001b[0m | 1           | 0.01000     | 1.385       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.fc1              | \u001b[91m477.45983887\u001b[0m | 1           | 0.01000     | 2.050       | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 4         | mlp.fc2              | \u001b[91m54975.36718750\u001b[0m | 1           | 0.01250     | 10.434      | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.q_proj     | \u001b[91m167.74491882\u001b[0m | 1           | 0.01000     | 1.313       | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.k_proj     | \u001b[91m160.31777954\u001b[0m | 1           | 0.01000     | 1.294       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.v_proj     | \u001b[91m82.51467896\u001b[0m | 1           | 0.01000     | 1.391       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | self_attn.dense      | \u001b[33m17.27911377\u001b[0m | 1           | 0.01000     | 1.347       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.fc1              | \u001b[91m527.64160156\u001b[0m | 1           | 0.01000     | 1.987       | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 5         | mlp.fc2              | \u001b[91m75403.63281250\u001b[0m | 1           | 0.01250     | 12.733      | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.q_proj     | \u001b[91m160.98776245\u001b[0m | 1           | 0.01000     | 1.376       | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.k_proj     | \u001b[91m154.01689148\u001b[0m | 1           | 0.01000     | 1.272       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.v_proj     | \u001b[91m88.86698914\u001b[0m | 1           | 0.01000     | 1.278       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | self_attn.dense      | \u001b[91m20.87316513\u001b[0m | 1           | 0.01000     | 1.247       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.fc1              | \u001b[91m519.89935303\u001b[0m | 1           | 0.01000     | 1.907       | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 6         | mlp.fc2              | \u001b[91m73255.48437500\u001b[0m | 1           | 0.01250     | 9.412       | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.q_proj     | \u001b[91m164.87022400\u001b[0m | 1           | 0.01000     | 1.354       | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.k_proj     | \u001b[91m163.55050659\u001b[0m | 1           | 0.01000     | 1.239       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.v_proj     | \u001b[91m98.75587463\u001b[0m | 1           | 0.01000     | 1.265       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | self_attn.dense      | \u001b[91m22.03719330\u001b[0m | 1           | 0.01000     | 1.297       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.fc1              | \u001b[91m527.94042969\u001b[0m | 1           | 0.01000     | 1.868       | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 7         | mlp.fc2              | \u001b[91m66821.23437500\u001b[0m | 1           | 0.01250     | 11.115      | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.q_proj     | \u001b[91m165.80398560\u001b[0m | 1           | 0.01000     | 1.325       | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.k_proj     | \u001b[91m165.28674316\u001b[0m | 1           | 0.01000     | 1.328       | 0.005        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.v_proj     | \u001b[91m99.01081848\u001b[0m | 1           | 0.01000     | 1.416       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | self_attn.dense      | \u001b[91m25.10817719\u001b[0m | 1           | 0.01000     | 1.273       | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.fc1              | \u001b[91m502.25500488\u001b[0m | 1           | 0.01000     | 1.857       | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 8         | mlp.fc2              | \u001b[91m53658.82031250\u001b[0m | 1           | 0.01250     | 11.122      | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.q_proj     | \u001b[91m169.34075928\u001b[0m | 1           | 0.01000     | 1.329       | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.k_proj     | \u001b[91m169.55943298\u001b[0m | 1           | 0.01000     | 1.274       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.v_proj     | \u001b[91m109.37715912\u001b[0m | 1           | 0.01000     | 1.310       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | self_attn.dense      | \u001b[91m32.04048538\u001b[0m | 1           | 0.01000     | 1.259       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.fc1              | \u001b[91m511.23162842\u001b[0m | 1           | 0.01000     | 1.816       | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 9         | mlp.fc2              | \u001b[91m56574.25000000\u001b[0m | 1           | 0.01250     | 9.608       | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.q_proj     | \u001b[91m171.60531616\u001b[0m | 1           | 0.01000     | 1.304       | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.k_proj     | \u001b[91m174.18914795\u001b[0m | 1           | 0.01000     | 1.237       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.v_proj     | \u001b[91m112.08305359\u001b[0m | 1           | 0.01000     | 1.298       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | self_attn.dense      | \u001b[91m37.83961105\u001b[0m | 1           | 0.01000     | 1.307       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.fc1              | \u001b[91m532.65209961\u001b[0m | 1           | 0.01000     | 1.905       | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 10        | mlp.fc2              | \u001b[91m52050.59375000\u001b[0m | 1           | 0.01250     | 9.918       | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.q_proj     | \u001b[91m164.91189575\u001b[0m | 1           | 0.01000     | 1.338       | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.k_proj     | \u001b[91m167.51042175\u001b[0m | 1           | 0.01000     | 1.254       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.v_proj     | \u001b[91m111.37121582\u001b[0m | 1           | 0.01000     | 1.330       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | self_attn.dense      | \u001b[91m37.10642242\u001b[0m | 1           | 0.01000     | 1.359       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.fc1              | \u001b[91m499.54428101\u001b[0m | 1           | 0.01000     | 1.907       | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 11        | mlp.fc2              | \u001b[91m50568.96484375\u001b[0m | 1           | 0.01250     | 9.282       | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.q_proj     | \u001b[91m176.05184937\u001b[0m | 1           | 0.01000     | 1.310       | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.k_proj     | \u001b[91m178.61288452\u001b[0m | 1           | 0.01000     | 1.248       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.v_proj     | \u001b[91m117.83205414\u001b[0m | 1           | 0.01000     | 1.359       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | self_attn.dense      | \u001b[91m32.42706299\u001b[0m | 1           | 0.01000     | 1.326       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.fc1              | \u001b[91m528.94921875\u001b[0m | 1           | 0.01000     | 1.827       | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 12        | mlp.fc2              | \u001b[91m61275.61718750\u001b[0m | 1           | 0.01250     | 8.681       | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.q_proj     | \u001b[91m179.52462769\u001b[0m | 1           | 0.01000     | 1.688       | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.k_proj     | \u001b[91m185.83723450\u001b[0m | 1           | 0.01000     | 1.305       | 0.002        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.v_proj     | \u001b[91m120.39207458\u001b[0m | 1           | 0.01000     | 1.302       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | self_attn.dense      | \u001b[91m31.16594315\u001b[0m | 1           | 0.01000     | 1.289       | 0.005        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.fc1              | \u001b[91m505.40542603\u001b[0m | 1           | 0.01000     | 1.916       | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 13        | mlp.fc2              | \u001b[91m58707.30468750\u001b[0m | 1           | 0.01250     | 8.846       | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.q_proj     | \u001b[91m188.16873169\u001b[0m | 1           | 0.01000     | 1.313       | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.k_proj     | \u001b[91m188.36560059\u001b[0m | 1           | 0.01000     | 1.321       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.v_proj     | \u001b[91m137.21459961\u001b[0m | 1           | 0.01000     | 1.297       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | self_attn.dense      | \u001b[91m40.09495544\u001b[0m | 1           | 0.01000     | 1.348       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.fc1              | \u001b[91m572.48791504\u001b[0m | 1           | 0.01000     | 1.897       | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 14        | mlp.fc2              | \u001b[91m83905.14843750\u001b[0m | 1           | 0.01250     | 8.963       | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.q_proj     | \u001b[91m210.46829224\u001b[0m | 1           | 0.01000     | 1.395       | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.k_proj     | \u001b[91m195.38082886\u001b[0m | 1           | 0.01000     | 1.463       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.v_proj     | \u001b[91m133.09068298\u001b[0m | 1           | 0.01000     | 1.325       | 0.005        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | self_attn.dense      | \u001b[91m35.28706360\u001b[0m | 1           | 0.01000     | 1.242       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.fc1              | \u001b[91m590.91345215\u001b[0m | 1           | 0.01000     | 1.770       | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 15        | mlp.fc2              | \u001b[91m73120.64062500\u001b[0m | 1           | 0.01250     | 8.867       | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.q_proj     | \u001b[91m229.86494446\u001b[0m | 1           | 0.01000     | 1.281       | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.k_proj     | \u001b[91m213.65203857\u001b[0m | 1           | 0.01000     | 1.217       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.v_proj     | \u001b[91m164.72760010\u001b[0m | 1           | 0.01000     | 1.237       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | self_attn.dense      | \u001b[91m41.04069901\u001b[0m | 1           | 0.01000     | 1.232       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.fc1              | \u001b[91m660.77929688\u001b[0m | 1           | 0.01000     | 1.761       | 0.012        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 16        | mlp.fc2              | \u001b[91m86064.12500000\u001b[0m | 1           | 0.01250     | 10.419      | 0.017        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.q_proj     | \u001b[91m215.35038757\u001b[0m | 1           | 0.01000     | 1.300       | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.k_proj     | \u001b[91m209.51354980\u001b[0m | 1           | 0.01000     | 1.257       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.v_proj     | \u001b[91m170.38163757\u001b[0m | 1           | 0.01000     | 1.225       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | self_attn.dense      | \u001b[91m56.80055237\u001b[0m | 1           | 0.01000     | 1.626       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.fc1              | \u001b[91m706.26361084\u001b[0m | 1           | 0.01000     | 2.208       | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 17        | mlp.fc2              | \u001b[91m79953.09375000\u001b[0m | 1           | 0.01250     | 8.789       | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.q_proj     | \u001b[91m270.84716797\u001b[0m | 1           | 0.01000     | 1.256       | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.k_proj     | \u001b[91m226.15997314\u001b[0m | 1           | 0.01000     | 1.223       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.v_proj     | \u001b[91m194.39843750\u001b[0m | 1           | 0.01000     | 1.354       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | self_attn.dense      | \u001b[91m69.43669128\u001b[0m | 1           | 0.01000     | 1.250       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.fc1              | \u001b[91m785.70294189\u001b[0m | 1           | 0.01000     | 1.810       | 0.013        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 18        | mlp.fc2              | \u001b[91m89266.96875000\u001b[0m | 1           | 0.01250     | 8.496       | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.q_proj     | \u001b[91m275.03341675\u001b[0m | 1           | 0.01000     | 1.292       | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.k_proj     | \u001b[91m235.25959778\u001b[0m | 1           | 0.01000     | 1.544       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.v_proj     | \u001b[91m208.88961792\u001b[0m | 1           | 0.01000     | 1.315       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | self_attn.dense      | \u001b[91m81.07504272\u001b[0m | 1           | 0.01000     | 1.779       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.fc1              | \u001b[91m844.42565918\u001b[0m | 1           | 0.01000     | 2.408       | 0.024        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 19        | mlp.fc2              | \u001b[91m116.27314758\u001b[0m | 1           | 0.01000     | 8.349       | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.q_proj     | \u001b[91m284.80050659\u001b[0m | 1           | 0.01000     | 1.350       | 0.006        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.k_proj     | \u001b[91m247.33421326\u001b[0m | 1           | 0.01000     | 1.403       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.v_proj     | \u001b[91m228.78973389\u001b[0m | 1           | 0.01000     | 1.511       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | self_attn.dense      | \u001b[91m96.50898743\u001b[0m | 1           | 0.01000     | 1.559       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.fc1              | \u001b[91m898.98852539\u001b[0m | 1           | 0.01000     | 1.892       | 0.025        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 20        | mlp.fc2              | \u001b[91m82697.67187500\u001b[0m | 1           | 0.01250     | 8.608       | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.q_proj     | \u001b[91m303.11837769\u001b[0m | 1           | 0.01000     | 1.276       | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.k_proj     | \u001b[91m263.23474121\u001b[0m | 1           | 0.01000     | 1.215       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.v_proj     | \u001b[91m226.34330750\u001b[0m | 1           | 0.01000     | 1.267       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | self_attn.dense      | \u001b[91m114.99917603\u001b[0m | 1           | 0.01000     | 1.341       | 0.004        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.fc1              | \u001b[91m956.84716797\u001b[0m | 1           | 0.01000     | 2.575       | 0.011        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 21        | mlp.fc2              | \u001b[91m75974.10937500\u001b[0m | 1           | 0.01250     | 8.439       | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.q_proj     | \u001b[91m405.26477051\u001b[0m | 1           | 0.01000     | 1.334       | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.k_proj     | \u001b[91m288.67654419\u001b[0m | 1           | 0.01000     | 1.326       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.v_proj     | \u001b[91m211.17440796\u001b[0m | 1           | 0.01000     | 1.274       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | self_attn.dense      | \u001b[91m131.90333557\u001b[0m | 1           | 0.01000     | 1.949       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | mlp.fc1              | \u001b[91m996.25524902\u001b[0m | 1           | 0.01000     | 1.974       | 0.024        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 22        | mlp.fc2              | \u001b[91m87565.25000000\u001b[0m | 1           | 0.01250     | 8.771       | 0.009        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | process     | layer     | module               | loss                | samples     | damp        | time        | fwd_time     |\n",
      "\u001b[32mINFO\u001b[0m  ---------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.q_proj     | \u001b[91m685.08447266\u001b[0m | 1           | 0.01000     | 1.295       | 0.007        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.k_proj     | \u001b[91m336.97122192\u001b[0m | 1           | 0.01000     | 1.204       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.v_proj     | \u001b[91m115.07063293\u001b[0m | 1           | 0.01000     | 1.248       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | self_attn.dense      | \u001b[91m98.04502869\u001b[0m | 1           | 0.01000     | 1.233       | 0.003        | \n",
      "\u001b[32mINFO\u001b[0m  -----------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | mlp.fc1              | \u001b[91m914.19116211\u001b[0m | 1           | 0.01000     | 1.861       | 0.013        | \n",
      "\u001b[32mINFO\u001b[0m  ------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[33mWARN\u001b[0m  Quantization: Current `damp_percent = 0.01000` is too low, auto-incrementing by `0.00250`\n",
      "\u001b[32mINFO\u001b[0m  | gptq        | 23        | mlp.fc2              | \u001b[91m74833.96875000\u001b[0m | 1           | 0.01250     | 8.877       | 0.010        | \n",
      "\u001b[32mINFO\u001b[0m  --------------------------------------------------------------------------------------------------------------------------------------\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.q_proj', 'loss': '101.11450195', 'samples': '1', 'damp': '0.01000', 'time': '1.753', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.k_proj', 'loss': '100.74877930', 'samples': '1', 'damp': '0.01000', 'time': '1.738', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.v_proj', 'loss': '32.58218765', 'samples': '1', 'damp': '0.01000', 'time': '1.603', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'self_attn.dense', 'loss': '16.47297478', 'samples': '1', 'damp': '0.01000', 'time': '1.622', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.fc1', 'loss': '126.84600830', 'samples': '1', 'damp': '0.01000', 'time': '2.075', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 0, 'module': 'mlp.fc2', 'loss': '106996.78906250', 'samples': '1', 'damp': '0.01250', 'time': '15.632', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.q_proj', 'loss': '106.33714294', 'samples': '1', 'damp': '0.01000', 'time': '1.811', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.k_proj', 'loss': '107.21305847', 'samples': '1', 'damp': '0.01000', 'time': '1.660', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.v_proj', 'loss': '22.91265106', 'samples': '1', 'damp': '0.01000', 'time': '1.560', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'self_attn.dense', 'loss': '4.92205477', 'samples': '1', 'damp': '0.01000', 'time': '1.722', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.fc1', 'loss': '351.60754395', 'samples': '1', 'damp': '0.01000', 'time': '2.182', 'fwd_time': '0.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 1, 'module': 'mlp.fc2', 'loss': '115594.28125000', 'samples': '1', 'damp': '0.01250', 'time': '459.037', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.q_proj', 'loss': '130.03463745', 'samples': '1', 'damp': '0.01000', 'time': '1.622', 'fwd_time': '0.008'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.k_proj', 'loss': '129.76574707', 'samples': '1', 'damp': '0.01000', 'time': '1.367', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.v_proj', 'loss': '46.67893982', 'samples': '1', 'damp': '0.01000', 'time': '1.390', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'self_attn.dense', 'loss': '10.42765903', 'samples': '1', 'damp': '0.01000', 'time': '1.495', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.fc1', 'loss': '460.92077637', 'samples': '1', 'damp': '0.01000', 'time': '2.128', 'fwd_time': '0.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 2, 'module': 'mlp.fc2', 'loss': '94196.98437500', 'samples': '1', 'damp': '0.01250', 'time': '8.948', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.q_proj', 'loss': '141.92184448', 'samples': '1', 'damp': '0.01000', 'time': '1.342', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.k_proj', 'loss': '137.03282166', 'samples': '1', 'damp': '0.01000', 'time': '1.212', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.v_proj', 'loss': '63.80435562', 'samples': '1', 'damp': '0.01000', 'time': '1.234', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'self_attn.dense', 'loss': '16.43011284', 'samples': '1', 'damp': '0.01000', 'time': '1.264', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.fc1', 'loss': '511.16094971', 'samples': '1', 'damp': '0.01000', 'time': '1.807', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 3, 'module': 'mlp.fc2', 'loss': '63795.21484375', 'samples': '1', 'damp': '0.01250', 'time': '9.502', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.q_proj', 'loss': '138.67892456', 'samples': '1', 'damp': '0.01000', 'time': '1.515', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.k_proj', 'loss': '132.08502197', 'samples': '1', 'damp': '0.01000', 'time': '1.340', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.v_proj', 'loss': '71.12254333', 'samples': '1', 'damp': '0.01000', 'time': '1.243', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'self_attn.dense', 'loss': '20.34513474', 'samples': '1', 'damp': '0.01000', 'time': '1.385', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.fc1', 'loss': '477.45983887', 'samples': '1', 'damp': '0.01000', 'time': '2.050', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 4, 'module': 'mlp.fc2', 'loss': '54975.36718750', 'samples': '1', 'damp': '0.01250', 'time': '10.434', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.q_proj', 'loss': '167.74491882', 'samples': '1', 'damp': '0.01000', 'time': '1.313', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.k_proj', 'loss': '160.31777954', 'samples': '1', 'damp': '0.01000', 'time': '1.294', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.v_proj', 'loss': '82.51467896', 'samples': '1', 'damp': '0.01000', 'time': '1.391', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'self_attn.dense', 'loss': '17.27911377', 'samples': '1', 'damp': '0.01000', 'time': '1.347', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.fc1', 'loss': '527.64160156', 'samples': '1', 'damp': '0.01000', 'time': '1.987', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 5, 'module': 'mlp.fc2', 'loss': '75403.63281250', 'samples': '1', 'damp': '0.01250', 'time': '12.733', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.q_proj', 'loss': '160.98776245', 'samples': '1', 'damp': '0.01000', 'time': '1.376', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.k_proj', 'loss': '154.01689148', 'samples': '1', 'damp': '0.01000', 'time': '1.272', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.v_proj', 'loss': '88.86698914', 'samples': '1', 'damp': '0.01000', 'time': '1.278', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'self_attn.dense', 'loss': '20.87316513', 'samples': '1', 'damp': '0.01000', 'time': '1.247', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.fc1', 'loss': '519.89935303', 'samples': '1', 'damp': '0.01000', 'time': '1.907', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 6, 'module': 'mlp.fc2', 'loss': '73255.48437500', 'samples': '1', 'damp': '0.01250', 'time': '9.412', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.q_proj', 'loss': '164.87022400', 'samples': '1', 'damp': '0.01000', 'time': '1.354', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.k_proj', 'loss': '163.55050659', 'samples': '1', 'damp': '0.01000', 'time': '1.239', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.v_proj', 'loss': '98.75587463', 'samples': '1', 'damp': '0.01000', 'time': '1.265', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'self_attn.dense', 'loss': '22.03719330', 'samples': '1', 'damp': '0.01000', 'time': '1.297', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.fc1', 'loss': '527.94042969', 'samples': '1', 'damp': '0.01000', 'time': '1.868', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 7, 'module': 'mlp.fc2', 'loss': '66821.23437500', 'samples': '1', 'damp': '0.01250', 'time': '11.115', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.q_proj', 'loss': '165.80398560', 'samples': '1', 'damp': '0.01000', 'time': '1.325', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.k_proj', 'loss': '165.28674316', 'samples': '1', 'damp': '0.01000', 'time': '1.328', 'fwd_time': '0.005'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.v_proj', 'loss': '99.01081848', 'samples': '1', 'damp': '0.01000', 'time': '1.416', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'self_attn.dense', 'loss': '25.10817719', 'samples': '1', 'damp': '0.01000', 'time': '1.273', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.fc1', 'loss': '502.25500488', 'samples': '1', 'damp': '0.01000', 'time': '1.857', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 8, 'module': 'mlp.fc2', 'loss': '53658.82031250', 'samples': '1', 'damp': '0.01250', 'time': '11.122', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.q_proj', 'loss': '169.34075928', 'samples': '1', 'damp': '0.01000', 'time': '1.329', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.k_proj', 'loss': '169.55943298', 'samples': '1', 'damp': '0.01000', 'time': '1.274', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.v_proj', 'loss': '109.37715912', 'samples': '1', 'damp': '0.01000', 'time': '1.310', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'self_attn.dense', 'loss': '32.04048538', 'samples': '1', 'damp': '0.01000', 'time': '1.259', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.fc1', 'loss': '511.23162842', 'samples': '1', 'damp': '0.01000', 'time': '1.816', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 9, 'module': 'mlp.fc2', 'loss': '56574.25000000', 'samples': '1', 'damp': '0.01250', 'time': '9.608', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.q_proj', 'loss': '171.60531616', 'samples': '1', 'damp': '0.01000', 'time': '1.304', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.k_proj', 'loss': '174.18914795', 'samples': '1', 'damp': '0.01000', 'time': '1.237', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.v_proj', 'loss': '112.08305359', 'samples': '1', 'damp': '0.01000', 'time': '1.298', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'self_attn.dense', 'loss': '37.83961105', 'samples': '1', 'damp': '0.01000', 'time': '1.307', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.fc1', 'loss': '532.65209961', 'samples': '1', 'damp': '0.01000', 'time': '1.905', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 10, 'module': 'mlp.fc2', 'loss': '52050.59375000', 'samples': '1', 'damp': '0.01250', 'time': '9.918', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.q_proj', 'loss': '164.91189575', 'samples': '1', 'damp': '0.01000', 'time': '1.338', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.k_proj', 'loss': '167.51042175', 'samples': '1', 'damp': '0.01000', 'time': '1.254', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.v_proj', 'loss': '111.37121582', 'samples': '1', 'damp': '0.01000', 'time': '1.330', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'self_attn.dense', 'loss': '37.10642242', 'samples': '1', 'damp': '0.01000', 'time': '1.359', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.fc1', 'loss': '499.54428101', 'samples': '1', 'damp': '0.01000', 'time': '1.907', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 11, 'module': 'mlp.fc2', 'loss': '50568.96484375', 'samples': '1', 'damp': '0.01250', 'time': '9.282', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.q_proj', 'loss': '176.05184937', 'samples': '1', 'damp': '0.01000', 'time': '1.310', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.k_proj', 'loss': '178.61288452', 'samples': '1', 'damp': '0.01000', 'time': '1.248', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.v_proj', 'loss': '117.83205414', 'samples': '1', 'damp': '0.01000', 'time': '1.359', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'self_attn.dense', 'loss': '32.42706299', 'samples': '1', 'damp': '0.01000', 'time': '1.326', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.fc1', 'loss': '528.94921875', 'samples': '1', 'damp': '0.01000', 'time': '1.827', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 12, 'module': 'mlp.fc2', 'loss': '61275.61718750', 'samples': '1', 'damp': '0.01250', 'time': '8.681', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.q_proj', 'loss': '179.52462769', 'samples': '1', 'damp': '0.01000', 'time': '1.688', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.k_proj', 'loss': '185.83723450', 'samples': '1', 'damp': '0.01000', 'time': '1.305', 'fwd_time': '0.002'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.v_proj', 'loss': '120.39207458', 'samples': '1', 'damp': '0.01000', 'time': '1.302', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'self_attn.dense', 'loss': '31.16594315', 'samples': '1', 'damp': '0.01000', 'time': '1.289', 'fwd_time': '0.005'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.fc1', 'loss': '505.40542603', 'samples': '1', 'damp': '0.01000', 'time': '1.916', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 13, 'module': 'mlp.fc2', 'loss': '58707.30468750', 'samples': '1', 'damp': '0.01250', 'time': '8.846', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.q_proj', 'loss': '188.16873169', 'samples': '1', 'damp': '0.01000', 'time': '1.313', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.k_proj', 'loss': '188.36560059', 'samples': '1', 'damp': '0.01000', 'time': '1.321', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.v_proj', 'loss': '137.21459961', 'samples': '1', 'damp': '0.01000', 'time': '1.297', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'self_attn.dense', 'loss': '40.09495544', 'samples': '1', 'damp': '0.01000', 'time': '1.348', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.fc1', 'loss': '572.48791504', 'samples': '1', 'damp': '0.01000', 'time': '1.897', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 14, 'module': 'mlp.fc2', 'loss': '83905.14843750', 'samples': '1', 'damp': '0.01250', 'time': '8.963', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.q_proj', 'loss': '210.46829224', 'samples': '1', 'damp': '0.01000', 'time': '1.395', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.k_proj', 'loss': '195.38082886', 'samples': '1', 'damp': '0.01000', 'time': '1.463', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.v_proj', 'loss': '133.09068298', 'samples': '1', 'damp': '0.01000', 'time': '1.325', 'fwd_time': '0.005'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'self_attn.dense', 'loss': '35.28706360', 'samples': '1', 'damp': '0.01000', 'time': '1.242', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.fc1', 'loss': '590.91345215', 'samples': '1', 'damp': '0.01000', 'time': '1.770', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 15, 'module': 'mlp.fc2', 'loss': '73120.64062500', 'samples': '1', 'damp': '0.01250', 'time': '8.867', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.q_proj', 'loss': '229.86494446', 'samples': '1', 'damp': '0.01000', 'time': '1.281', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.k_proj', 'loss': '213.65203857', 'samples': '1', 'damp': '0.01000', 'time': '1.217', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.v_proj', 'loss': '164.72760010', 'samples': '1', 'damp': '0.01000', 'time': '1.237', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'self_attn.dense', 'loss': '41.04069901', 'samples': '1', 'damp': '0.01000', 'time': '1.232', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.fc1', 'loss': '660.77929688', 'samples': '1', 'damp': '0.01000', 'time': '1.761', 'fwd_time': '0.012'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 16, 'module': 'mlp.fc2', 'loss': '86064.12500000', 'samples': '1', 'damp': '0.01250', 'time': '10.419', 'fwd_time': '0.017'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.q_proj', 'loss': '215.35038757', 'samples': '1', 'damp': '0.01000', 'time': '1.300', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.k_proj', 'loss': '209.51354980', 'samples': '1', 'damp': '0.01000', 'time': '1.257', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.v_proj', 'loss': '170.38163757', 'samples': '1', 'damp': '0.01000', 'time': '1.225', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'self_attn.dense', 'loss': '56.80055237', 'samples': '1', 'damp': '0.01000', 'time': '1.626', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.fc1', 'loss': '706.26361084', 'samples': '1', 'damp': '0.01000', 'time': '2.208', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 17, 'module': 'mlp.fc2', 'loss': '79953.09375000', 'samples': '1', 'damp': '0.01250', 'time': '8.789', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.q_proj', 'loss': '270.84716797', 'samples': '1', 'damp': '0.01000', 'time': '1.256', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.k_proj', 'loss': '226.15997314', 'samples': '1', 'damp': '0.01000', 'time': '1.223', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.v_proj', 'loss': '194.39843750', 'samples': '1', 'damp': '0.01000', 'time': '1.354', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'self_attn.dense', 'loss': '69.43669128', 'samples': '1', 'damp': '0.01000', 'time': '1.250', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.fc1', 'loss': '785.70294189', 'samples': '1', 'damp': '0.01000', 'time': '1.810', 'fwd_time': '0.013'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 18, 'module': 'mlp.fc2', 'loss': '89266.96875000', 'samples': '1', 'damp': '0.01250', 'time': '8.496', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.q_proj', 'loss': '275.03341675', 'samples': '1', 'damp': '0.01000', 'time': '1.292', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.k_proj', 'loss': '235.25959778', 'samples': '1', 'damp': '0.01000', 'time': '1.544', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.v_proj', 'loss': '208.88961792', 'samples': '1', 'damp': '0.01000', 'time': '1.315', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'self_attn.dense', 'loss': '81.07504272', 'samples': '1', 'damp': '0.01000', 'time': '1.779', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.fc1', 'loss': '844.42565918', 'samples': '1', 'damp': '0.01000', 'time': '2.408', 'fwd_time': '0.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 19, 'module': 'mlp.fc2', 'loss': '116.27314758', 'samples': '1', 'damp': '0.01000', 'time': '8.349', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.q_proj', 'loss': '284.80050659', 'samples': '1', 'damp': '0.01000', 'time': '1.350', 'fwd_time': '0.006'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.k_proj', 'loss': '247.33421326', 'samples': '1', 'damp': '0.01000', 'time': '1.403', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.v_proj', 'loss': '228.78973389', 'samples': '1', 'damp': '0.01000', 'time': '1.511', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'self_attn.dense', 'loss': '96.50898743', 'samples': '1', 'damp': '0.01000', 'time': '1.559', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.fc1', 'loss': '898.98852539', 'samples': '1', 'damp': '0.01000', 'time': '1.892', 'fwd_time': '0.025'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 20, 'module': 'mlp.fc2', 'loss': '82697.67187500', 'samples': '1', 'damp': '0.01250', 'time': '8.608', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.q_proj', 'loss': '303.11837769', 'samples': '1', 'damp': '0.01000', 'time': '1.276', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.k_proj', 'loss': '263.23474121', 'samples': '1', 'damp': '0.01000', 'time': '1.215', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.v_proj', 'loss': '226.34330750', 'samples': '1', 'damp': '0.01000', 'time': '1.267', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'self_attn.dense', 'loss': '114.99917603', 'samples': '1', 'damp': '0.01000', 'time': '1.341', 'fwd_time': '0.004'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.fc1', 'loss': '956.84716797', 'samples': '1', 'damp': '0.01000', 'time': '2.575', 'fwd_time': '0.011'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 21, 'module': 'mlp.fc2', 'loss': '75974.10937500', 'samples': '1', 'damp': '0.01250', 'time': '8.439', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.q_proj', 'loss': '405.26477051', 'samples': '1', 'damp': '0.01000', 'time': '1.334', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.k_proj', 'loss': '288.67654419', 'samples': '1', 'damp': '0.01000', 'time': '1.326', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.v_proj', 'loss': '211.17440796', 'samples': '1', 'damp': '0.01000', 'time': '1.274', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'self_attn.dense', 'loss': '131.90333557', 'samples': '1', 'damp': '0.01000', 'time': '1.949', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'mlp.fc1', 'loss': '996.25524902', 'samples': '1', 'damp': '0.01000', 'time': '1.974', 'fwd_time': '0.024'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 22, 'module': 'mlp.fc2', 'loss': '87565.25000000', 'samples': '1', 'damp': '0.01250', 'time': '8.771', 'fwd_time': '0.009'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.q_proj', 'loss': '685.08447266', 'samples': '1', 'damp': '0.01000', 'time': '1.295', 'fwd_time': '0.007'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.k_proj', 'loss': '336.97122192', 'samples': '1', 'damp': '0.01000', 'time': '1.204', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.v_proj', 'loss': '115.07063293', 'samples': '1', 'damp': '0.01000', 'time': '1.248', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'self_attn.dense', 'loss': '98.04502869', 'samples': '1', 'damp': '0.01000', 'time': '1.233', 'fwd_time': '0.003'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'mlp.fc1', 'loss': '914.19116211', 'samples': '1', 'damp': '0.01000', 'time': '1.861', 'fwd_time': '0.013'}\n",
      "\u001b[32mINFO\u001b[0m  {'process': 'gptq', 'layer': 23, 'module': 'mlp.fc2', 'loss': '74833.96875000', 'samples': '1', 'damp': '0.01250', 'time': '8.877', 'fwd_time': '0.010'}\n",
      "\u001b[32mINFO\u001b[0m  Packing model...                                                         \n",
      "\u001b[32mINFO\u001b[0m  Packing Kernel: Auto-selection: adding candidate `TorchQuantLinear`      \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                               \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                  \n",
      "\u001b[32mINFO\u001b[0m  Model packed.                                                            0%\n",
      "\u001b[32mINFO\u001b[0m  Saved Quantize Config: \n",
      "{\n",
      "  \"bits\": 2,\n",
      "  \"group_size\": 64,\n",
      "  \"desc_act\": true,\n",
      "  \"sym\": true,\n",
      "  \"lm_head\": false,\n",
      "  \"quant_method\": \"gptq\",\n",
      "  \"checkpoint_format\": \"gptq\",\n",
      "  \"pack_dtype\": \"int32\",\n",
      "  \"meta\": {\n",
      "    \"quantizer\": [\n",
      "      \"gptqmodel:2.2.0\"\n",
      "    ],\n",
      "    \"uri\": \"https://github.com/modelcloud/gptqmodel\",\n",
      "    \"damp_percent\": 0.01,\n",
      "    \"damp_auto_increment\": 0.0025,\n",
      "    \"static_groups\": false,\n",
      "    \"true_sequential\": true,\n",
      "    \"mse\": 0.0\n",
      "  }\n",
      "}\n",
      "Files in directory:\n",
      "config.json\n",
      "generation_config.json\n",
      "quantize_config.json\n",
      "quant_log.csv\n",
      "Content of saved `generation_config.json`:\n",
      "{\n",
      "    \"_from_model_config\": true,\n",
      "    \"transformers_version\": \"4.53.3\"\n",
      "}\n",
      "Content of saved `config.json`:\n",
      "{\n",
      "    \"architectures\": [\n",
      "        \"PhiForCausalLM\"\n",
      "    ],\n",
      "    \"attention_dropout\": 0.0,\n",
      "    \"bos_token_id\": null,\n",
      "    \"embd_pdrop\": 0.0,\n",
      "    \"eos_token_id\": null,\n",
      "    \"hidden_act\": \"gelu_new\",\n",
      "    \"hidden_size\": 2048,\n",
      "    \"initializer_range\": 0.02,\n",
      "    \"intermediate_size\": 8192,\n",
      "    \"layer_norm_eps\": 1e-05,\n",
      "    \"max_position_embeddings\": 2048,\n",
      "    \"model_type\": \"phi\",\n",
      "    \"num_attention_heads\": 32,\n",
      "    \"num_hidden_layers\": 24,\n",
      "    \"num_key_value_heads\": 32,\n",
      "    \"partial_rotary_factor\": 0.5,\n",
      "    \"qk_layernorm\": false,\n",
      "    \"quantization_config\": {\n",
      "        \"bits\": 2,\n",
      "        \"checkpoint_format\": \"gptq\",\n",
      "        \"desc_act\": true,\n",
      "        \"group_size\": 64,\n",
      "        \"lm_head\": false,\n",
      "        \"meta\": {\n",
      "            \"damp_auto_increment\": 0.0025,\n",
      "            \"damp_percent\": 0.01,\n",
      "            \"mse\": 0.0,\n",
      "            \"quantizer\": [\n",
      "                \"gptqmodel:2.2.0\"\n",
      "            ],\n",
      "            \"static_groups\": false,\n",
      "            \"true_sequential\": true,\n",
      "            \"uri\": \"https://github.com/modelcloud/gptqmodel\"\n",
      "        },\n",
      "        \"pack_dtype\": \"int32\",\n",
      "        \"quant_method\": \"gptq\",\n",
      "        \"sym\": true\n",
      "    },\n",
      "    \"resid_pdrop\": 0.0,\n",
      "    \"rope_scaling\": null,\n",
      "    \"rope_theta\": 10000.0,\n",
      "    \"tie_word_embeddings\": false,\n",
      "    \"torch_dtype\": \"float16\",\n",
      "    \"transformers_version\": \"4.53.3\",\n",
      "    \"use_cache\": true,\n",
      "    \"vocab_size\": 51200\n",
      "}\n",
      "\u001b[32mINFO\u001b[0m  Pre-Quantized model size: 2705.17MB, 2.64GB                              \n",
      "\u001b[32mINFO\u001b[0m  Quantized model size: 731.40MB, 0.71GB                                   \n",
      "\u001b[32mINFO\u001b[0m  Size difference: 1973.77MB, 1.93GB - 72.96%                              \n"
     ]
    }
   ],
   "source": [
    "QuantizeModelandSaveP(quantize_config_2, output_dir_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c444ce29",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native float16): `torch.float16`                     \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 2.44375 bpw, based on [bits: 2, group_size: 64]\n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`             \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                               \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                  \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.\n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.011798620223999023s                       \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=50256 (token='<|endoftext|>').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {}\n",
      "                  \n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                   \n"
     ]
    },
    {
     "ename": "TritonMissing",
     "evalue": "Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTritonMissing\u001b[39m                             Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m device = get_best_device()\n\u001b[32m      5\u001b[39m model = GPTQModel.load(quantized_model_id, device=device)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokenizer.decode(model.generate(**tokenizer(\u001b[33m\"\u001b[39m\u001b[33mgptqmodel is\u001b[39m\u001b[33m\"\u001b[39m, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m).to(model.device))[\u001b[32m0\u001b[39m]))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\gptqmodel\\models\\base.py:1175\u001b[39m, in \u001b[36mBaseGPTQModel.generate\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m   1172\u001b[39m     inputs = \u001b[38;5;28mself\u001b[39m.tokenizer(inputs, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, padding_side=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m).to(\u001b[38;5;28mself\u001b[39m.model.device)\n\u001b[32m   1173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.generate(**inputs, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.generate(inputs=inputs, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:2625\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2617\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2618\u001b[39m         input_ids=input_ids,\n\u001b[32m   2619\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2620\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2621\u001b[39m         **model_kwargs,\n\u001b[32m   2622\u001b[39m     )\n\u001b[32m   2624\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2625\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._sample(\n\u001b[32m   2626\u001b[39m         input_ids,\n\u001b[32m   2627\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   2628\u001b[39m         stopping_criteria=prepared_stopping_criteria,\n\u001b[32m   2629\u001b[39m         generation_config=generation_config,\n\u001b[32m   2630\u001b[39m         synced_gpus=synced_gpus,\n\u001b[32m   2631\u001b[39m         streamer=streamer,\n\u001b[32m   2632\u001b[39m         **model_kwargs,\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2635\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2636\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2637\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2638\u001b[39m         input_ids=input_ids,\n\u001b[32m   2639\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2640\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2641\u001b[39m         **model_kwargs,\n\u001b[32m   2642\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:3606\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3603\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3605\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m3606\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m(**model_inputs, return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3607\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3608\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:943\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     output = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    944\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    945\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:527\u001b[39m, in \u001b[36mPhiForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    522\u001b[39m output_hidden_states = (\n\u001b[32m    523\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    524\u001b[39m )\n\u001b[32m    526\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28mself\u001b[39m.model(\n\u001b[32m    528\u001b[39m     input_ids=input_ids,\n\u001b[32m    529\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    530\u001b[39m     position_ids=position_ids,\n\u001b[32m    531\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    532\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    533\u001b[39m     use_cache=use_cache,\n\u001b[32m    534\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    535\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m    536\u001b[39m     cache_position=cache_position,\n\u001b[32m    537\u001b[39m     **kwargs,\n\u001b[32m    538\u001b[39m )\n\u001b[32m    540\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    541\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:943\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     output = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    944\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    945\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:415\u001b[39m, in \u001b[36mPhiModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    413\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m layer_outputs = decoder_layer(\n\u001b[32m    416\u001b[39m     hidden_states,\n\u001b[32m    417\u001b[39m     attention_mask=causal_mask,\n\u001b[32m    418\u001b[39m     position_ids=position_ids,\n\u001b[32m    419\u001b[39m     past_key_value=past_key_values,\n\u001b[32m    420\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    421\u001b[39m     use_cache=use_cache,\n\u001b[32m    422\u001b[39m     cache_position=cache_position,\n\u001b[32m    423\u001b[39m     position_embeddings=position_embeddings,\n\u001b[32m    424\u001b[39m     **flash_attn_kwargs,\n\u001b[32m    425\u001b[39m )\n\u001b[32m    427\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:83\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m         logger.warning(message)\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:234\u001b[39m, in \u001b[36mPhiDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m attn_outputs, self_attn_weights = \u001b[38;5;28mself\u001b[39m.self_attn(\n\u001b[32m    235\u001b[39m     hidden_states=hidden_states,\n\u001b[32m    236\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    237\u001b[39m     position_ids=position_ids,\n\u001b[32m    238\u001b[39m     past_key_value=past_key_value,\n\u001b[32m    239\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    240\u001b[39m     use_cache=use_cache,\n\u001b[32m    241\u001b[39m     cache_position=cache_position,\n\u001b[32m    242\u001b[39m     position_embeddings=position_embeddings,\n\u001b[32m    243\u001b[39m     **kwargs,\n\u001b[32m    244\u001b[39m )\n\u001b[32m    245\u001b[39m attn_outputs = \u001b[38;5;28mself\u001b[39m.resid_dropout(attn_outputs)\n\u001b[32m    247\u001b[39m feed_forward_hidden_states = \u001b[38;5;28mself\u001b[39m.resid_dropout(\u001b[38;5;28mself\u001b[39m.mlp(hidden_states))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:144\u001b[39m, in \u001b[36mPhiAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    141\u001b[39m input_shape = hidden_states.shape[:-\u001b[32m1\u001b[39m]\n\u001b[32m    142\u001b[39m hidden_shape = (*input_shape, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m query_states = \u001b[38;5;28mself\u001b[39m.q_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    145\u001b[39m key_states = \u001b[38;5;28mself\u001b[39m.k_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    146\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\gptqmodel\\nn_modules\\qlinear\\torch.py:154\u001b[39m, in \u001b[36mTorchQuantLinear.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    152\u001b[39m out_shape = x.shape[:-\u001b[32m1\u001b[39m] + (\u001b[38;5;28mself\u001b[39m.out_features,)\n\u001b[32m    153\u001b[39m x = x.reshape(-\u001b[32m1\u001b[39m, x.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m out = \u001b[38;5;28mself\u001b[39m._forward(x, out_shape)\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\gptqmodel\\nn_modules\\qlinear\\torch.py:160\u001b[39m, in \u001b[36mTorchQuantLinear._forward\u001b[39m\u001b[34m(self, x, out_shape)\u001b[39m\n\u001b[32m    158\u001b[39m num_itr = \u001b[38;5;28mself\u001b[39m.g_idx.shape[\u001b[32m0\u001b[39m] // x.shape[-\u001b[32m1\u001b[39m]\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# make sure dequant dtype matches input x\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m weights = \u001b[38;5;28mself\u001b[39m.dequantize_weight(num_itr=num_itr).to(x.dtype)\n\u001b[32m    162\u001b[39m out = torch.matmul(x, weights).reshape(out_shape)\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:663\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    659\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    660\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    661\u001b[39m     \u001b[38;5;66;03m# Failures in the backend likely don't have useful\u001b[39;00m\n\u001b[32m    662\u001b[39m     \u001b[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m663\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.remove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001b[39;00m\n\u001b[32m    664\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    665\u001b[39m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[32m    666\u001b[39m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_inductor\\scheduler.py:3957\u001b[39m, in \u001b[36mScheduler.create_backend\u001b[39m\u001b[34m(self, device)\u001b[39m\n\u001b[32m   3955\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m GPUTooOldForTriton(device_props, inspect.currentframe())\n\u001b[32m   3956\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m is_gpu(device.type) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m device.type == \u001b[33m\"\u001b[39m\u001b[33mmps\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3957\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m TritonMissing(inspect.currentframe())\n\u001b[32m   3959\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m device_scheduling(\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mTritonMissing\u001b[39m: Cannot find a working triton installation. Either the package is not installed or it is too old. More information on installing Triton can be found at: https://github.com/triton-lang/triton\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "from gptqmodel import get_best_device\n",
    "\n",
    "# load quantized model to the first GPU\n",
    "device = get_best_device()\n",
    "model = GPTQModel.load(quantized_model_id, device=device)\n",
    "print(tokenizer.decode(model.generate(**tokenizer(\"gptqmodel is\", return_tensors=\"pt\").to(model.device))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "af9a5063",
   "metadata": {
    "vscode": {
     "languageId": "ruby"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "from_quantized: adapter: None\n",
      "\u001b[32mINFO\u001b[0m  Loader: Auto dtype (native float16): `torch.float16`                     \n",
      "\u001b[32mINFO\u001b[0m  Estimated Quantization BPW (bits per weight): 2.44375 bpw, based on [bits: 2, group_size: 64]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`             \n",
      "\u001b[32mINFO\u001b[0m  Kernel: candidates -> `[TorchQuantLinear]`                               \n",
      "\u001b[32mINFO\u001b[0m  Kernel: selected -> `TorchQuantLinear`.                                  \n",
      "\u001b[32mINFO\u001b[0m  Format: Converting `checkpoint_format` from `gptq` to internal `gptq_v2`.\n",
      "\u001b[32mINFO\u001b[0m  Format: Conversion complete: 0.011531829833984375s                       \n",
      "\u001b[32mINFO\u001b[0m   Kernel: Auto-selection: adding candidate `TorchQuantLinear`             \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tokenicer.tokenicer:Tokenicer: Auto fixed pad_token_id=50256 (token='<|endoftext|>').\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mINFO\u001b[0m  Model: Loaded `generation_config`: GenerationConfig {}\n",
      "                  \n",
      "\u001b[32mINFO\u001b[0m  Kernel: loaded -> `[TorchQuantLinear]`                                   \n"
     ]
    },
    {
     "ename": "TorchRuntimeError",
     "evalue": "Dynamo failed to run FX node with fake tensors: call_function <built-in method bitwise_right_shift of type object at 0x00007FF823D0C450>(*(FakeTensor(..., size=(32, 128, 16), dtype=torch.int32), FakeTensor(..., device='cuda:0', size=(1, 1, 16), dtype=torch.int32)), **{}): got RuntimeError('Unhandled FakeTensor Device Propagation for aten.bitwise_right_shift.Tensor, found two different devices cpu, cuda:0')\n\nfrom user code:\n   File \"c:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\gptqmodel\\nn_modules\\qlinear\\__init__.py\", line 441, in dequantize_weight\n    zeros = t.bitwise_right_shift(\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTorchRuntimeError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m# Ensure inputs are on CPU (they should be by default now)\u001b[39;00m\n\u001b[32m     20\u001b[39m \n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Generate on CPU\u001b[39;00m\n\u001b[32m     22\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m     output = model.generate(**inputs, max_length=\u001b[32m50\u001b[39m, do_sample=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(tokenizer.decode(output[\u001b[32m0\u001b[39m], skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\gptqmodel\\models\\base.py:1175\u001b[39m, in \u001b[36mBaseGPTQModel.generate\u001b[39m\u001b[34m(self, inputs, **kwargs)\u001b[39m\n\u001b[32m   1172\u001b[39m     inputs = \u001b[38;5;28mself\u001b[39m.tokenizer(inputs, return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m, padding=\u001b[38;5;28;01mTrue\u001b[39;00m, padding_side=\u001b[33m\"\u001b[39m\u001b[33mleft\u001b[39m\u001b[33m\"\u001b[39m).to(\u001b[38;5;28mself\u001b[39m.model.device)\n\u001b[32m   1173\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.generate(**inputs, **kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model.generate(inputs=inputs, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:2625\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2617\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2618\u001b[39m         input_ids=input_ids,\n\u001b[32m   2619\u001b[39m         expand_size=generation_config.num_return_sequences,\n\u001b[32m   2620\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2621\u001b[39m         **model_kwargs,\n\u001b[32m   2622\u001b[39m     )\n\u001b[32m   2624\u001b[39m     \u001b[38;5;66;03m# 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2625\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._sample(\n\u001b[32m   2626\u001b[39m         input_ids,\n\u001b[32m   2627\u001b[39m         logits_processor=prepared_logits_processor,\n\u001b[32m   2628\u001b[39m         stopping_criteria=prepared_stopping_criteria,\n\u001b[32m   2629\u001b[39m         generation_config=generation_config,\n\u001b[32m   2630\u001b[39m         synced_gpus=synced_gpus,\n\u001b[32m   2631\u001b[39m         streamer=streamer,\n\u001b[32m   2632\u001b[39m         **model_kwargs,\n\u001b[32m   2633\u001b[39m     )\n\u001b[32m   2635\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m generation_mode \u001b[38;5;129;01min\u001b[39;00m (GenerationMode.BEAM_SAMPLE, GenerationMode.BEAM_SEARCH):\n\u001b[32m   2636\u001b[39m     \u001b[38;5;66;03m# 11. interleave input_ids with `num_beams` additional sequences per batch\u001b[39;00m\n\u001b[32m   2637\u001b[39m     input_ids, model_kwargs = \u001b[38;5;28mself\u001b[39m._expand_inputs_for_generation(\n\u001b[32m   2638\u001b[39m         input_ids=input_ids,\n\u001b[32m   2639\u001b[39m         expand_size=generation_config.num_beams,\n\u001b[32m   2640\u001b[39m         is_encoder_decoder=\u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder,\n\u001b[32m   2641\u001b[39m         **model_kwargs,\n\u001b[32m   2642\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\generation\\utils.py:3606\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   3603\u001b[39m model_inputs.update({\u001b[33m\"\u001b[39m\u001b[33moutput_hidden_states\u001b[39m\u001b[33m\"\u001b[39m: output_hidden_states} \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;28;01melse\u001b[39;00m {})\n\u001b[32m   3605\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m3606\u001b[39m     outputs = \u001b[38;5;28mself\u001b[39m(**model_inputs, return_dict=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   3607\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   3608\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:943\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     output = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    944\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    945\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:527\u001b[39m, in \u001b[36mPhiForCausalLM.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m    522\u001b[39m output_hidden_states = (\n\u001b[32m    523\u001b[39m     output_hidden_states \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.output_hidden_states\n\u001b[32m    524\u001b[39m )\n\u001b[32m    526\u001b[39m \u001b[38;5;66;03m# decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m527\u001b[39m outputs: BaseModelOutputWithPast = \u001b[38;5;28mself\u001b[39m.model(\n\u001b[32m    528\u001b[39m     input_ids=input_ids,\n\u001b[32m    529\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    530\u001b[39m     position_ids=position_ids,\n\u001b[32m    531\u001b[39m     past_key_values=past_key_values,\n\u001b[32m    532\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    533\u001b[39m     use_cache=use_cache,\n\u001b[32m    534\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    535\u001b[39m     output_hidden_states=output_hidden_states,\n\u001b[32m    536\u001b[39m     cache_position=cache_position,\n\u001b[32m    537\u001b[39m     **kwargs,\n\u001b[32m    538\u001b[39m )\n\u001b[32m    540\u001b[39m hidden_states = outputs.last_hidden_state\n\u001b[32m    541\u001b[39m \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\utils\\generic.py:943\u001b[39m, in \u001b[36mcan_return_tuple.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    940\u001b[39m     set_attribute_for_modules(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m_is_top_level_module\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    942\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m943\u001b[39m     output = func(\u001b[38;5;28mself\u001b[39m, *args, **kwargs)\n\u001b[32m    944\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m is_requested_to_return_tuple \u001b[38;5;129;01mor\u001b[39;00m (is_configured_to_return_tuple \u001b[38;5;129;01mand\u001b[39;00m is_top_level_module):\n\u001b[32m    945\u001b[39m         output = output.to_tuple()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:415\u001b[39m, in \u001b[36mPhiModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, cache_position, **flash_attn_kwargs)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    413\u001b[39m     all_hidden_states += (hidden_states,)\n\u001b[32m--> \u001b[39m\u001b[32m415\u001b[39m layer_outputs = decoder_layer(\n\u001b[32m    416\u001b[39m     hidden_states,\n\u001b[32m    417\u001b[39m     attention_mask=causal_mask,\n\u001b[32m    418\u001b[39m     position_ids=position_ids,\n\u001b[32m    419\u001b[39m     past_key_value=past_key_values,\n\u001b[32m    420\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    421\u001b[39m     use_cache=use_cache,\n\u001b[32m    422\u001b[39m     cache_position=cache_position,\n\u001b[32m    423\u001b[39m     position_embeddings=position_embeddings,\n\u001b[32m    424\u001b[39m     **flash_attn_kwargs,\n\u001b[32m    425\u001b[39m )\n\u001b[32m    427\u001b[39m hidden_states = layer_outputs[\u001b[32m0\u001b[39m]\n\u001b[32m    429\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\modeling_layers.py:83\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     80\u001b[39m         logger.warning(message)\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:234\u001b[39m, in \u001b[36mPhiDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, position_ids, past_key_value, output_attentions, use_cache, cache_position, position_embeddings, **kwargs)\u001b[39m\n\u001b[32m    231\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    233\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m attn_outputs, self_attn_weights = \u001b[38;5;28mself\u001b[39m.self_attn(\n\u001b[32m    235\u001b[39m     hidden_states=hidden_states,\n\u001b[32m    236\u001b[39m     attention_mask=attention_mask,\n\u001b[32m    237\u001b[39m     position_ids=position_ids,\n\u001b[32m    238\u001b[39m     past_key_value=past_key_value,\n\u001b[32m    239\u001b[39m     output_attentions=output_attentions,\n\u001b[32m    240\u001b[39m     use_cache=use_cache,\n\u001b[32m    241\u001b[39m     cache_position=cache_position,\n\u001b[32m    242\u001b[39m     position_embeddings=position_embeddings,\n\u001b[32m    243\u001b[39m     **kwargs,\n\u001b[32m    244\u001b[39m )\n\u001b[32m    245\u001b[39m attn_outputs = \u001b[38;5;28mself\u001b[39m.resid_dropout(attn_outputs)\n\u001b[32m    247\u001b[39m feed_forward_hidden_states = \u001b[38;5;28mself\u001b[39m.resid_dropout(\u001b[38;5;28mself\u001b[39m.mlp(hidden_states))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\transformers\\models\\phi\\modeling_phi.py:144\u001b[39m, in \u001b[36mPhiAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_value, cache_position, **kwargs)\u001b[39m\n\u001b[32m    141\u001b[39m input_shape = hidden_states.shape[:-\u001b[32m1\u001b[39m]\n\u001b[32m    142\u001b[39m hidden_shape = (*input_shape, -\u001b[32m1\u001b[39m, \u001b[38;5;28mself\u001b[39m.head_dim)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m query_states = \u001b[38;5;28mself\u001b[39m.q_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    145\u001b[39m key_states = \u001b[38;5;28mself\u001b[39m.k_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m    146\u001b[39m value_states = \u001b[38;5;28mself\u001b[39m.v_proj(hidden_states).view(hidden_shape).transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._call_impl(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(*args, **kwargs)\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\gptqmodel\\nn_modules\\qlinear\\torch.py:154\u001b[39m, in \u001b[36mTorchQuantLinear.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    152\u001b[39m out_shape = x.shape[:-\u001b[32m1\u001b[39m] + (\u001b[38;5;28mself\u001b[39m.out_features,)\n\u001b[32m    153\u001b[39m x = x.reshape(-\u001b[32m1\u001b[39m, x.shape[-\u001b[32m1\u001b[39m])\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m out = \u001b[38;5;28mself\u001b[39m._forward(x, out_shape)\n\u001b[32m    155\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\gptqmodel\\nn_modules\\qlinear\\torch.py:160\u001b[39m, in \u001b[36mTorchQuantLinear._forward\u001b[39m\u001b[34m(self, x, out_shape)\u001b[39m\n\u001b[32m    158\u001b[39m num_itr = \u001b[38;5;28mself\u001b[39m.g_idx.shape[\u001b[32m0\u001b[39m] // x.shape[-\u001b[32m1\u001b[39m]\n\u001b[32m    159\u001b[39m \u001b[38;5;66;03m# make sure dequant dtype matches input x\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m160\u001b[39m weights = \u001b[38;5;28mself\u001b[39m.dequantize_weight(num_itr=num_itr).to(x.dtype)\n\u001b[32m    162\u001b[39m out = torch.matmul(x, weights).reshape(out_shape)\n\u001b[32m    164\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.bias \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\eval_frame.py:655\u001b[39m, in \u001b[36m_TorchDynamoContext.__call__.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    652\u001b[39m _maybe_set_eval_frame(_callback_from_stance(callback))\n\u001b[32m    654\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m655\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    657\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m config.verbose:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1432\u001b[39m, in \u001b[36mCatchErrorsWrapper.__call__\u001b[39m\u001b[34m(self, frame, cache_entry, frame_state)\u001b[39m\n\u001b[32m   1426\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m hijacked_callback(\n\u001b[32m   1427\u001b[39m                 frame, cache_entry, \u001b[38;5;28mself\u001b[39m.hooks, frame_state\n\u001b[32m   1428\u001b[39m             )\n\u001b[32m   1430\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m compile_lock, _disable_current_modes():\n\u001b[32m   1431\u001b[39m     \u001b[38;5;66;03m# skip=1: skip this frame\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1432\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._torchdynamo_orig_callable(\n\u001b[32m   1433\u001b[39m         frame, cache_entry, \u001b[38;5;28mself\u001b[39m.hooks, frame_state, skip=\u001b[32m1\u001b[39m\n\u001b[32m   1434\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1213\u001b[39m, in \u001b[36mConvertFrame.__call__\u001b[39m\u001b[34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[39m\n\u001b[32m   1211\u001b[39m counters[\u001b[33m\"\u001b[39m\u001b[33mframes\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mtotal\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m   1212\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1213\u001b[39m     result = \u001b[38;5;28mself\u001b[39m._inner_convert(\n\u001b[32m   1214\u001b[39m         frame, cache_entry, hooks, frame_state, skip=skip + \u001b[32m1\u001b[39m\n\u001b[32m   1215\u001b[39m     )\n\u001b[32m   1216\u001b[39m     counters[\u001b[33m\"\u001b[39m\u001b[33mframes\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m] += \u001b[32m1\u001b[39m\n\u001b[32m   1217\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:598\u001b[39m, in \u001b[36mConvertFrameAssert.__call__\u001b[39m\u001b[34m(self, frame, cache_entry, hooks, frame_state, skip)\u001b[39m\n\u001b[32m    595\u001b[39m     dynamo_tls.traced_frame_infos.append(info)\n\u001b[32m    597\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m compile_context(CompileContext(compile_id)):\n\u001b[32m--> \u001b[39m\u001b[32m598\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile(\n\u001b[32m    599\u001b[39m         frame.f_code,\n\u001b[32m    600\u001b[39m         frame.f_globals,\n\u001b[32m    601\u001b[39m         frame.f_locals,\n\u001b[32m    602\u001b[39m         frame.f_builtins,\n\u001b[32m    603\u001b[39m         frame.closure,\n\u001b[32m    604\u001b[39m         \u001b[38;5;28mself\u001b[39m._torchdynamo_orig_callable,\n\u001b[32m    605\u001b[39m         \u001b[38;5;28mself\u001b[39m._one_graph,\n\u001b[32m    606\u001b[39m         \u001b[38;5;28mself\u001b[39m._export,\n\u001b[32m    607\u001b[39m         \u001b[38;5;28mself\u001b[39m._export_constraints,\n\u001b[32m    608\u001b[39m         hooks,\n\u001b[32m    609\u001b[39m         cache_entry,\n\u001b[32m    610\u001b[39m         cache_size,\n\u001b[32m    611\u001b[39m         frame,\n\u001b[32m    612\u001b[39m         frame_state=frame_state,\n\u001b[32m    613\u001b[39m         compile_id=compile_id,\n\u001b[32m    614\u001b[39m         skip=skip + \u001b[32m1\u001b[39m,\n\u001b[32m    615\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:1059\u001b[39m, in \u001b[36m_compile\u001b[39m\u001b[34m(code, globals, locals, builtins, closure, compiler_fn, one_graph, export, export_constraints, hooks, cache_entry, cache_size, frame, frame_state, compile_id, skip)\u001b[39m\n\u001b[32m   1057\u001b[39m guarded_code = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1059\u001b[39m     guarded_code = compile_inner(code, one_graph, hooks, transform)\n\u001b[32m   1061\u001b[39m     \u001b[38;5;66;03m# NB: We only put_code_state in success case.  Success case here\u001b[39;00m\n\u001b[32m   1062\u001b[39m     \u001b[38;5;66;03m# does include graph breaks; specifically, if a graph break still\u001b[39;00m\n\u001b[32m   1063\u001b[39m     \u001b[38;5;66;03m# resulted in a partially compiled graph, we WILL return here.  An\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1068\u001b[39m     \u001b[38;5;66;03m# to upload for graph break though, because this can prevent\u001b[39;00m\n\u001b[32m   1069\u001b[39m     \u001b[38;5;66;03m# extra graph break compilations.)\u001b[39;00m\n\u001b[32m   1070\u001b[39m     put_code_state()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_utils_internal.py:97\u001b[39m, in \u001b[36mcompile_time_strobelight_meta.<locals>.compile_time_strobelight_meta_inner.<locals>.wrapper_function\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     94\u001b[39m \u001b[38;5;66;03m# This is not needed but we have it here to avoid having profile_compile_time\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[38;5;66;03m# in stack traces when profiling is not enabled.\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m StrobelightCompileTimeProfiler.enabled:\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m function(*args, **kwargs)\n\u001b[32m     99\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m StrobelightCompileTimeProfiler.profile_compile_time(\n\u001b[32m    100\u001b[39m     function, phase_name, *args, **kwargs\n\u001b[32m    101\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:761\u001b[39m, in \u001b[36m_compile.<locals>.compile_inner\u001b[39m\u001b[34m(code, one_graph, hooks, transform)\u001b[39m\n\u001b[32m    759\u001b[39m     stack.enter_context(torch._dynamo.callback_handler.install_callbacks())\n\u001b[32m    760\u001b[39m     stack.enter_context(CompileTimeInstructionCounter.record())\n\u001b[32m--> \u001b[39m\u001b[32m761\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _compile_inner(code, one_graph, hooks, transform)\n\u001b[32m    763\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    764\u001b[39m     ConvertFrameReturn()\n\u001b[32m    765\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:797\u001b[39m, in \u001b[36m_compile.<locals>._compile_inner\u001b[39m\u001b[34m(code, one_graph, hooks, transform)\u001b[39m\n\u001b[32m    795\u001b[39m CompileContext.get().attempt = attempt\n\u001b[32m    796\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m797\u001b[39m     out_code = transform_code_object(code, transform)\n\u001b[32m    798\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m    799\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exc.RestartAnalysis \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\bytecode_transformation.py:1422\u001b[39m, in \u001b[36mtransform_code_object\u001b[39m\u001b[34m(code, transformations, safe)\u001b[39m\n\u001b[32m   1419\u001b[39m instructions = cleaned_instructions(code, safe)\n\u001b[32m   1420\u001b[39m propagate_line_nums(instructions)\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m transformations(instructions, code_options)\n\u001b[32m   1423\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m clean_and_assemble_instructions(instructions, keys, code_options)[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:257\u001b[39m, in \u001b[36mpreserve_global_state.<locals>._fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    255\u001b[39m exit_stack.enter_context(torch_function_mode_stack_state_mgr)\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    259\u001b[39m     cleanup.close()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\convert_frame.py:715\u001b[39m, in \u001b[36m_compile.<locals>.transform\u001b[39m\u001b[34m(instructions, code_options)\u001b[39m\n\u001b[32m    713\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    714\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tracing(tracer.output.tracing_context), tracer.set_current_tx():\n\u001b[32m--> \u001b[39m\u001b[32m715\u001b[39m         tracer.run()\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m exc.UnspecializeRestartAnalysis:\n\u001b[32m    717\u001b[39m     speculation_log.clear()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:3498\u001b[39m, in \u001b[36mInstructionTranslator.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   3497\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m3498\u001b[39m     \u001b[38;5;28msuper\u001b[39m().run()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:1337\u001b[39m, in \u001b[36mInstructionTranslatorBase.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1335\u001b[39m     \u001b[38;5;28mself\u001b[39m.output.push_tx(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1336\u001b[39m     \u001b[38;5;28mself\u001b[39m.start_point = \u001b[38;5;28mself\u001b[39m.instruction_pointer\n\u001b[32m-> \u001b[39m\u001b[32m1337\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m.step():\n\u001b[32m   1338\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[32m   1339\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:1246\u001b[39m, in \u001b[36mInstructionTranslatorBase.step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1243\u001b[39m \u001b[38;5;28mself\u001b[39m.update_block_stack(inst)\n\u001b[32m   1245\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1246\u001b[39m     \u001b[38;5;28mself\u001b[39m.dispatch_table[inst.opcode](\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[32m   1247\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.output.should_exit\n\u001b[32m   1248\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m TensorifyScalarRestartAnalysis:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:819\u001b[39m, in \u001b[36mbreak_graph_if_unsupported.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(self, inst)\u001b[39m\n\u001b[32m    817\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_graph_break(\u001b[38;5;28mself\u001b[39m, inst, speculation.reason)\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m819\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_fn(\u001b[38;5;28mself\u001b[39m, inst)\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported \u001b[38;5;28;01mas\u001b[39;00m excp:\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.active_generic_context_managers:\n\u001b[32m    822\u001b[39m         \u001b[38;5;66;03m# We don't support graph break under GenericContextWrappingVariable,\u001b[39;00m\n\u001b[32m    823\u001b[39m         \u001b[38;5;66;03m# If there is, we roll back to the checkpoint and fall back.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2931\u001b[39m, in \u001b[36mInstructionTranslatorBase.CALL\u001b[39m\u001b[34m(self, inst)\u001b[39m\n\u001b[32m   2929\u001b[39m \u001b[38;5;129m@break_graph_if_unsupported\u001b[39m(push=\u001b[32m1\u001b[39m)\n\u001b[32m   2930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mCALL\u001b[39m(\u001b[38;5;28mself\u001b[39m, inst):\n\u001b[32m-> \u001b[39m\u001b[32m2931\u001b[39m     \u001b[38;5;28mself\u001b[39m._call(inst)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:2925\u001b[39m, in \u001b[36mInstructionTranslatorBase._call\u001b[39m\u001b[34m(self, inst, call_kw)\u001b[39m\n\u001b[32m   2920\u001b[39m     kwargs = {}\n\u001b[32m   2922\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   2923\u001b[39m     \u001b[38;5;66;03m# if call_function fails, need to set kw_names to None, otherwise\u001b[39;00m\n\u001b[32m   2924\u001b[39m     \u001b[38;5;66;03m# a subsequent call may have self.kw_names set to an old value\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2925\u001b[39m     \u001b[38;5;28mself\u001b[39m.call_function(fn, args, kwargs)\n\u001b[32m   2926\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m   2927\u001b[39m     \u001b[38;5;28mself\u001b[39m.kw_names = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\symbolic_convert.py:1170\u001b[39m, in \u001b[36mInstructionTranslatorBase.call_function\u001b[39m\u001b[34m(self, fn, args, kwargs)\u001b[39m\n\u001b[32m   1168\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inner_fn \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(inner_fn) \u001b[38;5;129;01mand\u001b[39;00m is_forbidden(inner_fn):\n\u001b[32m   1169\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mAttempt to trace forbidden callable \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minner_fn\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1170\u001b[39m \u001b[38;5;28mself\u001b[39m.push(fn.call_function(\u001b[38;5;28mself\u001b[39m, args, kwargs))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\variables\\torch.py:1181\u001b[39m, in \u001b[36mTorchInGraphFunctionVariable.call_function\u001b[39m\u001b[34m(self, tx, args, kwargs)\u001b[39m\n\u001b[32m   1172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mout\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(kwargs[\u001b[33m\"\u001b[39m\u001b[33mout\u001b[39m\u001b[33m\"\u001b[39m], variables.TensorVariable):\n\u001b[32m   1173\u001b[39m     \u001b[38;5;66;03m# Calling fake tensor propagation can mutate the out= tensor in\u001b[39;00m\n\u001b[32m   1174\u001b[39m     \u001b[38;5;66;03m# tx.output.tracked_fakes. tracked_fakes are used to apply\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1177\u001b[39m     \u001b[38;5;66;03m# guards. So save the shape now, and check later if it has\u001b[39;00m\n\u001b[32m   1178\u001b[39m     \u001b[38;5;66;03m# changed. If it has, graph break.\u001b[39;00m\n\u001b[32m   1179\u001b[39m     fake_out_shape = kwargs[\u001b[33m\"\u001b[39m\u001b[33mout\u001b[39m\u001b[33m\"\u001b[39m].proxy.node.meta[\u001b[33m\"\u001b[39m\u001b[33mexample_value\u001b[39m\u001b[33m\"\u001b[39m].shape\n\u001b[32m-> \u001b[39m\u001b[32m1181\u001b[39m tensor_variable = wrap_fx_proxy(\n\u001b[32m   1182\u001b[39m     tx=tx,\n\u001b[32m   1183\u001b[39m     proxy=tx.output.create_proxy(\n\u001b[32m   1184\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mcall_function\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1185\u001b[39m         fn_,\n\u001b[32m   1186\u001b[39m         *proxy_args_kwargs(args, kwargs),\n\u001b[32m   1187\u001b[39m     ),\n\u001b[32m   1188\u001b[39m )\n\u001b[32m   1190\u001b[39m \u001b[38;5;66;03m# Handle e.g., `torch.ones(10, requires_grad=True)`\u001b[39;00m\n\u001b[32m   1191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1192\u001b[39m     \u001b[38;5;28misinstance\u001b[39m(tensor_variable, TensorVariable)\n\u001b[32m   1193\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mrequires_grad\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m kwargs\n\u001b[32m   1194\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m kwargs[\u001b[33m\"\u001b[39m\u001b[33mrequires_grad\u001b[39m\u001b[33m\"\u001b[39m].as_python_constant()\n\u001b[32m   1195\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\variables\\builder.py:2302\u001b[39m, in \u001b[36mwrap_fx_proxy\u001b[39m\u001b[34m(tx, proxy, example_value, subclass_type, **options)\u001b[39m\n\u001b[32m   2294\u001b[39m kwargs = {\n\u001b[32m   2295\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtx\u001b[39m\u001b[33m\"\u001b[39m: tx,\n\u001b[32m   2296\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mproxy\u001b[39m\u001b[33m\"\u001b[39m: proxy,\n\u001b[32m   (...)\u001b[39m\u001b[32m   2299\u001b[39m     **options,\n\u001b[32m   2300\u001b[39m }\n\u001b[32m   2301\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m subclass_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2302\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m wrap_fx_proxy_cls(target_cls=TensorVariable, **kwargs)\n\u001b[32m   2303\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2304\u001b[39m     result = wrap_fx_proxy_cls(target_cls=TensorWithTFOverrideVariable, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\variables\\builder.py:2368\u001b[39m, in \u001b[36mwrap_fx_proxy_cls\u001b[39m\u001b[34m(target_cls, tx, proxy, example_value, subclass_type, **options)\u001b[39m\n\u001b[32m   2364\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_fx_proxy_cls\u001b[39m(\n\u001b[32m   2365\u001b[39m     target_cls, tx, proxy, example_value=\u001b[38;5;28;01mNone\u001b[39;00m, subclass_type=\u001b[38;5;28;01mNone\u001b[39;00m, **options\n\u001b[32m   2366\u001b[39m ):\n\u001b[32m   2367\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m example_value \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2368\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_fx_proxy(\n\u001b[32m   2369\u001b[39m             target_cls, tx, proxy, example_value, subclass_type, **options\n\u001b[32m   2370\u001b[39m         )\n\u001b[32m   2371\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(example_value, torch.Tensor):\n\u001b[32m   2372\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m _wrap_fx_preexisting_tensor(\n\u001b[32m   2373\u001b[39m             target_cls, tx, proxy, example_value, subclass_type, **options\n\u001b[32m   2374\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\variables\\builder.py:2464\u001b[39m, in \u001b[36m_wrap_fx_proxy\u001b[39m\u001b[34m(target_cls, tx, proxy, example_value, subclass_type, **options)\u001b[39m\n\u001b[32m   2459\u001b[39m \u001b[38;5;66;03m# See NOTE: [Deferring tensor pack/unpack hooks until runtime]\u001b[39;00m\n\u001b[32m   2460\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch._dynamo.utils._disable_saved_tensors_hooks_during_tracing():\n\u001b[32m   2461\u001b[39m     \u001b[38;5;66;03m# with preserve_rng_state():\u001b[39;00m\n\u001b[32m   2462\u001b[39m     \u001b[38;5;66;03m# only allow_non_graph_fake in this instance because we handle the non-fake\u001b[39;00m\n\u001b[32m   2463\u001b[39m     \u001b[38;5;66;03m# cases properly below.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2464\u001b[39m     example_value = get_fake_value(proxy.node, tx, allow_non_graph_fake=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m   2466\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m handle_traced_output(\n\u001b[32m   2467\u001b[39m     example_value, tx, proxy, options, subclass_type, target_cls\n\u001b[32m   2468\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\utils.py:3229\u001b[39m, in \u001b[36mget_fake_value\u001b[39m\u001b[34m(node, tx, allow_non_graph_fake)\u001b[39m\n\u001b[32m   3221\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(cause, \u001b[38;5;167;01mTypeError\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33margument\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(cause):\n\u001b[32m   3222\u001b[39m         unimplemented_v2(\n\u001b[32m   3223\u001b[39m             gb_type=\u001b[33m\"\u001b[39m\u001b[33mTypeError when making fake tensor call\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3224\u001b[39m             context=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTypeError \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode.target\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcause\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m   3225\u001b[39m             explanation=\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   3226\u001b[39m             hints=[],\n\u001b[32m   3227\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m3229\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m TorchRuntimeError(\u001b[38;5;28mstr\u001b[39m(e)).with_traceback(e.__traceback__) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   3231\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_non_graph_fake:\n\u001b[32m   3232\u001b[39m     _ = pytree.tree_map_only(\n\u001b[32m   3233\u001b[39m         torch.Tensor, functools.partial(ensure_graph_fake, tx=tx), ret_val\n\u001b[32m   3234\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\utils.py:3127\u001b[39m, in \u001b[36mget_fake_value\u001b[39m\u001b[34m(node, tx, allow_non_graph_fake)\u001b[39m\n\u001b[32m   3125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   3126\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tx.fake_mode, enable_python_dispatcher():\n\u001b[32m-> \u001b[39m\u001b[32m3127\u001b[39m         ret_val = wrap_fake_exception(\n\u001b[32m   3128\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m: run_node(tx.output, node, args, kwargs, nnmodule)\n\u001b[32m   3129\u001b[39m         )\n\u001b[32m   3130\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n\u001b[32m   3131\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\utils.py:2641\u001b[39m, in \u001b[36mwrap_fake_exception\u001b[39m\u001b[34m(fn)\u001b[39m\n\u001b[32m   2639\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrap_fake_exception\u001b[39m(fn):\n\u001b[32m   2640\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2641\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m fn()\n\u001b[32m   2642\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m UnsupportedFakeTensorException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   2643\u001b[39m         \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mexc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m unimplemented_v2\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\utils.py:3128\u001b[39m, in \u001b[36mget_fake_value.<locals>.<lambda>\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   3125\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   3126\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m tx.fake_mode, enable_python_dispatcher():\n\u001b[32m   3127\u001b[39m         ret_val = wrap_fake_exception(\n\u001b[32m-> \u001b[39m\u001b[32m3128\u001b[39m             \u001b[38;5;28;01mlambda\u001b[39;00m: run_node(tx.output, node, args, kwargs, nnmodule)\n\u001b[32m   3129\u001b[39m         )\n\u001b[32m   3130\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m Unsupported:\n\u001b[32m   3131\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\utils.py:3325\u001b[39m, in \u001b[36mrun_node\u001b[39m\u001b[34m(tracer, node, args, kwargs, nnmodule)\u001b[39m\n\u001b[32m   3323\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   3324\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m-> \u001b[39m\u001b[32m3325\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(make_error_message(e)).with_traceback(\n\u001b[32m   3326\u001b[39m             e.__traceback__\n\u001b[32m   3327\u001b[39m         ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   3329\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(op)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_dynamo\\utils.py:3284\u001b[39m, in \u001b[36mrun_node\u001b[39m\u001b[34m(tracer, node, args, kwargs, nnmodule)\u001b[39m\n\u001b[32m   3282\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   3283\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m op == \u001b[33m\"\u001b[39m\u001b[33mcall_function\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m3284\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m node.target(*args, **kwargs)\n\u001b[32m   3285\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m op == \u001b[33m\"\u001b[39m\u001b[33mcall_method\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   3286\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(args[\u001b[32m0\u001b[39m], node.target):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\utils\\_stats.py:27\u001b[39m, in \u001b[36mcount.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     25\u001b[39m     simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] = \u001b[32m0\u001b[39m\n\u001b[32m     26\u001b[39m simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] = simple_call_counter[fn.\u001b[34m__qualname__\u001b[39m] + \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m fn(*args, **kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_subclasses\\fake_tensor.py:1282\u001b[39m, in \u001b[36mFakeTensorMode.__torch_dispatch__\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1278\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (\n\u001b[32m   1279\u001b[39m     torch._C._get_dispatch_mode(torch._C._TorchDispatchModeKey.FAKE) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1280\u001b[39m ), func\n\u001b[32m   1281\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1282\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dispatch(func, types, args, kwargs)\n\u001b[32m   1283\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   1284\u001b[39m     log.exception(\u001b[33m\"\u001b[39m\u001b[33mfake tensor raised TypeError\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_subclasses\\fake_tensor.py:1823\u001b[39m, in \u001b[36mFakeTensorMode.dispatch\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1820\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m func(*args, **kwargs)\n\u001b[32m   1822\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cache_enabled:\n\u001b[32m-> \u001b[39m\u001b[32m1823\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._cached_dispatch_impl(func, types, args, kwargs)\n\u001b[32m   1824\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1825\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._dispatch_impl(func, types, args, kwargs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_subclasses\\fake_tensor.py:1384\u001b[39m, in \u001b[36mFakeTensorMode._cached_dispatch_impl\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1383\u001b[39m     \u001b[38;5;28mself\u001b[39m._validate_cache_key(func, args, kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m1384\u001b[39m     output = \u001b[38;5;28mself\u001b[39m._dispatch_impl(func, types, args, kwargs)\n\u001b[32m   1385\u001b[39m     entry = \u001b[38;5;28mself\u001b[39m._make_cache_entry(state, key, func, args, kwargs, output)\n\u001b[32m   1386\u001b[39m     key.strip_shape_env()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_subclasses\\fake_tensor.py:2435\u001b[39m, in \u001b[36mFakeTensorMode._dispatch_impl\u001b[39m\u001b[34m(self, func, types, args, kwargs)\u001b[39m\n\u001b[32m   2431\u001b[39m     log.exception(\u001b[33m\"\u001b[39m\u001b[33mfailed while attempting to run meta for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m, func)\n\u001b[32m   2432\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m   2434\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m maybe_propagate_real_tensors(\n\u001b[32m-> \u001b[39m\u001b[32m2435\u001b[39m     \u001b[38;5;28mself\u001b[39m.wrap_meta_outputs_with_default_device_logic(\n\u001b[32m   2436\u001b[39m         r, func, flat_args, device=kwargs.get(\u001b[33m\"\u001b[39m\u001b[33mdevice\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2437\u001b[39m     )\n\u001b[32m   2438\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_subclasses\\fake_tensor.py:2562\u001b[39m, in \u001b[36mFakeTensorMode.wrap_meta_outputs_with_default_device_logic\u001b[39m\u001b[34m(self, r, func, flat_args, device)\u001b[39m\n\u001b[32m   2559\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2560\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m e\n\u001b[32m-> \u001b[39m\u001b[32m2562\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m tree_map(wrap, r)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:1145\u001b[39m, in \u001b[36mtree_map\u001b[39m\u001b[34m(func, tree, is_leaf, *rests)\u001b[39m\n\u001b[32m   1143\u001b[39m leaves, treespec = tree_flatten(tree, is_leaf=is_leaf)\n\u001b[32m   1144\u001b[39m flat_args = [leaves] + [treespec.flatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[32m-> \u001b[39m\u001b[32m1145\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m treespec.unflatten(\u001b[38;5;28mmap\u001b[39m(func, *flat_args))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\utils\\_pytree.py:982\u001b[39m, in \u001b[36mTreeSpec.unflatten\u001b[39m\u001b[34m(self, leaves)\u001b[39m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34munflatten\u001b[39m(\u001b[38;5;28mself\u001b[39m, leaves: Iterable[Any]) -> PyTree:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(leaves, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[32m--> \u001b[39m\u001b[32m982\u001b[39m         leaves = \u001b[38;5;28mlist\u001b[39m(leaves)\n\u001b[32m    983\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(leaves) != \u001b[38;5;28mself\u001b[39m.num_leaves:\n\u001b[32m    984\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    985\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mtreespec.unflatten(leaves): `leaves` has length \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(leaves)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    986\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mbut the spec refers to a pytree that holds \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.num_leaves\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    987\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mitems (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m).\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    988\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_subclasses\\fake_tensor.py:2540\u001b[39m, in \u001b[36mFakeTensorMode.wrap_meta_outputs_with_default_device_logic.<locals>.wrap\u001b[39m\u001b[34m(e)\u001b[39m\n\u001b[32m   2534\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m e\n\u001b[32m   2536\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m common_device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   2537\u001b[39m     (\n\u001b[32m   2538\u001b[39m         common_device,\n\u001b[32m   2539\u001b[39m         has_scalar_only_inputs,\n\u001b[32m-> \u001b[39m\u001b[32m2540\u001b[39m     ) = FakeTensor._find_common_device(func, flat_args)\n\u001b[32m   2542\u001b[39m is_our_fake = \u001b[38;5;28mself\u001b[39m.is_our_fake(e)\n\u001b[32m   2543\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_our_fake:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_subclasses\\fake_tensor.py:896\u001b[39m, in \u001b[36mFakeTensor._find_common_device\u001b[39m\u001b[34m(func, flat_args)\u001b[39m\n\u001b[32m    891\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    892\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnhandled FakeTensor Device Propagation for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, found two different devices \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommon_device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    893\u001b[39m     )\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m arg \u001b[38;5;129;01min\u001b[39;00m flat_args:\n\u001b[32m--> \u001b[39m\u001b[32m896\u001b[39m     merge_devices(arg)\n\u001b[32m    898\u001b[39m \u001b[38;5;66;03m# some functions that allow Python numbers to bind to Tensors\u001b[39;00m\n\u001b[32m    899\u001b[39m \u001b[38;5;66;03m# if we have failed to find a device, and we're running one of these operators,\u001b[39;00m\n\u001b[32m    900\u001b[39m \u001b[38;5;66;03m# we must have scalar only inputs\u001b[39;00m\n\u001b[32m    901\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m should_allow_numbers_as_tensors(func) \u001b[38;5;129;01mand\u001b[39;00m common_device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    902\u001b[39m     \u001b[38;5;66;03m# ops with scalar only inputs always have result on cpu\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\torch\\_subclasses\\fake_tensor.py:891\u001b[39m, in \u001b[36mFakeTensor._find_common_device.<locals>.merge_devices\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m    887\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    889\u001b[39m \u001b[38;5;66;03m# mismatching devices of non-zero dim tensors, throw\u001b[39;00m\n\u001b[32m    890\u001b[39m \u001b[38;5;66;03m# This might be valid behavior and need to be explicitly modeled, e.g. reshape_as\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m891\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    892\u001b[39m     \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnhandled FakeTensor Device Propagation for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, found two different devices \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcommon_device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mt.device\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    893\u001b[39m )\n",
      "\u001b[31mTorchRuntimeError\u001b[39m: Dynamo failed to run FX node with fake tensors: call_function <built-in method bitwise_right_shift of type object at 0x00007FF823D0C450>(*(FakeTensor(..., size=(32, 128, 16), dtype=torch.int32), FakeTensor(..., device='cuda:0', size=(1, 1, 16), dtype=torch.int32)), **{}): got RuntimeError('Unhandled FakeTensor Device Propagation for aten.bitwise_right_shift.Tensor, found two different devices cpu, cuda:0')\n\nfrom user code:\n   File \"c:\\Users\\marwa\\miniconda3\\Lib\\site-packages\\gptqmodel\\nn_modules\\qlinear\\__init__.py\", line 441, in dequantize_weight\n    zeros = t.bitwise_right_shift(\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "\n",
    "# Set debug flags\n",
    "os.environ['TORCHDYNAMO_VERBOSE'] = '1'\n",
    "os.environ['TORCH_LOGS'] = '+dynamo'\n",
    "\n",
    "# Force CPU usage\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = ''\n",
    "torch.cuda.is_available = lambda: False  # Force PyTorch to think no GPU is available\n",
    "\n",
    "# Load model and ensure it's on CPU\n",
    "quantized_model_id = output_dir_2\n",
    "model = GPTQModel.load(quantized_model_id)\n",
    "model = model.cpu()  # Explicitly move model to CPU\n",
    "\n",
    "# Prepare inputs on CPU\n",
    "inputs = tokenizer(\"gptqmodel is\", return_tensors=\"pt\")\n",
    "# Ensure inputs are on CPU (they should be by default now)\n",
    "\n",
    "# Generate on CPU\n",
    "with torch.no_grad():\n",
    "    output = model.generate(**inputs, max_length=50, do_sample=False)\n",
    "    \n",
    "print(tokenizer.decode(output[0], skip_special_tokens=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
