{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lk7-e-1ZzfUQ"
      },
      "source": [
        "# Quantization Method Using AWQ"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "RJuKv8q8_A1c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f833df9b-477e-406b-d1cf-cc5e608ce913"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q --upgrade transformers accelerate huggingface_hub peft fsspec==2025.3.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "GPZIH2fg9ZSx",
        "outputId": "3235633f-2371-40bb-b9bc-5f216485431a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m118.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m85.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m43.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Quantization of TinyLlama using AWQ to int4"
      ],
      "metadata": {
        "id": "xUExODN0CPaR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"ğŸ§  PyTorch version:\", torch.__version__)\n",
        "print(\"âš™ï¸ CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"ğŸš€ CUDA device name:\", torch.cuda.get_device_name(0))\n",
        "    print(\"ğŸ’¾ Total GPU memory (MB):\", torch.cuda.get_device_properties(0).total_memory // (1024 * 1024))\n",
        "else:\n",
        "    print(\"âŒ No CUDA-compatible GPU found or PyTorch not built with CUDA\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LDp7rTkbGRrF",
        "outputId": "92a79ae2-c66c-4981-ecec-37a7a173a367"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  PyTorch version: 2.7.1+cu126\n",
            "âš™ï¸ CUDA available: True\n",
            "ğŸš€ CUDA device name: Tesla T4\n",
            "ğŸ’¾ Total GPU memory (MB): 15095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q autoawq"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ebrxoie4Th6y",
        "outputId": "a0b8ef28-060f-4bb7-98c8-e03c27f951aa"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m0.0/74.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m74.3/74.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for autoawq (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "print(\"ğŸ§  PyTorch version:\", torch.__version__)\n",
        "print(\"âš™ï¸ CUDA available:\", torch.cuda.is_available())\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(\"ğŸš€ CUDA device name:\", torch.cuda.get_device_name(0))\n",
        "    print(\"ğŸ’¾ Total GPU memory (MB):\", torch.cuda.get_device_properties(0).total_memory // (1024 * 1024))\n",
        "else:\n",
        "    print(\"âŒ No CUDA-compatible GPU found or PyTorch not built with CUDA\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O2jrzbT8i6Sa",
        "outputId": "1e5eb45b-2c7d-4dc8-9424-65652126aede"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ§  PyTorch version: 2.6.0+cu124\n",
            "âš™ï¸ CUDA available: True\n",
            "ğŸš€ CUDA device name: Tesla T4\n",
            "ğŸ’¾ Total GPU memory (MB): 15095\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DWuRZIksl79R"
      },
      "outputs": [],
      "source": [
        "!mkdir -p /content/drive/MyDrive/llm_quant_awq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "from awq import AutoAWQForCausalLM\n",
        "\n",
        "# Model path\n",
        "model_path = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
        "save_dir = \"/content/drive/MyDrive/llm_quant_awq/TinyLlama-awq-int4\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
        "model = AutoAWQForCausalLM.from_pretrained(model_path)\n",
        "\n",
        "# AWQ quantization config\n",
        "quant_config = {\n",
        "    \"w_bit\": 4,\n",
        "    \"q_group_size\": 128,\n",
        "    \"zero_point\": True,\n",
        "    \"version\": \"GEMM\"\n",
        "}\n",
        "\n",
        "# Quantize and save\n",
        "print(\"ğŸš€ Quantizing to 4-bit (AWQ)...\")\n",
        "model.quantize(tokenizer=tokenizer, quant_config=quant_config)\n",
        "model.save_quantized(save_dir)\n",
        "tokenizer.save_pretrained(save_dir)\n",
        "print(f\"âœ… Quantized model saved to: {save_dir}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173,
          "referenced_widgets": [
            "284b5c9c70c24e6899247c98077bdac8",
            "e4404a2e4a1f4884af974972eee9f9ca",
            "39fcc7631dbf43568459ec84bf0fb902",
            "665f0b95685240b28c79ab48b879eb77",
            "1e605c1d1b274504909931d3cf629565",
            "da84f12de7554856a5a058e2946f013c",
            "4eea44afe7f34490800b685ffbf83c5e",
            "d1ac4f16069e4842827f8362a043ba76",
            "d23334d5b9a14dd7a7c09ecf889957fc",
            "953a62c5066943a4b2ffc01d4950f6e5",
            "2a5c100afef049e0acd78294f1e42cf0"
          ]
        },
        "id": "yNBJjKcXcEvn",
        "outputId": "842e466f-b897-445e-e805-7021fd6b75ab"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 10 files:   0%|          | 0/10 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "284b5c9c70c24e6899247c98077bdac8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸš€ Quantizing to 4-bit (AWQ)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Repo card metadata block was not found. Setting CardData to empty.\n",
            "WARNING:huggingface_hub.repocard:Repo card metadata block was not found. Setting CardData to empty.\n",
            "Token indices sequence length is longer than the specified maximum sequence length for this model (8322 > 2048). Running this sequence through the model will result in indexing errors\n",
            "AWQ: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [19:26<00:00, 53.03s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Quantized model saved to: /content/drive/MyDrive/llm_quant_awq/TinyLlama-awq-int4_1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOKW3E-s_YEZ"
      },
      "source": [
        "# Testing TinyLlma - 1.1 B AWQ-int4 model Chatbot"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import math\n",
        "import psutil\n",
        "import os\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from transformers import AutoTokenizer\n",
        "from awq import AutoAWQForCausalLM\n",
        "\n",
        "# Configuration\n",
        "QUANT_DIR = \"/content/drive/MyDrive/llm_quant_awq/TinyLlama-awq-int4\"\n",
        "\n",
        "def load_quantized_model():\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"ğŸ”§ Using device: {device}\")\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(QUANT_DIR)\n",
        "    model = AutoAWQForCausalLM.from_quantized(\n",
        "        QUANT_DIR,\n",
        "        device_map=\"auto\",\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    model.eval()\n",
        "    return tokenizer, model, device\n",
        "\n",
        "def get_cpu_ram_usage():\n",
        "    process = psutil.Process(os.getpid())\n",
        "    ram = process.memory_info().rss / (1024 * 1024)\n",
        "    cpu = process.cpu_percent(interval=0.1)\n",
        "    return round(cpu, 2), round(ram, 2)\n",
        "\n",
        "def get_gpu_ram_usage():\n",
        "    if torch.cuda.is_available():\n",
        "        used = torch.cuda.memory_allocated() / (1024 ** 2)\n",
        "        total = torch.cuda.get_device_properties(0).total_memory / (1024 ** 2)\n",
        "        return round(used, 2), round(total, 2)\n",
        "    return 0, 0\n",
        "\n",
        "def compute_perplexity(model, tokenizer, prompt, device):\n",
        "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs, labels=inputs[\"input_ids\"])\n",
        "        loss = outputs.loss\n",
        "        return math.exp(loss.item())\n",
        "\n",
        "def compute_entropy_from_scores(scores):\n",
        "    entropies = []\n",
        "    for score in scores:\n",
        "        probs = F.softmax(score, dim=-1)\n",
        "        log_probs = F.log_softmax(score, dim=-1)\n",
        "        entropy = -(probs * log_probs).sum(dim=-1)\n",
        "        entropies.append(entropy.item())\n",
        "    avg_entropy = sum(entropies) / len(entropies) if entropies else 0.0\n",
        "    return round(avg_entropy, 4)\n",
        "\n",
        "def chat_loop(tokenizer, model, device, max_new_tokens=150):\n",
        "    print(\"\\nğŸ¤– TinyLlama Chatbot (4-bit AWQ) is ready! Type 'exit' to quit.\\n\")\n",
        "    print(f\"âš¡ Quantized model loaded from: {QUANT_DIR}\\n\")\n",
        "\n",
        "    while True:\n",
        "        user_input = input(\"ğŸ‘¤ You: \").strip()\n",
        "        if user_input.lower() in {\"exit\", \"quit\"}:\n",
        "            print(\"ğŸ‘‹ Exiting chat. Goodbye!\")\n",
        "            break\n",
        "\n",
        "        messages = [{\"role\": \"user\", \"content\": user_input}]\n",
        "        prompt_text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "\n",
        "        # Tokenize and get input length\n",
        "        inputs = tokenizer(prompt_text, return_tensors=\"pt\").to(device)\n",
        "        input_token_count = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "        # Generation with timing\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=False,\n",
        "                pad_token_id=tokenizer.eos_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                output_scores=True,\n",
        "                return_dict_in_generate=True\n",
        "            )\n",
        "        duration = time.time() - start_time\n",
        "\n",
        "        # Calculate output metrics\n",
        "        generated_ids = outputs.sequences[0][input_token_count:]\n",
        "        response = tokenizer.decode(generated_ids, skip_special_tokens=True).strip()\n",
        "        response_token_count = len(generated_ids)\n",
        "        response_length_chars = len(response)\n",
        "\n",
        "        # System metrics\n",
        "        cpu_usage, ram_usage = get_cpu_ram_usage()\n",
        "        gpu_used, gpu_total = get_gpu_ram_usage()\n",
        "        perplexity = compute_perplexity(model, tokenizer, prompt_text, device)\n",
        "        tokens_per_sec = round(response_token_count / max(duration, 0.001), 2)\n",
        "        avg_entropy = compute_entropy_from_scores(outputs.scores)\n",
        "\n",
        "        # Display all metrics\n",
        "        print(f\"\\nğŸ¤– Bot: {response}\")\n",
        "        print(f\"ğŸ“ Input tokens: {input_token_count}\")\n",
        "        print(f\"ğŸ“ Output tokens: {response_token_count} ({response_length_chars} chars)\")\n",
        "        print(f\"â± Generation time: {duration:.2f}s ({tokens_per_sec} tokens/sec)\")\n",
        "        print(f\"ğŸ’» CPU: {cpu_usage}%, RAM: {ram_usage} MB\")\n",
        "        print(f\"ğŸ® GPU: {gpu_used} / {gpu_total} MB (allocated)\")\n",
        "        print(f\"ğŸ“‰ Perplexity (prompt): {perplexity:.2f}\")\n",
        "        print(f\"ğŸ§  Avg Token Entropy: {avg_entropy}\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    tokenizer, model, device = load_quantized_model()\n",
        "    chat_loop(tokenizer, model, device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUsRRNGDuffk",
        "outputId": "e642d5d9-0a78-4c5b-de2c-6d4d3fc4c53e"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/awq/__init__.py:21: DeprecationWarning: \n",
            "I have left this message as the final dev message to help you transition.\n",
            "\n",
            "Important Notice:\n",
            "- AutoAWQ is officially deprecated and will no longer be maintained.\n",
            "- The last tested configuration used Torch 2.6.0 and Transformers 4.51.3.\n",
            "- If future versions of Transformers break AutoAWQ compatibility, please report the issue to the Transformers project.\n",
            "\n",
            "Alternative:\n",
            "- AutoAWQ has been adopted by the vLLM Project: https://github.com/vllm-project/llm-compressor\n",
            "\n",
            "For further inquiries, feel free to reach out:\n",
            "- X: https://x.com/casper_hansen_\n",
            "- LinkedIn: https://www.linkedin.com/in/casper-hansen-804005170/\n",
            "\n",
            "  warnings.warn(_FINAL_DEV_MESSAGE, category=DeprecationWarning, stacklevel=1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ğŸ”§ Using device: cuda\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Replacing layers...: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22/22 [00:21<00:00,  1.01it/s]\n",
            "/usr/local/lib/python3.11/dist-packages/awq/models/base.py:541: UserWarning: Skipping fusing modules because AWQ extension is not installed.No module named 'awq_ext'\n",
            "  warnings.warn(\"Skipping fusing modules because AWQ extension is not installed.\" + msg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¤– TinyLlama Chatbot (4-bit AWQ) is ready! Type 'exit' to quit.\n",
            "\n",
            "âš¡ Quantized model loaded from: /content/drive/MyDrive/llm_quant_awq/TinyLlama-awq-int4\n",
            "\n",
            "ğŸ‘¤ You: Hi. How are you?\n",
            "\n",
            "ğŸ¤– Bot: I am doing well. How about you?\n",
            "ğŸ“ Input tokens: 23\n",
            "ğŸ“ Output tokens: 10 (31 chars)\n",
            "â± Generation time: 4.81s (2.08 tokens/sec)\n",
            "ğŸ’» CPU: 0.0%, RAM: 2007.03 MB\n",
            "ğŸ® GPU: 747.57 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 5.03\n",
            "ğŸ§  Avg Token Entropy: 0.9351\n",
            "\n",
            "ğŸ‘¤ You: What is your name?\n",
            "\n",
            "ğŸ¤– Bot: My name is Sarah.\n",
            "ğŸ“ Input tokens: 22\n",
            "ğŸ“ Output tokens: 6 (17 chars)\n",
            "â± Generation time: 0.34s (17.63 tokens/sec)\n",
            "ğŸ’» CPU: 0.0%, RAM: 2030.8 MB\n",
            "ğŸ® GPU: 747.08 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 4.92\n",
            "ğŸ§  Avg Token Entropy: 1.413\n",
            "\n",
            "ğŸ‘¤ You: How old are you?\n",
            "\n",
            "ğŸ¤– Bot: I do not have a physical body to answer questions. However, based on the given text, it is implied that the speaker is in their mid-20s or early 30s.\n",
            "ğŸ“ Input tokens: 22\n",
            "ğŸ“ Output tokens: 42 (149 chars)\n",
            "â± Generation time: 2.28s (18.38 tokens/sec)\n",
            "ğŸ’» CPU: 0.0%, RAM: 2031.12 MB\n",
            "ğŸ® GPU: 751.48 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 4.78\n",
            "ğŸ§  Avg Token Entropy: 1.2411\n",
            "\n",
            "ğŸ‘¤ You: Where do you live?\n",
            "\n",
            "ğŸ¤– Bot: I do not have a physical location. However, I can provide you with information about the location of the author's residence. The author lives in the united states.\n",
            "ğŸ“ Input tokens: 22\n",
            "ğŸ“ Output tokens: 36 (163 chars)\n",
            "â± Generation time: 1.95s (18.44 tokens/sec)\n",
            "ğŸ’» CPU: 0.0%, RAM: 2031.12 MB\n",
            "ğŸ® GPU: 750.74 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 4.69\n",
            "ğŸ§  Avg Token Entropy: 1.5573\n",
            "\n",
            "ğŸ‘¤ You: Where is Berlin?\n",
            "\n",
            "ğŸ¤– Bot: Berlin is the capital city of Germany.\n",
            "ğŸ“ Input tokens: 21\n",
            "ğŸ“ Output tokens: 10 (38 chars)\n",
            "â± Generation time: 0.55s (18.21 tokens/sec)\n",
            "ğŸ’» CPU: 0.0%, RAM: 2031.12 MB\n",
            "ğŸ® GPU: 747.57 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 7.35\n",
            "ğŸ§  Avg Token Entropy: 0.4758\n",
            "\n",
            "ğŸ‘¤ You: Where is Dhaka?\n",
            "\n",
            "ğŸ¤– Bot: Dhaka is the capital and largest city of Bangladesh. It is located in the southern part of the country, on the banks of the Buriganga River.\n",
            "ğŸ“ Input tokens: 23\n",
            "ğŸ“ Output tokens: 36 (140 chars)\n",
            "â± Generation time: 1.97s (18.27 tokens/sec)\n",
            "ğŸ’» CPU: 0.0%, RAM: 2031.12 MB\n",
            "ğŸ® GPU: 750.74 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 6.38\n",
            "ğŸ§  Avg Token Entropy: 0.7188\n",
            "\n",
            "ğŸ‘¤ You: Who is Albert Einstein?\n",
            "\n",
            "ğŸ¤– Bot: Albert Einstein was a German-born theoretical physicist, mathematician, and philosopher who made significant contributions to the field of physics, particularly in the fields of general relativity and quantum mechanics. He was born in Ulm, Germany, in 1879 and died in Princeton, New Jersey, in 1955.\n",
            "\n",
            "Einstein is best known for his theory of general relativity, which explains how gravity behaves when it is combined with the curvature of space-time. This theory revolutionized the understanding of the universe and has since been applied to a wide range of fields, including cosmology, astrophysics, and space exploration.\n",
            "\n",
            "Ein\n",
            "ğŸ“ Input tokens: 23\n",
            "ğŸ“ Output tokens: 150 (629 chars)\n",
            "â± Generation time: 9.60s (15.62 tokens/sec)\n",
            "ğŸ’» CPU: 0.0%, RAM: 2031.38 MB\n",
            "ğŸ® GPU: 764.66 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 4.69\n",
            "ğŸ§  Avg Token Entropy: 0.943\n",
            "\n",
            "ğŸ‘¤ You: Did Albert Einstein get nobel prize?\n",
            "\n",
            "ğŸ¤– Bot: Yes, Albert Einstein did receive the Nobel Prize in Physics in 1921.\n",
            "ğŸ“ Input tokens: 26\n",
            "ğŸ“ Output tokens: 20 (68 chars)\n",
            "â± Generation time: 5.89s (3.4 tokens/sec)\n",
            "ğŸ’» CPU: 0.0%, RAM: 2031.38 MB\n",
            "ğŸ® GPU: 748.79 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 9.48\n",
            "ğŸ§  Avg Token Entropy: 0.4616\n",
            "\n",
            "ğŸ‘¤ You: 2+2=?\n",
            "\n",
            "ğŸ¤– Bot: Yes, the correct answer is 4.\n",
            "ğŸ“ Input tokens: 22\n",
            "ğŸ“ Output tokens: 10 (29 chars)\n",
            "â± Generation time: 0.56s (17.94 tokens/sec)\n",
            "ğŸ’» CPU: 0.0%, RAM: 2031.38 MB\n",
            "ğŸ® GPU: 747.57 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 5.65\n",
            "ğŸ§  Avg Token Entropy: 1.8611\n",
            "\n",
            "ğŸ‘¤ You: Add 2 and 2\n",
            "\n",
            "ğŸ¤– Bot: Certainly! Here's an updated version of the poem with 2 and 2 added:\n",
            "\n",
            "In the midst of the night,\n",
            "Amidst the darkness,\n",
            "Amidst the chaos,\n",
            "Amidst the confusion,\n",
            "\n",
            "2 and 2,\n",
            "2 and 2,\n",
            "2 and 2,\n",
            "2 and 2,\n",
            "\n",
            "2 and 2,\n",
            "2 and 2,\n",
            "2 and 2,\n",
            "2 and 2,\n",
            "\n",
            "2 and 2,\n",
            "2 and 2,\n",
            "2 and 2,\n",
            "2 and 2,\n",
            "\n",
            "2 and 2,\n",
            "2 and 2,\n",
            "2 and 2,\n",
            "2 and\n",
            "ğŸ“ Input tokens: 23\n",
            "ğŸ“ Output tokens: 150 (302 chars)\n",
            "â± Generation time: 9.16s (16.37 tokens/sec)\n",
            "ğŸ’» CPU: 0.0%, RAM: 2031.63 MB\n",
            "ğŸ® GPU: 764.66 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 5.46\n",
            "ğŸ§  Avg Token Entropy: 0.9148\n",
            "\n",
            "ğŸ‘¤ You: 2*2=?\n",
            "\n",
            "ğŸ¤– Bot: The product of two integers is the same as the product of their respective factors. For example, 2*2 = 4, since 2 and 2 are factors of 4.\n",
            "ğŸ“ Input tokens: 22\n",
            "ğŸ“ Output tokens: 40 (137 chars)\n",
            "â± Generation time: 2.21s (18.1 tokens/sec)\n",
            "ğŸ’» CPU: 0.0%, RAM: 2031.63 MB\n",
            "ğŸ® GPU: 751.23 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 8.41\n",
            "ğŸ§  Avg Token Entropy: 1.5444\n",
            "\n",
            "ğŸ‘¤ You: Multiply 2 and 2\n",
            "\n",
            "ğŸ¤– Bot: To multiply 2 and 2, you would use the following formula:\n",
            "\n",
            "2 * 2 = 4\n",
            "\n",
            "So, the result would be 4.\n",
            "ğŸ“ Input tokens: 25\n",
            "ğŸ“ Output tokens: 36 (96 chars)\n",
            "â± Generation time: 2.65s (13.58 tokens/sec)\n",
            "ğŸ’» CPU: 0.0%, RAM: 2031.63 MB\n",
            "ğŸ® GPU: 750.74 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 4.58\n",
            "ğŸ§  Avg Token Entropy: 0.974\n",
            "\n",
            "ğŸ‘¤ You: 2*2+4=?\n",
            "\n",
            "ğŸ¤– Bot: 2 * 2 + 4 = ?\n",
            "ğŸ“ Input tokens: 24\n",
            "ğŸ“ Output tokens: 10 (13 chars)\n",
            "â± Generation time: 0.74s (13.53 tokens/sec)\n",
            "ğŸ’» CPU: 0.0%, RAM: 2031.63 MB\n",
            "ğŸ® GPU: 747.57 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 7.59\n",
            "ğŸ§  Avg Token Entropy: 0.6729\n",
            "\n",
            "ğŸ‘¤ You: Multiply 2 and 2, and then add 4\n",
            "\n",
            "ğŸ¤– Bot: To multiply 2 and 2, you can use the following formula:\n",
            "\n",
            "2 * 2 = 4\n",
            "\n",
            "Therefore, 2 and 2 are equal.\n",
            "\n",
            "To add 4, you can use the following formula:\n",
            "\n",
            "4 + 4 = 8\n",
            "\n",
            "Therefore, 2 and 2 are equal.\n",
            "ğŸ“ Input tokens: 31\n",
            "ğŸ“ Output tokens: 74 (185 chars)\n",
            "â± Generation time: 4.11s (18.02 tokens/sec)\n",
            "ğŸ’» CPU: 0.0%, RAM: 2031.63 MB\n",
            "ğŸ® GPU: 755.38 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 5.15\n",
            "ğŸ§  Avg Token Entropy: 0.6445\n",
            "\n",
            "ğŸ‘¤ You: Between 2 and 4 which one is greater?\n",
            "\n",
            "ğŸ¤– Bot: Between 2 and 4, the greater number is 4.\n",
            "ğŸ“ Input tokens: 30\n",
            "ğŸ“ Output tokens: 17 (41 chars)\n",
            "â± Generation time: 0.97s (17.5 tokens/sec)\n",
            "ğŸ’» CPU: 0.0%, RAM: 2031.63 MB\n",
            "ğŸ® GPU: 748.42 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 8.40\n",
            "ğŸ§  Avg Token Entropy: 0.8234\n",
            "\n",
            "ğŸ‘¤ You: I am Raju and 31 years old. What is the name and the age in this sentence?\n",
            "\n",
            "ğŸ¤– Bot: The name in this sentence is \"Raju\" and the age is 31 years old.\n",
            "ğŸ“ Input tokens: 39\n",
            "ğŸ“ Output tokens: 21 (64 chars)\n",
            "â± Generation time: 1.14s (18.37 tokens/sec)\n",
            "ğŸ’» CPU: 0.0%, RAM: 2031.63 MB\n",
            "ğŸ® GPU: 748.91 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 9.35\n",
            "ğŸ§  Avg Token Entropy: 0.5429\n",
            "\n",
            "ğŸ‘¤ You: The movie was absolutely amazing, and I loved every moment of it. Is the sentiment of this sentence positive, or negative, or neutral?\n",
            "\n",
            "ğŸ¤– Bot: The sentiment of this sentence is positive.\n",
            "ğŸ“ Input tokens: 46\n",
            "ğŸ“ Output tokens: 9 (43 chars)\n",
            "â± Generation time: 0.50s (18.03 tokens/sec)\n",
            "ğŸ’» CPU: 0.0%, RAM: 2031.63 MB\n",
            "ğŸ® GPU: 747.45 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 8.25\n",
            "ğŸ§  Avg Token Entropy: 0.9223\n",
            "\n",
            "ğŸ‘¤ You: Than you. bye\n",
            "\n",
            "ğŸ¤– Bot: Sure, here's a revised version of the text with the added line:\n",
            "\n",
            "\"Than you. bye\"\n",
            "ğŸ“ Input tokens: 23\n",
            "ğŸ“ Output tokens: 29 (80 chars)\n",
            "â± Generation time: 1.57s (18.44 tokens/sec)\n",
            "ğŸ’» CPU: 10.0%, RAM: 2031.63 MB\n",
            "ğŸ® GPU: 749.89 / 15095.06 MB (allocated)\n",
            "ğŸ“‰ Perplexity (prompt): 10.69\n",
            "ğŸ§  Avg Token Entropy: 1.7416\n",
            "\n",
            "ğŸ‘¤ You: exit\n",
            "ğŸ‘‹ Exiting chat. Goodbye!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This method only supports 4 bit quantization. Also it only supportsDecoder only model quantization. Not Encode-decoder LLMs."
      ],
      "metadata": {
        "id": "UB0i398PCYzy"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "284b5c9c70c24e6899247c98077bdac8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_e4404a2e4a1f4884af974972eee9f9ca",
              "IPY_MODEL_39fcc7631dbf43568459ec84bf0fb902",
              "IPY_MODEL_665f0b95685240b28c79ab48b879eb77"
            ],
            "layout": "IPY_MODEL_1e605c1d1b274504909931d3cf629565"
          }
        },
        "e4404a2e4a1f4884af974972eee9f9ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_da84f12de7554856a5a058e2946f013c",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_4eea44afe7f34490800b685ffbf83c5e",
            "value": "Fetchingâ€‡10â€‡files:â€‡100%"
          }
        },
        "39fcc7631dbf43568459ec84bf0fb902": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d1ac4f16069e4842827f8362a043ba76",
            "max": 10,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d23334d5b9a14dd7a7c09ecf889957fc",
            "value": 10
          }
        },
        "665f0b95685240b28c79ab48b879eb77": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_953a62c5066943a4b2ffc01d4950f6e5",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_2a5c100afef049e0acd78294f1e42cf0",
            "value": "â€‡10/10â€‡[00:00&lt;00:00,â€‡534.27it/s]"
          }
        },
        "1e605c1d1b274504909931d3cf629565": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da84f12de7554856a5a058e2946f013c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4eea44afe7f34490800b685ffbf83c5e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d1ac4f16069e4842827f8362a043ba76": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d23334d5b9a14dd7a7c09ecf889957fc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "953a62c5066943a4b2ffc01d4950f6e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a5c100afef049e0acd78294f1e42cf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}